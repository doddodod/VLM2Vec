import os
import json
import random
from typing import Dict, Any, List, Optional

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import HfArgumentParser, get_cosine_schedule_with_warmup
from peft import LoraConfig, get_peft_model

from src.arguments import ModelArguments, DataArguments, TrainingArguments
from src.model.model import MMEBModel
from src.model.processor import load_processor
from src.data.collator.train_collator import MultimodalDataCollator
from PIL import Image

import numpy as np
from tqdm import tqdm

import wandb


# ------------------------------
# Helper Functions
# ------------------------------
def format_bbox_as_special_token(bbox, normalize=True, original_width=1024, original_height=1024):
    """å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸ºQwen2-VLçš„special tokenæ ¼å¼"""
    if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        
        if normalize:
            x1_norm = int((x1 / original_width) * 1000)
            y1_norm = int((y1 / original_height) * 1000)
            x2_norm = int((x2 / original_width) * 1000)
            y2_norm = int((y2 / original_height) * 1000)
            
            x1_norm = max(0, min(x1_norm, 999))
            y1_norm = max(0, min(y1_norm, 999))
            x2_norm = max(0, min(x2_norm, 999))
            y2_norm = max(0, min(y2_norm, 999))
            
            x1_norm, x2_norm = min(x1_norm, x2_norm), max(x1_norm, x2_norm)
            y1_norm, y2_norm = min(y1_norm, y2_norm), max(y1_norm, y2_norm)
            
            if x1_norm == x2_norm:
                x2_norm = min(x1_norm + 1, 999)
            if y1_norm == y2_norm:
                y2_norm = min(y1_norm + 1, 999)
            
            return f"<|box_start|>({x1_norm}, {y1_norm}), ({x2_norm}, {y2_norm})<|box_end|>"
    return ""

def format_object_with_ref(object_label):
    """å°†ç‰©ä½“æ ‡ç­¾åŒ…è£…åœ¨å¯¹è±¡å¼•ç”¨tokenä¸­"""
    return f"<|object_ref_start|>{object_label}<|object_ref_end|>"


# image token placeholder
VLM_IMAGE_TOKENS = {"QWEN2_VL": "<|image_pad|>"}  # æ³¨æ„è¿™é‡Œå¿…é¡»æ˜¯ <|image_pad|>
QWEN2_VL = "QWEN2_VL"



# ------------------------------
# Dataset
# ------------------------------
class SGGContrastiveDataset(Dataset):
    def __init__(
        self, 
        json_path: str, 
        image_dir: str, 
        relation_vocabulary: Optional[List[str]] = None, 
        num_negatives: int = 12,
        topk_nearest: Optional[dict] = None,
        topk_nearest_file: Optional[str] = None
    ):
        # è¯»å–æ ·æœ¬
        with open(json_path, 'r') as f:
            content = f.read().strip()
            if content.startswith('['):
                self.samples = json.loads(content)
            else:
                self.samples = [json.loads(l) for l in content.splitlines() if l.strip()]

        self.image_dir = image_dir
        self.num_negatives = num_negatives

        # æ„å»º predicate vocab
        if relation_vocabulary is None:
            rels = set()
            for s in self.samples:
                if 'predicate' in s and s['predicate']:
                    rels.add(s['predicate'])
            self.vocab = sorted(list(rels))
        else:
            self.vocab = relation_vocabulary

        if len(self.vocab) == 0:
            raise ValueError("Empty relation vocabulary")

        # å¤„ç† top-k nearest
        if topk_nearest is not None:
            self.topk_nearest = topk_nearest
        elif topk_nearest_file is not None:
            with open(topk_nearest_file, 'r') as f:
                data = json.load(f)
                self.topk_nearest = {k: v["neighbors"] for k, v in data.items()}
        else:
            self.topk_nearest = {}

    def __len__(self):
        return len(self.samples)

    def _full_image_path(self, path: str):
        return path if os.path.isabs(path) else os.path.join(self.image_dir, path)

    def _make_image_field(self, path: str):
        full = self._full_image_path(path)
        return {'resolutions': [None], 'paths': [full], 'bytes': [None]}

    def _get_image_dimensions(self, image_path: str):
        try:
            with Image.open(image_path) as img:
                return img.size
        except Exception as e:
            print(f"Warning: Failed to get image dimensions for {image_path}: {e}")
            return (1024, 1024)

    def __getitem__(self, idx):
        s = self.samples[idx]
        image_path = s.get('image_path') or s.get('img_path') or s.get('image')
        predicate = s.get('predicate') or s.get('relation') or "related"
        subj = s.get('subject', {})
        obj = s.get('object', {})
        bbox1 = subj.get('bbox') or s.get('bbox1')
        bbox2 = obj.get('bbox') or s.get('bbox2')
        subj_name = subj.get('class_name', 'objectA')
        obj_name = obj.get('class_name', 'objectB')

        # å›¾åƒä¿¡æ¯
        full_image_path = self._full_image_path(image_path)
        original_width, original_height = self._get_image_dimensions(full_image_path)

        # æ ¼å¼åŒ– token
        subj_bbox_token = format_bbox_as_special_token(bbox1, True, original_width, original_height)
        obj_bbox_token = format_bbox_as_special_token(bbox2, True, original_width, original_height)
        subj_ref = format_object_with_ref(subj_name)
        obj_ref = format_object_with_ref(obj_name)

        query_text = f"{VLM_IMAGE_TOKENS[QWEN2_VL]} In the given image, the subject {subj_ref} is located at {subj_bbox_token}, the object {obj_ref} is located at {obj_bbox_token}. Please return the predicate relationship between the subject and the object."
        pos_text = f"The subject is {predicate} the object."

        # è´Ÿæ ·æœ¬ç”Ÿæˆï¼ˆhard nearest + randomï¼‰
        neg_candidates = [r for r in self.vocab if r != predicate]

        hard_negatives = []
        if self.topk_nearest:
            hard_negatives = [r for r in self.topk_nearest.get(predicate, []) if r != predicate]

        remaining = max(self.num_negatives - len(hard_negatives), 0)
        if neg_candidates:
            random_negatives = random.sample(neg_candidates, min(remaining, len(neg_candidates)))
        else:
            random_negatives = []

        final_negatives = hard_negatives + random_negatives
        neg_texts = [f"The subject is {r} the object." for r in final_negatives]

        # image field
        img_field = self._make_image_field(image_path)
        query_image = img_field
        pos_image = img_field
        neg_images = [img_field] * len(neg_texts)

        return {
            'query_text': query_text,
            'query_image': query_image,
            'pos_text': pos_text,
            'pos_image': pos_image,
            'neg_text': neg_texts,
            'neg_image': neg_images,
            'global_dataset_name': s.get('dataset_name', 'vg')
        }



# ------------------------------
# Utils
# ------------------------------
def batch_to_device(batch: Dict[str, Any], device: torch.device) -> Dict[str, Any]:
    if isinstance(batch, dict):
        return {k: batch_to_device(v, device) for k, v in batch.items()}
    elif isinstance(batch, (list, tuple)):
        return type(batch)(batch_to_device(x, device) for x in batch)
    elif isinstance(batch, torch.Tensor):
        return batch.to(device)
    else:
        return batch


def evaluate(model, val_loader, device, return_scores=False):
    """
    è¯„ä¼°æ¨¡å‹å¹³å‡ InfoNCE lossï¼ŒåŒæ—¶å¯é€‰æ‹©è¿”å›æœ€åä¸€ä¸ª batch çš„ scoresã€‚
    """
    model.eval()
    losses = []
    last_scores = None

    with torch.no_grad():
        for batch in val_loader:
            qry_inputs, pos_inputs, neg_inputs = batch  # collator è¿”å›ä¸‰éƒ¨åˆ†

            qry_inputs = batch_to_device(qry_inputs, device)
            pos_inputs = batch_to_device(pos_inputs, device)
            neg_inputs = batch_to_device(neg_inputs, device)

            # forward è¿”å› dict åŒ…å« loss å’Œ scores
            out = model(qry=qry_inputs, tgt=pos_inputs, neg=neg_inputs)
            loss_tensor = out["loss"]
            scores = out.get("scores", None)

            losses.append(loss_tensor.item())
            last_scores = scores  # è®°å½•æœ€åä¸€ä¸ª batch çš„ scores

    model.train()
    mean_loss = float(np.mean(losses)) if losses else 0.0
    if return_scores:
        return mean_loss, last_scores
    return mean_loss



# ------------------------------
# Train Loop
# ------------------------------
def train_loop(model_args: ModelArguments, data_args: DataArguments, training_args: TrainingArguments):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print("Loading processor...")
    processor = load_processor(model_args)

    print("Building model...")
    model = MMEBModel.build(model_args)
    model = model.to(device)
    model.train()

    # åº”ç”¨ LoRA
    if model_args.lora:
        lora_config = LoraConfig(
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            target_modules=model_args.lora_target_modules.split(","),
            lora_dropout=model_args.lora_dropout,
            bias="none",
            task_type="SEQ_2_SEQ_LM",
        )
        model.encoder = get_peft_model(model.encoder, lora_config)
        print("âœ… LoRA applied")
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š Trainable params: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")

        
    # -----------------------------
    # Initialize W&B
    # -----------------------------
    wandb.init(
        project="sgg_qwen2vl",
        name=os.path.basename(training_args.output_dir.rstrip("/")),
        config={**vars(model_args), **vars(data_args), **vars(training_args)},
        mode="offline"  # <-- ç¦»çº¿æ¨¡å¼
    )

    # å‡†å¤‡æ•°æ®
    dataset = SGGContrastiveDataset(
        json_path=data_args.dataset_json,             # JSON æ•°æ®é›†è·¯å¾„
        image_dir=data_args.image_dir,               # å›¾åƒç›®å½•
        num_negatives=data_args.num_negatives,       # æ¯ä¸ªæ ·æœ¬è´Ÿæ ·æœ¬æ€»æ•°
        topk_nearest_file=getattr(data_args, 'topk_nearest_file', None)  # å¯é€‰ topk nearest JSON æ–‡ä»¶
    )
    collator = MultimodalDataCollator(
        processor=processor,
        model_args=model_args,
        data_args=data_args,
        training_args=training_args,
        batch_size=training_args.per_device_train_batch_size
    )

    dataloader = DataLoader(
        dataset, 
        batch_size=training_args.per_device_train_batch_size, 
        shuffle=True, 
        collate_fn=collator,
        num_workers=0,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
        pin_memory=True if torch.cuda.is_available() else False
    )

    # ---------- evaluation dataset/loader (optional) ----------
    # Use the eval path supplied in DataArguments. The default should be set in `src/arguments.py`.
    val_loader = None
    eval_json = getattr(data_args, 'eval_dataset_json', None)
    if eval_json:
        try:
            val_dataset = SGGContrastiveDataset(
                eval_json,
                data_args.image_dir,
                num_negatives=data_args.num_negatives
            )
            eval_batch_size = getattr(training_args, 'per_device_eval_batch_size', training_args.per_device_train_batch_size)
            val_loader = DataLoader(
                val_dataset,
                batch_size=eval_batch_size,
                shuffle=False,
                collate_fn=collator,
                num_workers=0,
                pin_memory=True if torch.cuda.is_available() else False
            )
            print(f"âœ… Eval dataset loaded: {len(val_dataset)} samples from {eval_json}")
        except Exception as e:
            val_loader = None
            print(f"âš ï¸  Failed to load eval dataset from {eval_json}: {e}")

    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        [p for p in model.parameters() if p.requires_grad], 
        lr=training_args.learning_rate,
        weight_decay=training_args.weight_decay if hasattr(training_args, 'weight_decay') else 0.01
    )

    # è®¡ç®—æ€»æ­¥æ•°
    num_update_steps_per_epoch = len(dataloader) // training_args.gradient_accumulation_steps
    total_steps = int(training_args.num_train_epochs) * num_update_steps_per_epoch
    warmup_steps = int(total_steps * (training_args.warmup_ratio if hasattr(training_args, 'warmup_ratio') else 0.1))

    # Cosine å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )

    global_step = 0
    # Loss è®°å½•
    loss_history = {
        'steps': [],
        'losses': [],
        'epoch_losses': [],
        'eval_losses': [],  # eval loss at each epoch
        'eval_losses_steps': [],  # eval loss at each eval step
        'eval_steps': []  # step indices for eval loss
    }
    best_loss = float('inf')
    best_eval_loss = float('inf')

    # è®­ç»ƒå¾ªç¯
    for epoch in range(int(training_args.num_train_epochs)):
        model.train()
        epoch_losses = []
        optimizer.zero_grad()
        
        for batch_idx, (qry_inputs, pos_inputs, neg_inputs) in enumerate(dataloader):
            qry_inputs = batch_to_device(qry_inputs, device)
            pos_inputs = batch_to_device(pos_inputs, device)
            neg_inputs = batch_to_device(neg_inputs, device)
            # å‰å‘ä¼ æ’­
            loss = model(qry=qry_inputs, tgt=pos_inputs, neg=neg_inputs)
            loss_tensor = loss["loss"] if isinstance(loss, dict) else loss
            
            # æ¢¯åº¦ç´¯ç§¯
            loss_tensor = loss_tensor / training_args.gradient_accumulation_steps
            loss_tensor.backward()

            # è®°å½• loss
            epoch_losses.append(loss_tensor.item() * training_args.gradient_accumulation_steps)

            # æ›´æ–°å‚æ•°
            if (batch_idx + 1) % training_args.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª(å¯é€‰)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

                # Log to W&B
                current_lr = scheduler.get_last_lr()[0]
                step_loss = loss_tensor.item() * training_args.gradient_accumulation_steps
                wandb.log({
                    "train/loss_step": step_loss,
                    "lr": current_lr,
                    "epoch": epoch+1,
                    "global_step": global_step
                }, step=global_step)
                if global_step % 100 == 0:
                    print(f"[Step {global_step}] Epoch {epoch+1} | Loss: {step_loss:.4f} | LR: {current_lr:.2e}")

                global_step += 1

                # å®šæœŸ evaluationï¼ˆå¦‚æœé…ç½®äº† eval_steps å¹¶ä¸”æˆåŠŸåŠ è½½äº† val_loaderï¼‰
                if hasattr(training_args, 'eval_steps') and getattr(training_args, 'eval_steps') and val_loader is not None:
                    if global_step % int(training_args.eval_steps) == 0:
                        val_loss = evaluate(model, val_loader, device)
                        wandb.log({"eval/loss": val_loss, "step": global_step})
                        print(f"[Eval Step {global_step}] Validation Loss: {val_loss:.4f}")

                        # æ ¹æ® eval loss ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
                        if val_loss < best_eval_loss:
                            best_eval_loss = val_loss
                            best_dir_eval = os.path.join(training_args.output_dir, "best_model_eval")
                            os.makedirs(best_dir_eval, exist_ok=True)
                            model.save(best_dir_eval)
                            print(f"ğŸ† New best eval model saved at step {global_step} (val_loss: {best_eval_loss:.4f})")
                           
        # Epoch ç»“æŸ
        avg_epoch_loss = np.mean(epoch_losses)
        wandb.log({"train/epoch_loss": avg_epoch_loss, "epoch": epoch+1})
        print(f"ğŸ“Š Epoch {epoch+1} Avg Loss: {avg_epoch_loss:.4f} [W&B logged]\n")

        # Evaluate at the end of each epoch and track eval loss
        if val_loader is not None:
            val_loss_epoch = evaluate(model, val_loader, device)
            wandb.log({"eval/epoch_loss": val_loss_epoch, "epoch": epoch+1})
            print(f"ğŸ“Š Epoch {epoch+1} Validation Loss: {val_loss_epoch:.4f} [W&B logged]")
        
        print(f"\nğŸ“Š Epoch {epoch+1} Summary:")
        print(f"  Average Loss: {avg_epoch_loss:.6f}")
        print(f"  Min Loss: {min(epoch_losses):.6f}")
        print(f"  Max Loss: {max(epoch_losses):.6f}")

        # ä¿å­˜ checkpoint
        if (epoch + 1) % training_args.save_steps == 0 or (epoch + 1) == int(training_args.num_train_epochs):
            save_dir = os.path.join(training_args.output_dir, f"checkpoint-epoch{epoch+1}")
            os.makedirs(save_dir, exist_ok=True)
            model.save(save_dir)
            print(f"ğŸ’¾ Checkpoint saved to: {save_dir}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            wandb.run.summary["best_train_loss"] = best_loss
            best_dir = os.path.join(training_args.output_dir, "best_model")
            os.makedirs(best_dir, exist_ok=True)
            model.save(best_dir)
            print(f"ğŸ† Best model saved â€” loss: {best_loss:.6f} â€” {best_dir}")

    # æœ€ç»ˆä¿å­˜
    final_dir = os.path.join(training_args.output_dir, "final")
    os.makedirs(final_dir, exist_ok=True)
    model.save(final_dir)
    wandb.finish()

    print(f"\n{'='*60}")
    print("âœ… Training Complete!")
    print(f"{'='*60}")
    print(f"ğŸ“ Final model saved to: {final_dir}")
    print(f"ğŸ† Best model saved to: {os.path.join(training_args.output_dir, 'best_model')}")
    print(f"ğŸ¯ Best loss: {best_loss:.6f}")


# ------------------------------
# Main
# ------------------------------
def main():
    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    os.makedirs(training_args.output_dir, exist_ok=True)

    # ä½¿ç”¨ checkpoint path ä½œä¸º model name (ç¦»çº¿åŠ è½½)
    if model_args.checkpoint_path is not None:
        model_args.model_name = model_args.checkpoint_path

    train_loop(model_args, data_args, training_args)


if __name__ == "__main__":
    main()