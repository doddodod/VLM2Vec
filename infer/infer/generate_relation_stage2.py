"""
ç¬¬äºŒé˜¶æ®µï¼šåŸºäºç¬¬ä¸€é˜¶æ®µçš„é«˜ç½®ä¿¡åº¦relationç»“æœï¼Œç”Ÿæˆè¯¦ç»†çš„relationæè¿°ï¼ˆCoTé£æ ¼ï¼‰
è¾“å…¥ï¼šç¬¬ä¸€é˜¶æ®µçš„ç»“æœæ–‡ä»¶ï¼ˆpredict_scene_graph_recall.pyçš„è¾“å‡ºï¼‰
è¾“å‡ºï¼šåŒ…å«ç”Ÿæˆæè¿°çš„å®Œæ•´ç»“æœ

ä¸»è¦åŠŸèƒ½ï¼š
- ä¸ºæ¯å¼ å›¾ç‰‡çš„æ¯ä¸ªpairè°ƒç”¨ä¸€æ¬¡prompt
- promptä¸­åŒ…å«è¯¥pairçš„å€™é€‰è°“è¯çš„ç›¸å¯¹æ’åï¼ˆæœ€å¤š10ä¸ªï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
- åŸºäºç›¸å¯¹æ’åï¼Œç”Ÿæˆå°½å¯èƒ½é•¿çš„CoTé£æ ¼çš„å…³ç³»æè¿°å’Œæ€»ç»“

æ”¯æŒå¤šGPUå¤šworkerå’Œbatchä¼˜åŒ–
"""

import json
import torch
from PIL import Image
from tqdm import tqdm
import os
import sys
import warnings
import argparse
import multiprocessing as mp
from multiprocessing import Manager
import math
import time
import traceback

# è®¾ç½®ç¯å¢ƒå˜é‡æŠ‘åˆ¶transformersè­¦å‘Š
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# ========== é…ç½® ==========
# ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶ï¼ˆè¾“å…¥ï¼‰
STAGE1_RESULT_FILE = "/public/home/xiaojw2025/Data/embedding_similarity/vlm2vec_qwen2vl/result_recall_20_all.json"

# åŸå§‹è¾“å…¥æ•°æ®æ–‡ä»¶ï¼ˆç”¨äºè·å–å›¾ç‰‡è·¯å¾„å’Œç‰©ä½“bboxä¿¡æ¯ï¼‰
INPUT_DATA_FILE = "/public/home/xiaojw2025/Workspace/RAHP/DATASET/VG150/test_case_20.json"

# ç¬¬äºŒé˜¶æ®µè¾“å‡ºæ–‡ä»¶
STAGE2_OUTPUT_FILE = "/public/home/xiaojw2025/Data/stage2/stage2_generated_results_case_20_qwen2vl.json"

# ç”Ÿæˆæ¨¡å‹é…ç½®
GENERATION_MODEL_PATH = "/public/home/xiaojw2025/Workspace/VLM2Vec/models/qwen_vl/Qwen2-VL-2B-Instruct"

# ç”Ÿæˆå‚æ•°
MAX_NEW_TOKENS = 1024
TEMPERATURE = 0.1
TOP_K_RELATIONS = 10  # å·²åºŸå¼ƒï¼šç°åœ¨ä½¿ç”¨top100ä¸­çš„æ‰€æœ‰å€™é€‰è°“è¯åŠå…¶æ’åºåæ¬¡
BATCH_SIZE = 8  # æ‰¹é‡æ¨ç†çš„batch size
SAVE_INTERVAL = 50  # æ¯å¤„ç†50ä¸ªé…å¯¹ä¿å­˜ä¸€æ¬¡
MEMORY_CLEANUP_INTERVAL = 20  # æ¯å¤„ç†20ä¸ªé…å¯¹æ¸…ç†ä¸€æ¬¡å†…å­˜
USE_IMAGE_CACHE = False  # æ˜¯å¦ä½¿ç”¨å›¾åƒç¼“å­˜ä¼˜åŒ–

# ========================

def get_generation_model_class(model_path):
    """æ ¹æ®æ¨¡å‹è·¯å¾„è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æ­£ç¡®çš„ç”Ÿæˆæ¨¡å‹ç±»"""
    try:
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = config.model_type if hasattr(config, 'model_type') else None
        
        print(f"ğŸ” æ£€æµ‹åˆ°ç”Ÿæˆæ¨¡å‹ç±»å‹: {model_type}")
        
        if model_type == 'qwen2_vl':
            from transformers import Qwen2VLForConditionalGeneration
            print("âœ… ä½¿ç”¨ Qwen2VLForConditionalGeneration")
            return Qwen2VLForConditionalGeneration
        elif model_type == 'qwen3_vl':
            try:
                from transformers import Qwen3VLForConditionalGeneration
                print("âœ… ä½¿ç”¨ Qwen3VLForConditionalGeneration")
                return Qwen3VLForConditionalGeneration
            except ImportError:
                print("âš ï¸  Qwen3VL ç±»æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨ AutoModel")
                from transformers import AutoModelForVision2Seq
                return AutoModelForVision2Seq
        else:
            print(f"â„¹ï¸  ä½¿ç”¨é€šç”¨ AutoModelForVision2Seq (æ¨¡å‹ç±»å‹: {model_type})")
            from transformers import AutoModelForVision2Seq
            return AutoModelForVision2Seq
            
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
        print("â„¹ï¸  å›é€€åˆ° AutoModelForVision2Seq")
        from transformers import AutoModelForVision2Seq
        return AutoModelForVision2Seq


def configure_attention_backend():
    """é…ç½®æ³¨æ„åŠ›æœºåˆ¶åç«¯"""
    try:
        if not torch.cuda.is_available():
            return "eager"
        
        device_capability = torch.cuda.get_device_capability()
        major, minor = device_capability
        compute_capability = major * 10 + minor
        
        if compute_capability >= 80:
            try:
                import flash_attn
                return "flash_attention_2"
            except ImportError:
                return "eager"
        else:
            return "eager"
    except Exception as e:
        return "eager"


def normalize_bbox_for_generation(bbox, width, height):
    """å°†bboxåæ ‡å½’ä¸€åŒ–åˆ°[0, 1000)èŒƒå›´ï¼ˆç”¨äºç”Ÿæˆæ¨¡å‹ï¼‰"""
    x1, y1, x2, y2 = bbox
    norm_x1 = int((x1 / width) * 1000)
    norm_y1 = int((y1 / height) * 1000)
    norm_x2 = int((x2 / width) * 1000)
    norm_y2 = int((y2 / height) * 1000)
    
    norm_x1 = max(0, min(norm_x1, 999))
    norm_y1 = max(0, min(norm_y1, 999))
    norm_x2 = max(0, min(norm_x2, 999))
    norm_y2 = max(0, min(norm_y2, 999))
    
    norm_x1, norm_x2 = min(norm_x1, norm_x2), max(norm_x1, norm_x2)
    norm_y1, norm_y2 = min(norm_y1, norm_y2), max(norm_y1, norm_y2)
    
    if norm_x1 == norm_x2:
        norm_x2 = min(norm_x1 + 1, 999)
    if norm_y1 == norm_y2:
        norm_y2 = min(norm_y1 + 1, 999)
    
    return [norm_x1, norm_y1, norm_x2, norm_y2]


def build_prompt(subject_obj, object_obj, ranked_predicates, original_width, original_height):
    """
    æ„å»ºç”Ÿæˆpromptï¼ŒåŒ…å«å€™é€‰è°“è¯çš„ç›¸å¯¹æ’åï¼ˆæœ€å¤š10ä¸ªï¼‰
    
    Args:
        subject_obj: ä¸»ä½“å¯¹è±¡ä¿¡æ¯
        object_obj: å®¢ä½“å¯¹è±¡ä¿¡æ¯
        ranked_predicates: list of strï¼Œè°“è¯åˆ—è¡¨ï¼ˆå·²æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œæœ€å¤š10ä¸ªï¼‰
        original_width: å›¾ç‰‡å®½åº¦
        original_height: å›¾ç‰‡é«˜åº¦
    """
    # å½’ä¸€åŒ–bbox
    subject_norm_bbox = normalize_bbox_for_generation(
        subject_obj['bbox'], original_width, original_height
    )
    object_norm_bbox = normalize_bbox_for_generation(
        object_obj['bbox'], original_width, original_height
    )
    
    # æ ¼å¼åŒ–bboxå­—ç¬¦ä¸²ï¼ˆç®€åŒ–æ ¼å¼ï¼šx1, y1, x2, y2ï¼‰
    subject_bbox_str = f"{subject_norm_bbox[0]}, {subject_norm_bbox[1]}, {subject_norm_bbox[2]}, {subject_norm_bbox[3]}"
    object_bbox_str = f"{object_norm_bbox[0]}, {object_norm_bbox[1]}, {object_norm_bbox[2]}, {object_norm_bbox[3]}"
    
    # æ„å»ºå€™é€‰è°“è¯çš„ç›¸å¯¹æ’ååˆ—è¡¨ï¼ˆ1, 2, 3...ï¼‰
    predicates_text = []
    for idx, predicate in enumerate(ranked_predicates, 1):
        predicates_text.append(f"{idx}. {predicate}")
    
    predicates_text_str = "\n".join(predicates_text)
    
    # æ„å»ºpromptï¼ˆå…¨è‹±æ–‡ï¼ŒCoTé£æ ¼ï¼‰
    prompt_text = (
        f"In this image, there are two objects:\n"
        f"- <|object_ref_start|>{subject_obj['class_name']}<|object_ref_end|> at <|box_start|>({subject_bbox_str})<|box_end|>\n"
        f"- <|object_ref_start|>{object_obj['class_name']}<|object_ref_end|> at <|box_start|>({object_bbox_str})<|box_end|>\n\n"
        f"Stage 1 predicted candidate predicates for this pair (ranked by similarity, top candidates):\n{predicates_text_str}\n\n"
        f"Based on the ranking information above, please provide a comprehensive and detailed description of the relationship between "
        f"{subject_obj['class_name']} and {object_obj['class_name']}. "
        f"Your description should:\n"
        f"1. Consider the ranking positions of different candidate predicates\n"
        f"2. Consider the visual evidence in the image\n"
        f"3. Provide step-by-step reasoning (chain of thought) about why certain predicates are more likely than others\n"
        f"4. Give a thorough, detailed summary of the relationship that is as comprehensive as possible\n"
        f"5. Ensure your conclusions are well-reasoned and accurate\n\n"
        f"Please write a long, detailed description with clear reasoning steps."
    )
    
    return prompt_text


def parse_generated_text(generated_text):
    """
    ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­è§£æCoTé£æ ¼çš„æè¿°
    
    Args:
        generated_text: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬ï¼ˆCoTé£æ ¼çš„æè¿°ï¼‰
    
    Returns:
        str: è§£æåçš„æè¿°æ–‡æœ¬
    """
    # ç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå› ä¸ºCoTé£æ ¼çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„æè¿°
    # å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
    description = generated_text.strip()
    
    if not description or len(description) < 10:
        return f"Failed to parse description (text too short: {description[:100]}...)"
    
    return description


def generate_relation_for_pair(model, processor, image_path, pair_data, 
                              original_width, original_height):
    """
    ä¸ºä¸€ä¸ªé…å¯¹ç”ŸæˆCoTé£æ ¼çš„è¯¦ç»†æè¿°ï¼ˆä½¿ç”¨å•ä¸ªpromptï¼‰
    
    Args:
        model: ç”Ÿæˆæ¨¡å‹
        processor: ç”Ÿæˆæ¨¡å‹çš„processor
        image_path: å›¾ç‰‡è·¯å¾„
        pair_data: é…å¯¹æ•°æ®ï¼ŒåŒ…å«:
            - subject_obj: ä¸»ä½“å¯¹è±¡ä¿¡æ¯
            - object_obj: å®¢ä½“å¯¹è±¡ä¿¡æ¯
            - ranked_predicates: å€™é€‰è°“è¯åˆ—è¡¨ï¼ˆå·²æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œæœ€å¤š10ä¸ªï¼‰
        original_width: å›¾ç‰‡å®½åº¦
        original_height: å›¾ç‰‡é«˜åº¦
    
    Returns:
        dict: åŒ…å«generated_descriptionçš„ç»“æœ
    """
    try:
        # æ„å»ºåŒ…å«æ’åºåæ¬¡çš„prompt
        prompt_text = build_prompt(
            pair_data['subject_obj'], 
            pair_data['object_obj'], 
            pair_data['ranked_predicates'],
            original_width, 
            original_height
        )
        
        # æ„å»ºconversation
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": prompt_text}
                ]
            }
        ]
        
        # åº”ç”¨chat template
        text_prompt = processor.apply_chat_template(
            conversation,
            add_generation_prompt=True,
            tokenize=False
        )
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=text_prompt,
            images=Image.open(image_path),
            return_tensors="pt"
        )
        inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v 
                 for k, v in inputs.items()}
        
        # ç”Ÿæˆï¼ˆä½¿ç”¨è¾ƒå¤§çš„max_new_tokensä»¥å®¹çº³CoTé£æ ¼çš„è¯¦ç»†æè¿°ï¼‰
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS * 2,  # CoTé£æ ¼éœ€è¦æ›´å¤štokens
                do_sample=True,
                temperature=TEMPERATURE,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç 
        input_length = inputs["input_ids"].shape[1]
        generated_tokens = generated_ids[0][input_length:]
        
        generated_text = processor.tokenizer.decode(
            generated_tokens,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        ).strip()
        
        # è§£æç”Ÿæˆçš„æ–‡æœ¬
        description = parse_generated_text(generated_text)
        
        # æ¸…ç†æ˜¾å­˜
        del inputs, generated_ids
        torch.cuda.empty_cache()
        
        return {
            'generated_description': description
        }
        
    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
        traceback.print_exc()
        # è¿”å›é”™è¯¯ç»“æœ
        return {
            'generated_description': f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        }


def generate_relations_batch(model, processor, image_path, generation_tasks, 
                            original_width, original_height, batch_size=BATCH_SIZE, 
                            use_image_cache=USE_IMAGE_CACHE):
    """
    æ‰¹é‡ç”Ÿæˆå¤šä¸ªrelationæè¿°
    
    Args:
        model: ç”Ÿæˆæ¨¡å‹
        processor: ç”Ÿæˆæ¨¡å‹çš„processor
        image_path: å›¾ç‰‡è·¯å¾„
        generation_tasks: list of dictï¼Œæ¯ä¸ªdictåŒ…å«:
            - subject_obj: ä¸»ä½“å¯¹è±¡ä¿¡æ¯
            - object_obj: å®¢ä½“å¯¹è±¡ä¿¡æ¯
            - top_predicate: è°“è¯
            - similarity: ç›¸ä¼¼åº¦
        original_width: å›¾ç‰‡å®½åº¦
        original_height: å›¾ç‰‡é«˜åº¦
        batch_size: æ‰¹é‡å¤§å°
        use_image_cache: æ˜¯å¦ä½¿ç”¨å›¾åƒç¼“å­˜
    
    Returns:
        list of dict: æ¯ä¸ªdictåŒ…å«predicate, similarity, generated_description
    """
    all_results = []
    
    try:
        # ä¼˜åŒ–ï¼šé¢„å¤„ç†å›¾åƒä¸€æ¬¡ï¼ˆå¦‚æœä½¿ç”¨å›¾åƒç¼“å­˜ï¼‰
        cached_pixel_values = None
        cached_image_grid_thw = None
        
        if use_image_cache:
            try:
                dummy_inputs = processor(
                    text=[""],
                    images=[Image.open(image_path)],
                    return_tensors="pt"
                )
                cached_pixel_values = dummy_inputs.get('pixel_values', None)
                cached_image_grid_thw = dummy_inputs.get('image_grid_thw', None)
                
                if cached_pixel_values is not None:
                    cached_pixel_values = cached_pixel_values.to(model.device)
                    if cached_image_grid_thw is not None:
                        cached_image_grid_thw = cached_image_grid_thw.to(model.device)
                
                del dummy_inputs
            except Exception as e:
                use_image_cache = False
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(generation_tasks), batch_size):
            batch = generation_tasks[i:i+batch_size]
            batch_len = len(batch)
            
            # æ‰¹é‡æ„å»ºprompts
            text_prompts = []
            for task in batch:
                prompt_text = build_prompt(
                    task['subject_obj'], task['object_obj'], task['top_predicate'],
                    original_width, original_height
                )
                
                conversation = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image"},
                            {"type": "text", "text": prompt_text}
                        ]
                    }
                ]
                
                text_prompt = processor.apply_chat_template(
                    conversation,
                    add_generation_prompt=True,
                    tokenize=False
                )
                
                text_prompts.append(text_prompt)
            
            # æ‰¹é‡å¤„ç†è¾“å…¥
            if use_image_cache and cached_pixel_values is not None:
                text_inputs = processor(
                    text=text_prompts,
                    images=None,
                    return_tensors="pt",
                    padding=True
                )
                text_inputs['pixel_values'] = cached_pixel_values.repeat(batch_len, 1, 1, 1)
                if cached_image_grid_thw is not None:
                    text_inputs['image_grid_thw'] = cached_image_grid_thw.repeat(batch_len, 1)
                
                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) and v.device != model.device else v 
                         for k, v in text_inputs.items()}
            else:
                inputs = processor(
                    text=text_prompts,
                    images=[Image.open(image_path)] * batch_len,
                    return_tensors="pt",
                    padding=True
                )
                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v 
                         for k, v in inputs.items()}
            
            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=True,
                    temperature=TEMPERATURE,
                    pad_token_id=processor.tokenizer.eos_token_id,
                    eos_token_id=processor.tokenizer.eos_token_id
                )
            
            # æ‰¹é‡è§£ç 
            for j, gen_id in enumerate(generated_ids):
                input_length = inputs["input_ids"][j].shape[0]
                generated_tokens = gen_id[input_length:]
                
                generated_text = processor.tokenizer.decode(
                    generated_tokens,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False
                ).strip()
                
                all_results.append({
                    'predicate': batch[j]['top_predicate'],
                    'similarity': batch[j]['similarity'],
                    'generated_description': generated_text
                })
            
            # é‡Šæ”¾æ˜¾å­˜
            del inputs, generated_ids
            torch.cuda.empty_cache()
        
        return all_results
        
    except Exception as e:
        print(f"æ‰¹é‡ç”Ÿæˆå‡ºé”™: {str(e)}")
        traceback.print_exc()
        # è¿”å›é”™è¯¯ç»“æœ
        return [{
            'predicate': task['top_predicate'],
            'similarity': task['similarity'],
            'generated_description': f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        } for task in generation_tasks]


def load_existing_results(output_path):
    """åŠ è½½å·²å­˜åœ¨çš„ç»“æœ"""
    if os.path.exists(output_path):
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None
    return None


def get_processed_pairs(existing_results):
    """ä»å·²å­˜åœ¨ç»“æœä¸­æå–å·²å¤„ç†çš„é…å¯¹ï¼ˆä½¿ç”¨ç‰©ä½“IDåŒºåˆ†åŒåç‰©ä½“ï¼‰"""
    processed_pairs = set()
    if existing_results and 'results' in existing_results:
        for result in existing_results['results']:
            # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
            subject_id = result.get('subject_id', None)
            object_id = result.get('object_id', None)
            if subject_id is not None and object_id is not None:
                pair_key = (result['image_id'], subject_id, object_id)
            else:
                pair_key = (result['image_id'], result['subject'], result['object'])
            processed_pairs.add(pair_key)
    return processed_pairs


def split_data(data, num_splits):
    """å°†æ•°æ®å‡è¡¡åˆ†å‰²æˆnum_splitsä»½"""
    if num_splits <= 1:
        return [data]
    
    total = len(data)
    if total == 0:
        # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¿”å›num_splitsä¸ªç©ºåˆ—è¡¨
        return [[] for _ in range(num_splits)]
    
    chunk_size = math.ceil(total / num_splits)
    chunks = []
    
    for i in range(num_splits):
        start = i * chunk_size
        end = min(start + chunk_size, total)
        if start < total:
            chunks.append(data[start:end])
        else:
            # ç¡®ä¿æ€»æ˜¯è¿”å›num_splitsä¸ªchunksï¼Œå³ä½¿æœ‰äº›æ˜¯ç©ºçš„
            chunks.append([])
    
    return chunks


def inference_on_gpu(gpu_id, data_chunk, model_path, output_prefix,
                     shared_stats, batch_size=BATCH_SIZE,
                     worker_id=None, max_memory=None):
    """åœ¨æŒ‡å®šGPUä¸Šæ‰§è¡Œæ¨ç†"""
    start_time = time.time()
    
    if worker_id is not None:
        print(f"\n[GPU {gpu_id} Worker {worker_id}] å¼€å§‹åŠ è½½æ¨¡å‹...")
    else:
        print(f"\n[GPU {gpu_id}] å¼€å§‹åŠ è½½æ¨¡å‹...")
    
    torch.cuda.set_device(gpu_id)
    
    try:
        GenModelClass = get_generation_model_class(model_path)
        attn_implementation = configure_attention_backend()
        
        from transformers import AutoProcessor, AutoConfig
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        if hasattr(config, '_attn_implementation'):
            config._attn_implementation = attn_implementation
        
        # åŠ è½½æ¨¡å‹
        if max_memory is not None:
            max_memory_dict = {gpu_id: f"{max_memory}MB"}
            model = GenModelClass.from_pretrained(
                model_path,
                device_map=f"cuda:{gpu_id}",
                max_memory=max_memory_dict,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                config=config
            )
        else:
            model = GenModelClass.from_pretrained(
                model_path,
                device_map=f"cuda:{gpu_id}",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                config=config
            )
        
        processor = AutoProcessor.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        model.eval()
        
        if worker_id is not None:
            print(f"[GPU {gpu_id} Worker {worker_id}] âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¤„ç† {len(data_chunk)} ä¸ªé…å¯¹")
        else:
            print(f"[GPU {gpu_id}] âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¤„ç† {len(data_chunk)} ä¸ªé…å¯¹")
        
        # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
        if worker_id is not None:
            gpu_output_path = f"{output_prefix}_gpu{gpu_id}_worker{worker_id}.json"
        else:
            gpu_output_path = f"{output_prefix}_gpu{gpu_id}.json"
        
        existing_results = load_existing_results(gpu_output_path)
        processed_pairs = get_processed_pairs(existing_results)
        
        if processed_pairs:
            if worker_id is not None:
                print(f"[GPU {gpu_id} Worker {worker_id}] âœ“ å‘ç°å·²å¤„ç†ç»“æœ: {len(processed_pairs)} ä¸ªé…å¯¹")
            else:
                print(f"[GPU {gpu_id}] âœ“ å‘ç°å·²å¤„ç†ç»“æœ: {len(processed_pairs)} ä¸ªé…å¯¹")
        
        # è¿‡æ»¤æœªå¤„ç†çš„é…å¯¹ï¼ˆä½¿ç”¨ç‰©ä½“IDåŒºåˆ†åŒåç‰©ä½“ï¼‰
        unprocessed_chunk = []
        for pair_data in data_chunk:
            # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
            subject_id = pair_data.get('subject_id', None)
            object_id = pair_data.get('object_id', None)
            if subject_id is not None and object_id is not None:
                pair_key = (pair_data['image_id'], subject_id, object_id)
            else:
                pair_key = (pair_data['image_id'], pair_data['subject'], pair_data['object'])
            if pair_key not in processed_pairs:
                unprocessed_chunk.append(pair_data)
        
        if not unprocessed_chunk:
            if worker_id is not None:
                print(f"[GPU {gpu_id} Worker {worker_id}] âœ“ æ‰€æœ‰é…å¯¹å·²å¤„ç†å®Œæˆ")
            else:
                print(f"[GPU {gpu_id}] âœ“ æ‰€æœ‰é…å¯¹å·²å¤„ç†å®Œæˆ")
            return
        
        # åŠ è½½å·²å­˜åœ¨çš„ç»“æœ
        all_results = existing_results.get('results', []) if existing_results else []
        processed_count = len(processed_pairs)
        error_count = 0
        
        # å¤„ç†æœªå¤„ç†çš„é…å¯¹
        for pair_data in tqdm(unprocessed_chunk, desc=f"GPU{gpu_id}" + (f"W{worker_id}" if worker_id is not None else "")):
            try:
                image_path = pair_data['image_path']
                if not os.path.exists(image_path):
                    continue
                
                with Image.open(image_path) as img:
                    original_width, original_height = img.size
                
                # ç”ŸæˆCoTé£æ ¼çš„è¯¦ç»†æè¿°
                stage2_results = generate_relation_for_pair(
                    model, processor, image_path, pair_data,
                    original_width, original_height
                )
                
                # ä¿å­˜ç»“æœï¼ˆåŒ…å«ç‰©ä½“IDä»¥åŒºåˆ†åŒåç‰©ä½“ï¼‰
                result = {
                    'image_id': pair_data['image_id'],
                    'subject_id': pair_data.get('subject_id', None),  # æ·»åŠ subject_id
                    'object_id': pair_data.get('object_id', None),  # æ·»åŠ object_id
                    'subject': pair_data['subject'],
                    'object': pair_data['object'],
                    'ranked_predicates': pair_data['ranked_predicates'],
                    'stage2_generated_description': stage2_results['generated_description'],
                    'has_gt': pair_data.get('has_gt', False),
                    'gt_predicates': pair_data.get('gt_predicates', [])
                }
                
                all_results.append(result)
                processed_count += 1
                
                # å®šæœŸä¿å­˜
                if processed_count % SAVE_INTERVAL == 0:
                    output_data = {
                        'summary': {
                            'total_pairs': len(all_results),
                            'processed_pairs': processed_count
                        },
                        'results': all_results
                    }
                    temp_output_path = f"{output_prefix}_gpu{gpu_id}_worker{worker_id}_temp.json" if worker_id is not None else f"{output_prefix}_gpu{gpu_id}_temp.json"
                    with open(temp_output_path, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, indent=2, ensure_ascii=False)
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if processed_count % MEMORY_CLEANUP_INTERVAL == 0:
                    torch.cuda.empty_cache()
            
            except Exception as e:
                error_count += 1
                if worker_id is not None:
                    print(f"\n[GPU {gpu_id} Worker {worker_id}] å¤„ç†é…å¯¹ {pair_data.get('image_id', 'unknown')} å‡ºé”™: {str(e)}")
                else:
                    print(f"\n[GPU {gpu_id}] å¤„ç†é…å¯¹ {pair_data.get('image_id', 'unknown')} å‡ºé”™: {str(e)}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_data = {
            'summary': {
                'total_pairs': len(all_results),
                'processed_pairs': processed_count,
                'error_count': error_count
            },
            'results': all_results
        }
        
        with open(gpu_output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats_key = f"{gpu_id}_worker{worker_id}" if worker_id is not None else str(gpu_id)
        shared_stats[stats_key] = {
            'processed': processed_count,
            'errors': error_count,
            'time_minutes': (time.time() - start_time) / 60
        }
        
        if worker_id is not None:
            print(f"\n[GPU {gpu_id} Worker {worker_id}] âœ“ æ¨ç†å®Œæˆï¼å¤„ç†äº† {processed_count} ä¸ªé…å¯¹ï¼Œå¤±è´¥ {error_count} ä¸ª")
        else:
            print(f"\n[GPU {gpu_id}] âœ“ æ¨ç†å®Œæˆï¼å¤„ç†äº† {processed_count} ä¸ªé…å¯¹ï¼Œå¤±è´¥ {error_count} ä¸ª")
    
    except Exception as e:
        print(f"\n[GPU {gpu_id}] ä¸¥é‡é”™è¯¯: {str(e)}")
        traceback.print_exc()
        stats_key = f"{gpu_id}_worker{worker_id}" if worker_id is not None else str(gpu_id)
        shared_stats[stats_key] = {
            'processed': 0,
            'errors': len(data_chunk),
            'error_msg': str(e)
        }


def merge_results(output_prefix, num_gpus, final_output_path, total_pairs, workers_per_gpu=1):
    """åˆå¹¶æ‰€æœ‰GPUçš„ç»“æœ"""
    print(f"\nåˆå¹¶ {num_gpus} ä¸ªGPUçš„ç»“æœï¼ˆæ¯GPU {workers_per_gpu} ä¸ªworkerï¼‰...")
    
    all_results = []
    processed_pairs = set()
    
    for gpu_id in range(num_gpus):
        if workers_per_gpu > 1:
            for worker_id in range(workers_per_gpu):
                gpu_output_path = f"{output_prefix}_gpu{gpu_id}_worker{worker_id}.json"
                if os.path.exists(gpu_output_path):
                    with open(gpu_output_path, 'r', encoding='utf-8') as f:
                        gpu_results = json.load(f)
                        for result in gpu_results.get('results', []):
                            # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
                            subject_id = result.get('subject_id', None)
                            object_id = result.get('object_id', None)
                            if subject_id is not None and object_id is not None:
                                pair_key = (result['image_id'], subject_id, object_id)
                            else:
                                pair_key = (result['image_id'], result['subject'], result['object'])
                            if pair_key not in processed_pairs:
                                all_results.append(result)
                                processed_pairs.add(pair_key)
        else:
            gpu_output_path = f"{output_prefix}_gpu{gpu_id}.json"
            if os.path.exists(gpu_output_path):
                with open(gpu_output_path, 'r', encoding='utf-8') as f:
                    gpu_results = json.load(f)
                    for result in gpu_results.get('results', []):
                        # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
                        subject_id = result.get('subject_id', None)
                        object_id = result.get('object_id', None)
                        if subject_id is not None and object_id is not None:
                            pair_key = (result['image_id'], subject_id, object_id)
                        else:
                            pair_key = (result['image_id'], result['subject'], result['object'])
                        if pair_key not in processed_pairs:
                            all_results.append(result)
                            processed_pairs.add(pair_key)
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    output_data = {
        'summary': {
            'total_pairs': len(all_results),
            'total_images': len(set(r['image_id'] for r in all_results)),
            'top_k_relations': TOP_K_RELATIONS,
            'generation_max_tokens': MAX_NEW_TOKENS,
            'generation_temperature': TEMPERATURE,
            'num_gpus': num_gpus,
            'workers_per_gpu': workers_per_gpu
        },
        'results': all_results
    }
    
    os.makedirs(os.path.dirname(final_output_path), exist_ok=True)
    with open(final_output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ åˆå¹¶å®Œæˆï¼Œå…± {len(all_results)} ä¸ªé…å¯¹")


def prepare_data_for_inference(stage1_data, input_data_map):
    """å‡†å¤‡æ¨ç†æ•°æ®"""
    per_image_top100 = stage1_data.get('per_image_top100_candidates', {})
    
    all_pairs = []
    
    for image_id, top100_candidates in per_image_top100.items():
        # å°è¯•å¤šç§æ ¼å¼åŒ¹é…image_id
        img_data = None
        if image_id in input_data_map:
            img_data = input_data_map[image_id]
        elif str(image_id) in input_data_map:
            img_data = input_data_map[str(image_id)]
        elif isinstance(image_id, str) and image_id.isdigit() and int(image_id) in input_data_map:
            img_data = input_data_map[int(image_id)]
        
        if img_data is None:
            continue
        
        image_path = img_data['image_path']
        objects = img_data['objects']
        
        if not os.path.exists(image_path):
            continue
        
        # åˆ›å»ºç‰©ä½“åç§°åˆ°ç‰©ä½“ä¿¡æ¯çš„æ˜ å°„ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
        obj_dict_by_name = {obj['class_name']: obj for obj in objects}
        # åˆ›å»ºç‰©ä½“IDåˆ°ç‰©ä½“ä¿¡æ¯çš„æ˜ å°„
        obj_dict_by_id = {obj['id']: obj for obj in objects}
        
        # æŒ‰é…å¯¹åˆ†ç»„å€™é€‰ï¼ˆä½¿ç”¨ç‰©ä½“IDåŒºåˆ†åŒåç‰©ä½“ï¼‰
        pair_candidates = {}
        for candidate in top100_candidates:
            # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“ID
            subject_id = candidate.get('subject_id', None)
            object_id = candidate.get('object_id', None)
            subject = candidate['subject']
            object_name = candidate['object']
            predicate = candidate.get('predicted_predicate', 'no relation')
            
            # ä½¿ç”¨ç‰©ä½“IDä½œä¸ºkeyï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
            if subject_id is not None and object_id is not None:
                pair_key = (subject_id, object_id)
            else:
                pair_key = (subject, object_name)
            
            if pair_key not in pair_candidates:
                pair_candidates[pair_key] = {
                    'candidates': [],
                    'subject_id': subject_id,
                    'object_id': object_id,
                    'subject': subject,
                    'object': object_name
                }
            
            if predicate != 'no relation':
                pair_candidates[pair_key]['candidates'].append({
                    'predicate': predicate,
                    'similarity': candidate.get('similarity', 0)
                })
        
        # å¯¹æ¯ä¸ªé…å¯¹å‡†å¤‡æ•°æ®
        for pair_key, pair_data in pair_candidates.items():
            candidates = pair_data['candidates']
            subject_id = pair_data['subject_id']
            object_id = pair_data['object_id']
            subject_name = pair_data['subject']
            object_name = pair_data['object']
            
            if not candidates:
                continue
            
            # è·å–ç‰©ä½“å¯¹è±¡ä¿¡æ¯
            if subject_id is not None and object_id is not None:
                # ä½¿ç”¨ç‰©ä½“IDè·å–
                if subject_id not in obj_dict_by_id or object_id not in obj_dict_by_id:
                    continue
                subject_obj = obj_dict_by_id[subject_id]
                object_obj = obj_dict_by_id[object_id]
            else:
                # å‘åå…¼å®¹ï¼šä½¿ç”¨ç±»åˆ«åè·å–
                if subject_name not in obj_dict_by_name or object_name not in obj_dict_by_name:
                    continue
                subject_obj = obj_dict_by_name[subject_name]
                object_obj = obj_dict_by_name[object_name]
                # ä»å¯¹è±¡ä¸­è·å–ID
                subject_id = subject_obj.get('id', None)
                object_id = object_obj.get('id', None)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰10ä¸ªï¼ˆæœ€é«˜çš„10ä¸ªï¼‰
            candidates_sorted = sorted(
                candidates,
                key=lambda x: x['similarity'],
                reverse=True
            )
            top_predicates = candidates_sorted[:10]  # æœ€å¤š10ä¸ª
            
            # åªä¿å­˜predicateï¼Œä¸éœ€è¦rankå’Œsimilarity
            ranked_predicates = [item['predicate'] for item in top_predicates]
            
            # è·å–GTä¿¡æ¯ï¼ˆä»ç¬¬ä¸€ä¸ªå€™é€‰è·å–ï¼‰
            has_gt = False
            gt_predicates = []
            for candidate in top100_candidates:
                # åŒ¹é…æ¡ä»¶ï¼šä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦åˆ™ä½¿ç”¨ç±»åˆ«å
                match = False
                if subject_id is not None and object_id is not None:
                    if (candidate.get('subject_id') == subject_id and 
                        candidate.get('object_id') == object_id):
                        match = True
                else:
                    if (candidate['subject'] == subject_name and 
                        candidate['object'] == object_name):
                        match = True
                
                if match:
                    has_gt = candidate.get('has_gt', False)
                    gt_predicates = candidate.get('gt_predicates', [])
                    break
            
            all_pairs.append({
                'image_id': image_id,
                'image_path': image_path,
                'subject_id': subject_id,  # æ·»åŠ subject_id
                'object_id': object_id,  # æ·»åŠ object_id
                'subject': subject_name,
                'object': object_name,
                'subject_obj': subject_obj,
                'object_obj': object_obj,
                'ranked_predicates': ranked_predicates,  # åªåŒ…å«predicateåˆ—è¡¨ï¼ˆæœ€å¤š10ä¸ªï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
                'has_gt': has_gt,
                'gt_predicates': gt_predicates
            })
    
    return all_pairs


def main():
    parser = argparse.ArgumentParser(description='ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆè¯¦ç»†relationæè¿°')
    parser.add_argument('--stage1_result', type=str, default=STAGE1_RESULT_FILE,
                       help='ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input_data', type=str, default=INPUT_DATA_FILE,
                       help='åŸå§‹è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default=STAGE2_OUTPUT_FILE,
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model_path', type=str, default=GENERATION_MODEL_PATH,
                       help='ç”Ÿæˆæ¨¡å‹è·¯å¾„')
    parser.add_argument('--num_gpus', type=int, default=1,
                       help='ä½¿ç”¨çš„GPUæ•°é‡ï¼Œé»˜è®¤ä¸º1ï¼ˆå•GPUæ¨¡å¼ï¼‰')
    
    # å…ˆå£°æ˜globalï¼Œå†ä½¿ç”¨è¿™äº›å˜é‡
    global TOP_K_RELATIONS, BATCH_SIZE
    
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE,
                       help=f'æ‰¹é‡æ¨ç†çš„batch sizeï¼Œé»˜è®¤{BATCH_SIZE}')
    parser.add_argument('--workers_per_gpu', type=int, default=1,
                       help='æ¯ä¸ªGPUä¸Šçš„å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤1')
    parser.add_argument('--top_k', type=int, default=TOP_K_RELATIONS,
                       help=f'æ¯ä¸ªé…å¯¹é€‰æ‹©çš„é«˜ç½®ä¿¡åº¦relationæ•°é‡ï¼Œé»˜è®¤{TOP_K_RELATIONS}')
    
    args = parser.parse_args()
    
    # æ›´æ–°å…¨å±€å˜é‡
    TOP_K_RELATIONS = args.top_k
    BATCH_SIZE = args.batch_size
    
    print("="*80)
    print("ç¬¬äºŒé˜¶æ®µï¼šåŸºäºç¬¬ä¸€é˜¶æ®µç»“æœç”Ÿæˆè¯¦ç»†relationæè¿°")
    print("="*80)
    print(f"âœ“ Stage1ç»“æœæ–‡ä»¶: {args.stage1_result}")
    print(f"âœ“ è¾“å…¥æ•°æ®æ–‡ä»¶: {args.input_data}")
    print(f"âœ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"âœ“ æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"âœ“ GPUæ•°é‡: {args.num_gpus}")
    print(f"âœ“ Batch Size: {args.batch_size}")
    print(f"âœ“ Workers per GPU: {args.workers_per_gpu}")
    print(f"âœ“ Top-K Relations: {args.top_k}")
    
    # åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ...")
    with open(args.stage1_result, 'r', encoding='utf-8') as f:
        stage1_data = json.load(f)
    
    # åŠ è½½åŸå§‹è¾“å…¥æ•°æ®
    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸå§‹è¾“å…¥æ•°æ®...")
    with open(args.input_data, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    
    # åˆ›å»ºimage_idæ˜ å°„ï¼Œæ”¯æŒå­—ç¬¦ä¸²å’Œæ•´æ•°ç±»å‹çš„åŒ¹é…
    image_data_map = {}
    for img in input_data:
        img_id = img['image_id']
        # åŒæ—¶æ”¯æŒå­—ç¬¦ä¸²å’Œæ•´æ•°ç±»å‹çš„key
        image_data_map[str(img_id)] = img
        if isinstance(img_id, int):
            image_data_map[img_id] = img
        elif isinstance(img_id, str) and img_id.isdigit():
            image_data_map[int(img_id)] = img
    
    # å‡†å¤‡æ¨ç†æ•°æ®
    print(f"ğŸ“– æ­£åœ¨å‡†å¤‡æ¨ç†æ•°æ®...")
    all_pairs = prepare_data_for_inference(stage1_data, image_data_map)
    print(f"   å…± {len(all_pairs)} ä¸ªé…å¯¹éœ€è¦å¤„ç†")
    
    # æ ¹æ®GPUæ•°é‡é€‰æ‹©æ¨ç†æ¨¡å¼
    if args.num_gpus == 1:
        # å•GPUæ¨¡å¼
        print(f"\n[3/3] å•GPUæ¨ç†æ¨¡å¼")
        print("-" * 80)
        
        GenModelClass = get_generation_model_class(args.model_path)
        from transformers import AutoProcessor
        processor = AutoProcessor.from_pretrained(args.model_path, trust_remote_code=True)
        model = GenModelClass.from_pretrained(
            args.model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
        
        # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
        existing_results = load_existing_results(args.output)
        processed_pairs = get_processed_pairs(existing_results)
        
        if processed_pairs:
            print(f"âœ“ å‘ç°å·²å¤„ç†ç»“æœ: {len(processed_pairs)} ä¸ªé…å¯¹")
        
        unprocessed_pairs = []
        for p in all_pairs:
            # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
            subject_id = p.get('subject_id', None)
            object_id = p.get('object_id', None)
            if subject_id is not None and object_id is not None:
                pair_key = (p['image_id'], subject_id, object_id)
            else:
                pair_key = (p['image_id'], p['subject'], p['object'])
            if pair_key not in processed_pairs:
                unprocessed_pairs.append(p)
        
        if not unprocessed_pairs:
            print("âœ“ æ‰€æœ‰é…å¯¹å·²å¤„ç†å®Œæˆ")
            return
        
        all_results = existing_results.get('results', []) if existing_results else []
        processed_count = len(processed_pairs)
        
        for pair_data in tqdm(unprocessed_pairs, desc="å¤„ç†é…å¯¹"):
            try:
                image_path = pair_data['image_path']
                with Image.open(image_path) as img:
                    original_width, original_height = img.size
                
                # ç”ŸæˆCoTé£æ ¼çš„è¯¦ç»†æè¿°
                stage2_results = generate_relation_for_pair(
                    model, processor, image_path, pair_data,
                    original_width, original_height
                )
                
                result = {
                    'image_id': pair_data['image_id'],
                    'subject_id': pair_data.get('subject_id', None),  # æ·»åŠ subject_id
                    'object_id': pair_data.get('object_id', None),  # æ·»åŠ object_id
                    'subject': pair_data['subject'],
                    'object': pair_data['object'],
                    'ranked_predicates': pair_data['ranked_predicates'],
                    'stage2_generated_description': stage2_results['generated_description'],
                    'has_gt': pair_data.get('has_gt', False),
                    'gt_predicates': pair_data.get('gt_predicates', [])
                }
                
                all_results.append(result)
                processed_count += 1
                
                if processed_count % SAVE_INTERVAL == 0:
                    output_data = {
                        'summary': {
                            'total_pairs': len(all_results),
                            'processed_pairs': processed_count
                        },
                        'results': all_results
                    }
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            except Exception as e:
                print(f"å¤„ç†é…å¯¹å‡ºé”™: {str(e)}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_data = {
            'summary': {
                'total_pairs': len(all_results),
                'top_k_relations': TOP_K_RELATIONS,
                'generation_max_tokens': MAX_NEW_TOKENS,
                'generation_temperature': TEMPERATURE
            },
            'results': all_results
        }
        
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… æ¨ç†å®Œæˆï¼å¤„ç†äº† {processed_count} ä¸ªé…å¯¹")
    
    else:
        # å¤šGPUæ¨¡å¼
        total_workers = args.num_gpus * args.workers_per_gpu
        print(f"\n[3/3] å¤šGPUæ¨ç†æ¨¡å¼ ({args.num_gpus} ä¸ªGPU, æ¯GPU {args.workers_per_gpu} ä¸ªworker, å…± {total_workers} ä¸ªè¿›ç¨‹)")
        print("-" * 80)
        
        output_prefix = args.output.replace('.json', '')
        
        # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
        final_existing_results = load_existing_results(args.output)
        if final_existing_results:
            final_processed_pairs = get_processed_pairs(final_existing_results)
            print(f"âœ“ å‘ç°æœ€ç»ˆåˆå¹¶ç»“æœ: {len(final_processed_pairs)} ä¸ªé…å¯¹")
            filtered_pairs = []
            for p in all_pairs:
                # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
                subject_id = p.get('subject_id', None)
                object_id = p.get('object_id', None)
                if subject_id is not None and object_id is not None:
                    pair_key = (p['image_id'], subject_id, object_id)
                else:
                    pair_key = (p['image_id'], p['subject'], p['object'])
                if pair_key not in final_processed_pairs:
                    filtered_pairs.append(p)
            all_pairs = filtered_pairs
            print(f"âœ“ è¿‡æ»¤åå‰©ä½™æœªå¤„ç†é…å¯¹: {len(all_pairs)} ä¸ª")
            if len(all_pairs) == 0:
                print("âœ“ æ‰€æœ‰é…å¯¹å·²å¤„ç†å®Œæˆ")
                return
        
        # åˆ†å‰²æ•°æ®
        data_chunks = split_data(all_pairs, args.num_gpus)
        
        # ç¡®ä¿data_chunksçš„é•¿åº¦ç­‰äºnum_gpus
        if len(data_chunks) < args.num_gpus:
            # å¦‚æœchunksæ•°é‡ä¸è¶³ï¼Œè¡¥å……ç©ºåˆ—è¡¨
            while len(data_chunks) < args.num_gpus:
                data_chunks.append([])
        
        # å¦‚æœæ¯GPUæœ‰å¤šä¸ªworkerï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²æ•°æ®
        if args.workers_per_gpu > 1:
            if torch.cuda.is_available():
                gpu_memory_mb = torch.cuda.get_device_properties(0).total_memory / 1024**2
                max_memory_per_worker = int(gpu_memory_mb * 0.8 / args.workers_per_gpu)
                print(f"âœ“ æ¯GPUæ˜¾å­˜: {gpu_memory_mb/1024:.1f}GB, æ¯Workeræ˜¾å­˜é™åˆ¶: {max_memory_per_worker/1024:.1f}GB")
            else:
                max_memory_per_worker = None
            
            worker_chunks = []
            worker_gpu_ids = []
            worker_ids = []
            for gpu_id in range(args.num_gpus):
                if len(data_chunks[gpu_id]) > 0:
                    gpu_data = data_chunks[gpu_id]
                    worker_data_chunks = split_data(gpu_data, args.workers_per_gpu)
                    for worker_id in range(args.workers_per_gpu):
                        if len(worker_data_chunks[worker_id]) > 0:
                            worker_chunks.append(worker_data_chunks[worker_id])
                            worker_gpu_ids.append(gpu_id)
                            worker_ids.append(worker_id)
        else:
            worker_chunks = []
            worker_gpu_ids = []
            worker_ids = []
            max_memory_per_worker = None
            for gpu_id in range(args.num_gpus):
                if len(data_chunks[gpu_id]) > 0:
                    worker_chunks.append(data_chunks[gpu_id])
                    worker_gpu_ids.append(gpu_id)
                    worker_ids.append(None)
        
        print(f"âœ“ æ•°æ®å·²åˆ†å‰²æˆ {len(worker_chunks)} ä»½")
        
        # ä½¿ç”¨multiprocessingå¯åŠ¨å¤šä¸ªè¿›ç¨‹
        manager = Manager()
        shared_stats = manager.dict()
        
        processes = []
        start_time = time.time()
        
        for gpu_id, worker_id, chunk in zip(worker_gpu_ids, worker_ids, worker_chunks):
            p = mp.Process(
                target=inference_on_gpu,
                args=(gpu_id, chunk, args.model_path, output_prefix,
                     shared_stats, args.batch_size, worker_id, max_memory_per_worker)
            )
            p.start()
            processes.append(p)
        
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        for p in processes:
            p.join()
        
        print("\n" + "=" * 80)
        print("æ‰€æœ‰GPUæ¨ç†å®Œæˆï¼Œå¼€å§‹åˆå¹¶ç»“æœ...")
        print("=" * 80)
        
        # åˆå¹¶ç»“æœ
        merge_results(output_prefix, args.num_gpus, args.output, len(all_pairs), args.workers_per_gpu)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        total_processed = sum(stats.get("processed", 0) for stats in shared_stats.values())
        total_errors = sum(stats.get("errors", 0) for stats in shared_stats.values())
        
        print("\n" + "=" * 80)
        print("å¤šGPUæ¨ç†å®Œæˆï¼")
        print("=" * 80)
        print(f"æ€»é…å¯¹æ•°: {len(all_pairs)}")
        print(f"æˆåŠŸå¤„ç†: {total_processed}")
        print(f"å¤±è´¥æ•°: {total_errors}")
        print(f"æ€»è€—æ—¶: {total_time/60:.2f}åˆ†é’Ÿ")
        print(f"æ€»è¿›ç¨‹æ•°: {total_workers}")
        print("=" * 80)
        print(f"âœ“ æœ€ç»ˆç»“æœä¿å­˜è‡³: {args.output}")


if __name__ == "__main__":
    # åœ¨Linuxä¸Šï¼Œå¿…é¡»ä½¿ç”¨spawnæ–¹æ³•æ‰èƒ½åœ¨å¤šè¿›ç¨‹ä¸­æ­£ç¡®ä½¿ç”¨CUDA
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()
