# ç¬¬äºŒé˜¶æ®µ Relation ç”Ÿæˆ Pipeline è¯¦ç»†æµç¨‹è¯´æ˜

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
3. [æ•°æ®æµ](#æ•°æ®æµ)
4. [è¯¦ç»†å¤„ç†æµç¨‹](#è¯¦ç»†å¤„ç†æµç¨‹)
5. [å…³é”®æ•°æ®ç»“æ„](#å…³é”®æ•°æ®ç»“æ„)
6. [å¤šGPUå¤šWorkeræœºåˆ¶](#å¤šgpuå¤šworkeræœºåˆ¶)
7. [æ‰¹é‡ç”Ÿæˆä¼˜åŒ–](#æ‰¹é‡ç”Ÿæˆä¼˜åŒ–)
8. [æ–­ç‚¹ç»­ä¼ æœºåˆ¶](#æ–­ç‚¹ç»­ä¼ æœºåˆ¶)

---

## æ¦‚è¿°

### åŠŸèƒ½ç›®æ ‡
åŸºäºç¬¬ä¸€é˜¶æ®µï¼ˆembeddingæ–¹æ³•ï¼‰é¢„æµ‹çš„é«˜ç½®ä¿¡åº¦relationç»“æœï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆQwen3-VLï¼‰ç”Ÿæˆè¯¦ç»†çš„relationæè¿°æ–‡æœ¬ã€‚

### è¾“å…¥è¾“å‡º
- **è¾“å…¥1**: ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶ï¼ˆ`recall_results_*.json`ï¼‰
  - åŒ…å«æ¯å¼ å›¾ç‰‡çš„Top-100å€™é€‰relation
  - æ¯ä¸ªå€™é€‰åŒ…å«ï¼šsubject, object, predicted_predicate, similarityç­‰
  
- **è¾“å…¥2**: åŸå§‹è¾“å…¥æ•°æ®æ–‡ä»¶ï¼ˆ`test_2000_images.json`ï¼‰
  - åŒ…å«å›¾ç‰‡è·¯å¾„ã€ç‰©ä½“ä¿¡æ¯ï¼ˆbboxç­‰ï¼‰
  
- **è¾“å‡º**: ç¬¬äºŒé˜¶æ®µç”Ÿæˆç»“æœï¼ˆ`stage2_generated_results.json`ï¼‰
  - åŒ…å«æ¯ä¸ªé…å¯¹çš„ç¬¬ä¸€é˜¶æ®µtop-K relation
  - ä»¥åŠå¯¹åº”çš„è¯¦ç»†ç”Ÿæˆæè¿°

---

## æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      main() å‡½æ•°                             â”‚
â”‚  1. è§£æå‘½ä»¤è¡Œå‚æ•°                                           â”‚
â”‚  2. åŠ è½½æ•°æ®ï¼ˆç¬¬ä¸€é˜¶æ®µç»“æœ + åŸå§‹è¾“å…¥æ•°æ®ï¼‰                  â”‚
â”‚  3. å‡†å¤‡æ¨ç†æ•°æ®ï¼ˆprepare_data_for_inferenceï¼‰              â”‚
â”‚  4. é€‰æ‹©æ¨ç†æ¨¡å¼ï¼ˆå•GPU / å¤šGPUï¼‰                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   å•GPUæ¨¡å¼     â”‚         â”‚   å¤šGPUæ¨¡å¼     â”‚
        â”‚                 â”‚         â”‚                 â”‚
        â”‚ ç›´æ¥è°ƒç”¨ç”Ÿæˆå‡½æ•° â”‚         â”‚ å¯åŠ¨å¤šä¸ªè¿›ç¨‹    â”‚
        â”‚                 â”‚         â”‚ inference_on_gpuâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                   â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  inference_on_gpuâ”‚  â”‚  inference_on_gpuâ”‚
                            â”‚  (GPU 0)        â”‚  â”‚  (GPU 1)        â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚                   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   merge_results   â”‚
                                    â”‚   åˆå¹¶æ‰€æœ‰ç»“æœ     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ•°æ®æµ

### 1. æ•°æ®åŠ è½½é˜¶æ®µ

```
ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶ (JSON)
â”œâ”€â”€ per_image_top100_candidates: {
â”‚     "2343729": [
â”‚       {
â”‚         "subject": "windshield",
â”‚         "object": "train",
â”‚         "predicted_predicate": "on",
â”‚         "similarity": 0.85,
â”‚         "has_gt": true,
â”‚         "gt_predicates": ["on"]
â”‚       },
â”‚       ... (æœ€å¤š100ä¸ª)
â”‚     ]
â”‚   }
â””â”€â”€ ...

åŸå§‹è¾“å…¥æ•°æ®æ–‡ä»¶ (JSON)
â”œâ”€â”€ [
â”‚   {
â”‚     "image_id": 2343729,
â”‚     "image_path": "/path/to/image.jpg",
â”‚     "objects": [
â”‚       {
â”‚         "id": 1,
â”‚         "class_name": "windshield",
â”‚         "bbox": [100, 200, 300, 400]
â”‚       },
â”‚       ...
â”‚     ],
â”‚     "relations": [...]
â”‚   },
â”‚   ...
â”‚ ]
```

### 2. æ•°æ®å‡†å¤‡é˜¶æ®µï¼ˆprepare_data_for_inferenceï¼‰

**å¤„ç†æ­¥éª¤ï¼š**

1. **éå†æ¯å¼ å›¾ç‰‡çš„Top-100å€™é€‰**
   ```python
   for image_id, top100_candidates in per_image_top100.items():
   ```

2. **åŒ¹é…åŸå§‹æ•°æ®**
   - é€šè¿‡image_idåœ¨input_data_mapä¸­æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡æ•°æ®
   - æ”¯æŒå­—ç¬¦ä¸²å’Œæ•´æ•°ç±»å‹çš„image_idåŒ¹é…

3. **æŒ‰é…å¯¹åˆ†ç»„**
   - å°†åŒä¸€å¼ å›¾ç‰‡çš„å¤šä¸ªå€™é€‰æŒ‰(subject, object)é…å¯¹åˆ†ç»„
   - è¿‡æ»¤æ‰"no relation"çš„å€™é€‰

4. **é€‰æ‹©Top-K**
   - å¯¹æ¯ä¸ªé…å¯¹çš„å€™é€‰æŒ‰similarityæ’åº
   - é€‰æ‹©top-Kä¸ªï¼ˆé»˜è®¤10ä¸ªï¼‰é«˜ç½®ä¿¡åº¦relation

5. **æ„å»ºpair_data**
   - ä¸ºæ¯ä¸ªé…å¯¹åˆ›å»ºä¸€ä¸ªpair_dataå¯¹è±¡
   - åŒ…å«subject_objã€object_objã€stage1_top_kç­‰ä¿¡æ¯

**è¾“å‡ºï¼š`all_pairs` åˆ—è¡¨**
```python
[
  {
    'image_id': '2343729',
    'image_path': '/path/to/image.jpg',
    'subject': 'windshield',
    'object': 'train',
    'subject_obj': {'class_name': 'windshield', 'bbox': [...], ...},
    'object_obj': {'class_name': 'train', 'bbox': [...], ...},
    'stage1_top_k': [
      {'predicate': 'on', 'similarity': 0.85},
      {'predicate': 'above', 'similarity': 0.72},
      ... (æœ€å¤š10ä¸ª)
    ],
    'has_gt': True,
    'gt_predicates': ['on']
  },
  ... (æ‰€æœ‰é…å¯¹çš„åˆ—è¡¨)
]
```

---

## è¯¦ç»†å¤„ç†æµç¨‹

### é˜¶æ®µ1: åˆå§‹åŒ–ï¼ˆmainå‡½æ•°ï¼‰

```python
main()
â”œâ”€â”€ è§£æå‘½ä»¤è¡Œå‚æ•°
â”‚   â”œâ”€â”€ --stage1_result: ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶
â”‚   â”œâ”€â”€ --input_data: åŸå§‹è¾“å…¥æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ --output: è¾“å‡ºæ–‡ä»¶è·¯å¾„
â”‚   â”œâ”€â”€ --model_path: ç”Ÿæˆæ¨¡å‹è·¯å¾„
â”‚   â”œâ”€â”€ --num_gpus: GPUæ•°é‡
â”‚   â”œâ”€â”€ --batch_size: æ‰¹é‡å¤§å°
â”‚   â”œâ”€â”€ --workers_per_gpu: æ¯ä¸ªGPUçš„workeræ•°
â”‚   â””â”€â”€ --top_k: Top-K relationæ•°é‡
â”‚
â”œâ”€â”€ åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ
â”‚   â””â”€â”€ stage1_data = json.load(stage1_result_file)
â”‚
â”œâ”€â”€ åŠ è½½åŸå§‹è¾“å…¥æ•°æ®
â”‚   â””â”€â”€ input_data = json.load(input_data_file)
â”‚
â””â”€â”€ åˆ›å»ºimage_data_mapï¼ˆæ”¯æŒå­—ç¬¦ä¸²/æ•´æ•°ç±»å‹åŒ¹é…ï¼‰
```

### é˜¶æ®µ2: æ•°æ®å‡†å¤‡ï¼ˆprepare_data_for_inferenceå‡½æ•°ï¼‰

```python
prepare_data_for_inference(stage1_data, image_data_map)
â”‚
â”œâ”€â”€ è·å–per_image_top100_candidates
â”‚   â””â”€â”€ per_image_top100 = stage1_data['per_image_top100_candidates']
â”‚
â”œâ”€â”€ éå†æ¯å¼ å›¾ç‰‡
â”‚   for image_id, top100_candidates in per_image_top100.items():
â”‚   â”‚
â”‚   â”œâ”€â”€ åŒ¹é…åŸå§‹æ•°æ®ï¼ˆæ”¯æŒç±»å‹è½¬æ¢ï¼‰
â”‚   â”‚   â””â”€â”€ img_data = image_data_map[image_id]
â”‚   â”‚
â”‚   â”œâ”€â”€ åˆ›å»ºç‰©ä½“æ˜ å°„
â”‚   â”‚   â””â”€â”€ obj_dict = {obj['class_name']: obj for obj in objects}
â”‚   â”‚
â”‚   â”œâ”€â”€ æŒ‰é…å¯¹åˆ†ç»„å€™é€‰
â”‚   â”‚   â””â”€â”€ pair_candidates = group_by_pair(top100_candidates)
â”‚   â”‚
â”‚   â””â”€â”€ å¯¹æ¯ä¸ªé…å¯¹
â”‚       for (subject, object), candidates in pair_candidates.items():
â”‚       â”‚
â”‚       â”œâ”€â”€ æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–Top-K
â”‚       â”‚   â””â”€â”€ top_k_candidates = sorted(candidates)[:TOP_K_RELATIONS]
â”‚       â”‚
â”‚       â””â”€â”€ æ„å»ºpair_data
â”‚           â””â”€â”€ all_pairs.append({
â”‚                   'image_id': ...,
â”‚                   'subject': ...,
â”‚                   'object': ...,
â”‚                   'subject_obj': ...,
â”‚                   'object_obj': ...,
â”‚                   'stage1_top_k': [...],
â”‚                   ...
â”‚               })
â”‚
â””â”€â”€ è¿”å› all_pairs
```

### é˜¶æ®µ3: æ¨ç†æ¨¡å¼é€‰æ‹©

#### 3.1 å•GPUæ¨¡å¼

```python
if args.num_gpus == 1:
    â”œâ”€â”€ åŠ è½½ç”Ÿæˆæ¨¡å‹
    â”‚   â”œâ”€â”€ GenModelClass = get_generation_model_class(model_path)
    â”‚   â”œâ”€â”€ processor = AutoProcessor.from_pretrained(...)
    â”‚   â””â”€â”€ model = GenModelClass.from_pretrained(...)
    â”‚
    â”œâ”€â”€ æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
    â”‚   â””â”€â”€ processed_pairs = get_processed_pairs(existing_results)
    â”‚
    â”œâ”€â”€ è¿‡æ»¤æœªå¤„ç†é…å¯¹
    â”‚   â””â”€â”€ unprocessed_pairs = filter_unprocessed(all_pairs, processed_pairs)
    â”‚
    â””â”€â”€ é€ä¸ªå¤„ç†é…å¯¹
        for pair_data in unprocessed_pairs:
            â”œâ”€â”€ å‡†å¤‡generation_tasks
            â”‚   â””â”€â”€ ä¸ºæ¯ä¸ªstage1_top_kåˆ›å»ºtask
            â”‚
            â”œâ”€â”€ æ‰¹é‡ç”Ÿæˆ
            â”‚   â””â”€â”€ stage2_results = generate_relations_batch(...)
            â”‚
            â””â”€â”€ ä¿å­˜ç»“æœ
```

#### 3.2 å¤šGPUæ¨¡å¼

```python
else:  # å¤šGPUæ¨¡å¼
    â”œâ”€â”€ æ£€æŸ¥æœ€ç»ˆåˆå¹¶ç»“æœçš„æ–­ç‚¹ç»­ä¼ 
    â”‚   â””â”€â”€ è¿‡æ»¤å·²å¤„ç†çš„é…å¯¹
    â”‚
    â”œâ”€â”€ åˆ†å‰²æ•°æ®åˆ°GPU
    â”‚   â””â”€â”€ data_chunks = split_data(all_pairs, num_gpus)
    â”‚
    â”œâ”€â”€ å¦‚æœworkers_per_gpu > 1
    â”‚   â”œâ”€â”€ è®¡ç®—æ¯ä¸ªworkerçš„æ˜¾å­˜é™åˆ¶
    â”‚   â””â”€â”€ è¿›ä¸€æ­¥åˆ†å‰²æ•°æ®åˆ°worker
    â”‚       â””â”€â”€ worker_chunks = split_data(gpu_data, workers_per_gpu)
    â”‚
    â”œâ”€â”€ å¯åŠ¨å¤šä¸ªè¿›ç¨‹
    â”‚   for gpu_id, worker_id, chunk in zip(...):
    â”‚       â””â”€â”€ mp.Process(target=inference_on_gpu, ...)
    â”‚
    â”œâ”€â”€ ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    â”‚   â””â”€â”€ for p in processes: p.join()
    â”‚
    â””â”€â”€ åˆå¹¶ç»“æœ
        â””â”€â”€ merge_results(...)
```

### é˜¶æ®µ4: GPUæ¨ç†ï¼ˆinference_on_gpuå‡½æ•°ï¼‰

```python
inference_on_gpu(gpu_id, data_chunk, model_path, ...)
â”‚
â”œâ”€â”€ è®¾ç½®GPUè®¾å¤‡
â”‚   â””â”€â”€ torch.cuda.set_device(gpu_id)
â”‚
â”œâ”€â”€ åŠ è½½æ¨¡å‹
â”‚   â”œâ”€â”€ GenModelClass = get_generation_model_class(model_path)
â”‚   â”œâ”€â”€ config = AutoConfig.from_pretrained(...)
â”‚   â”œâ”€â”€ model = GenModelClass.from_pretrained(..., device_map=f"cuda:{gpu_id}")
â”‚   â””â”€â”€ processor = AutoProcessor.from_pretrained(...)
â”‚
â”œâ”€â”€ æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
â”‚   â”œâ”€â”€ existing_results = load_existing_results(gpu_output_path)
â”‚   â””â”€â”€ processed_pairs = get_processed_pairs(existing_results)
â”‚
â”œâ”€â”€ è¿‡æ»¤æœªå¤„ç†é…å¯¹
â”‚   â””â”€â”€ unprocessed_chunk = filter_unprocessed(data_chunk, processed_pairs)
â”‚
â””â”€â”€ å¤„ç†æ¯ä¸ªé…å¯¹
    for pair_data in tqdm(unprocessed_chunk):
        â”‚
        â”œâ”€â”€ æ‰“å¼€å›¾ç‰‡ï¼Œè·å–å°ºå¯¸
        â”‚   â””â”€â”€ original_width, original_height = Image.open(image_path).size
        â”‚
        â”œâ”€â”€ å‡†å¤‡æ‰¹é‡ç”Ÿæˆä»»åŠ¡
        â”‚   for stage1_item in pair_data['stage1_top_k']:
        â”‚       â””â”€â”€ generation_tasks.append({
        â”‚               'subject_obj': pair_data['subject_obj'],
        â”‚               'object_obj': pair_data['object_obj'],
        â”‚               'top_predicate': stage1_item['predicate'],
        â”‚               'similarity': stage1_item['similarity']
        â”‚           })
        â”‚
        â”œâ”€â”€ æ‰¹é‡ç”Ÿæˆ
        â”‚   â””â”€â”€ stage2_results = generate_relations_batch(
        â”‚           model, processor, image_path, generation_tasks,
        â”‚           original_width, original_height, batch_size
        â”‚       )
        â”‚
        â”œâ”€â”€ ä¿å­˜ç»“æœ
        â”‚   â””â”€â”€ result = {
        â”‚           'image_id': ...,
        â”‚           'subject': ...,
        â”‚           'object': ...,
        â”‚           'stage1_top_k': ...,
        â”‚           'stage2_generated': stage2_results,
        â”‚           ...
        â”‚       }
        â”‚
        â”œâ”€â”€ å®šæœŸä¿å­˜ï¼ˆæ¯SAVE_INTERVALä¸ªé…å¯¹ï¼‰
        â”‚   â””â”€â”€ ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        â”‚
        â””â”€â”€ å®šæœŸæ¸…ç†æ˜¾å­˜ï¼ˆæ¯MEMORY_CLEANUP_INTERVALä¸ªé…å¯¹ï¼‰
            â””â”€â”€ torch.cuda.empty_cache()
```

### é˜¶æ®µ5: æ‰¹é‡ç”Ÿæˆï¼ˆgenerate_relations_batchå‡½æ•°ï¼‰

```python
generate_relations_batch(model, processor, image_path, generation_tasks, ...)
â”‚
â”œâ”€â”€ å¯é€‰ï¼šé¢„å¤„ç†å›¾åƒï¼ˆå›¾åƒç¼“å­˜ä¼˜åŒ–ï¼‰
â”‚   if use_image_cache:
â”‚       â””â”€â”€ cached_pixel_values = processor(images=[image])['pixel_values']
â”‚
â”œâ”€â”€ åˆ†æ‰¹å¤„ç†
â”‚   for i in range(0, len(generation_tasks), batch_size):
â”‚       â”‚
â”‚       â”œâ”€â”€ è·å–å½“å‰batch
â”‚       â”‚   â””â”€â”€ batch = generation_tasks[i:i+batch_size]
â”‚       â”‚
â”‚       â”œâ”€â”€ æ‰¹é‡æ„å»ºprompts
â”‚       â”‚   for task in batch:
â”‚       â”‚       â”œâ”€â”€ prompt_text = build_prompt(
â”‚       â”‚       â”‚       task['subject_obj'], task['object_obj'],
â”‚       â”‚       â”‚       task['top_predicate'], ...
â”‚       â”‚       â”‚   )
â”‚       â”‚       â”œâ”€â”€ conversation = [{"role": "user", "content": [...]}]
â”‚       â”‚       â””â”€â”€ text_prompt = processor.apply_chat_template(...)
â”‚       â”‚
â”‚       â”œâ”€â”€ æ‰¹é‡å¤„ç†è¾“å…¥
â”‚       â”‚   if use_image_cache:
â”‚       â”‚       â””â”€â”€ å¤ç”¨ç¼“å­˜çš„å›¾åƒç‰¹å¾
â”‚       â”‚   else:
â”‚       â”‚       â””â”€â”€ inputs = processor(text=text_prompts, images=[image]*batch_len)
â”‚       â”‚
â”‚       â”œâ”€â”€ æ‰¹é‡ç”Ÿæˆ
â”‚       â”‚   â””â”€â”€ generated_ids = model.generate(**inputs, ...)
â”‚       â”‚
â”‚       â”œâ”€â”€ æ‰¹é‡è§£ç 
â”‚       â”‚   for gen_id in generated_ids:
â”‚       â”‚       â””â”€â”€ generated_text = processor.tokenizer.decode(...)
â”‚       â”‚
â”‚       â””â”€â”€ æ”¶é›†ç»“æœ
â”‚           â””â”€â”€ all_results.append({
â”‚                   'predicate': ...,
â”‚                   'similarity': ...,
â”‚                   'generated_description': generated_text
â”‚               })
â”‚
â””â”€â”€ è¿”å› all_results
```

---

## å…³é”®æ•°æ®ç»“æ„

### 1. pair_dataï¼ˆé…å¯¹æ•°æ®ï¼‰

```python
{
    'image_id': str/int,           # å›¾ç‰‡ID
    'image_path': str,             # å›¾ç‰‡è·¯å¾„
    'subject': str,                # ä¸»ä½“åç§°
    'object': str,                 # å®¢ä½“åç§°
    'subject_obj': {               # ä¸»ä½“å¯¹è±¡å®Œæ•´ä¿¡æ¯
        'class_name': str,
        'bbox': [x1, y1, x2, y2],
        'id': int,
        ...
    },
    'object_obj': {                # å®¢ä½“å¯¹è±¡å®Œæ•´ä¿¡æ¯
        'class_name': str,
        'bbox': [x1, y1, x2, y2],
        'id': int,
        ...
    },
    'stage1_top_k': [              # ç¬¬ä¸€é˜¶æ®µTop-Kä¸ªrelation
        {
            'predicate': str,       # è°“è¯
            'similarity': float     # ç›¸ä¼¼åº¦
        },
        ...
    ],
    'has_gt': bool,                # æ˜¯å¦æœ‰GTå…³ç³»
    'gt_predicates': [str, ...]    # GTè°“è¯åˆ—è¡¨
}
```

### 2. generation_taskï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰

```python
{
    'subject_obj': {...},          # ä¸»ä½“å¯¹è±¡ä¿¡æ¯
    'object_obj': {...},          # å®¢ä½“å¯¹è±¡ä¿¡æ¯
    'top_predicate': str,         # è°“è¯
    'similarity': float           # ç›¸ä¼¼åº¦
}
```

### 3. stage2_resultï¼ˆç”Ÿæˆç»“æœï¼‰

```python
{
    'predicate': str,                    # è°“è¯
    'similarity': float,                 # ç›¸ä¼¼åº¦
    'generated_description': str         # ç”Ÿæˆçš„è¯¦ç»†æè¿°æ–‡æœ¬
}
```

### 4. æœ€ç»ˆè¾“å‡ºç»“æœ

```python
{
    'summary': {
        'total_pairs': int,
        'total_images': int,
        'top_k_relations': int,
        'generation_max_tokens': int,
        'generation_temperature': float,
        'num_gpus': int,
        'workers_per_gpu': int
    },
    'results': [
        {
            'image_id': str/int,
            'subject': str,
            'object': str,
            'stage1_top_k': [...],
            'stage2_generated': [
                {
                    'predicate': str,
                    'similarity': float,
                    'generated_description': str
                },
                ...
            ],
            'has_gt': bool,
            'gt_predicates': [str, ...]
        },
        ...
    ]
}
```

---

## å¤šGPUå¤šWorkeræœºåˆ¶

### æ•°æ®åˆ†å‰²ç­–ç•¥

```
æ€»é…å¯¹æ•°æ® (all_pairs)
    â”‚
    â”œâ”€â”€ ç¬¬ä¸€å±‚åˆ†å‰²ï¼šæŒ‰GPUæ•°é‡
    â”‚   â”œâ”€â”€ GPU 0: data_chunks[0]
    â”‚   â”œâ”€â”€ GPU 1: data_chunks[1]
    â”‚   â”œâ”€â”€ GPU 2: data_chunks[2]
    â”‚   â””â”€â”€ GPU 3: data_chunks[3]
    â”‚
    â””â”€â”€ ç¬¬äºŒå±‚åˆ†å‰²ï¼ˆå¦‚æœworkers_per_gpu > 1ï¼‰ï¼šæŒ‰Workeræ•°é‡
        â”œâ”€â”€ GPU 0
        â”‚   â”œâ”€â”€ Worker 0: worker_chunks[0]
        â”‚   â””â”€â”€ Worker 1: worker_chunks[1]
        â”œâ”€â”€ GPU 1
        â”‚   â”œâ”€â”€ Worker 0: worker_chunks[2]
        â”‚   â””â”€â”€ Worker 1: worker_chunks[3]
        ...
```

### è¿›ç¨‹å¯åŠ¨

```python
# ä½¿ç”¨multiprocessingå¯åŠ¨å¤šä¸ªè¿›ç¨‹
for gpu_id, worker_id, chunk in zip(worker_gpu_ids, worker_ids, worker_chunks):
    p = mp.Process(
        target=inference_on_gpu,
        args=(gpu_id, chunk, model_path, output_prefix,
              shared_stats, batch_size, worker_id, max_memory_per_worker)
    )
    p.start()
    processes.append(p)

# ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
for p in processes:
    p.join()
```

### æ˜¾å­˜ç®¡ç†ï¼ˆå¤šWorkeræ¨¡å¼ï¼‰

- æ¯ä¸ªGPUçš„æ˜¾å­˜è¢«å¹³å‡åˆ†é…ç»™å¤šä¸ªworker
- æ¯ä¸ªworkerä½¿ç”¨ `max_memory` å‚æ•°é™åˆ¶æ˜¾å­˜ä½¿ç”¨
- è®¡ç®—å…¬å¼ï¼š`max_memory_per_worker = gpu_memory_mb * 0.8 / workers_per_gpu`

### ç»“æœåˆå¹¶

```python
merge_results(output_prefix, num_gpus, final_output_path, ...)
â”‚
â”œâ”€â”€ è¯»å–æ¯ä¸ªGPU/Workerçš„ç»“æœæ–‡ä»¶
â”‚   for gpu_id in range(num_gpus):
â”‚       if workers_per_gpu > 1:
â”‚           for worker_id in range(workers_per_gpu):
â”‚               â””â”€â”€ è¯»å– gpu{gpu_id}_worker{worker_id}.json
â”‚       else:
â”‚           â””â”€â”€ è¯»å– gpu{gpu_id}.json
â”‚
â”œâ”€â”€ å»é‡åˆå¹¶ï¼ˆåŸºäºimage_id, subject, objectï¼‰
â”‚   â””â”€â”€ ä½¿ç”¨processed_pairsé›†åˆé¿å…é‡å¤
â”‚
â””â”€â”€ ä¿å­˜æœ€ç»ˆç»“æœ
    â””â”€â”€ å†™å…¥ final_output_path
```

---

## æ‰¹é‡ç”Ÿæˆä¼˜åŒ–

### Batch Processingæµç¨‹

```
generation_tasks (ä¸€ä¸ªé…å¯¹çš„æ‰€æœ‰Top-K relation)
    â”‚
    â”œâ”€â”€ Batch 1: tasks[0:batch_size]
    â”‚   â”œâ”€â”€ æ„å»ºprompts
    â”‚   â”œâ”€â”€ å¤„ç†è¾“å…¥
    â”‚   â”œâ”€â”€ æ‰¹é‡ç”Ÿæˆ
    â”‚   â””â”€â”€ è§£ç ç»“æœ
    â”‚
    â”œâ”€â”€ Batch 2: tasks[batch_size:2*batch_size]
    â”‚   â””â”€â”€ ...
    â”‚
    â””â”€â”€ Batch N: tasks[(N-1)*batch_size:N*batch_size]
        â””â”€â”€ ...
```

### å›¾åƒç¼“å­˜ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

å¦‚æœ `USE_IMAGE_CACHE = True`ï¼š

1. **é¢„å¤„ç†é˜¶æ®µ**ï¼šåªå¤„ç†å›¾åƒä¸€æ¬¡ï¼Œç¼“å­˜å›¾åƒç‰¹å¾
   ```python
   cached_pixel_values = processor(images=[image])['pixel_values']
   ```

2. **æ‰¹é‡å¤„ç†é˜¶æ®µ**ï¼šåªå¤„ç†æ–‡æœ¬ï¼Œå¤ç”¨ç¼“å­˜çš„å›¾åƒç‰¹å¾
   ```python
   text_inputs = processor(text=text_prompts, images=None)
   text_inputs['pixel_values'] = cached_pixel_values.repeat(batch_len, 1, 1, 1)
   ```

**ä¼˜åŠ¿**ï¼šé¿å…é‡å¤ç¼–ç å›¾åƒï¼Œæé«˜å¤„ç†é€Ÿåº¦ï¼ˆ1.5-2å€åŠ é€Ÿï¼‰

---

## æ–­ç‚¹ç»­ä¼ æœºåˆ¶

### å•GPUæ¨¡å¼

1. **æ£€æŸ¥å·²å­˜åœ¨ç»“æœ**
   ```python
   existing_results = load_existing_results(output_path)
   processed_pairs = get_processed_pairs(existing_results)
   ```

2. **è¿‡æ»¤æœªå¤„ç†é…å¯¹**
   ```python
   unprocessed_pairs = [p for p in all_pairs 
                        if (p['image_id'], p['subject'], p['object']) 
                        not in processed_pairs]
   ```

3. **ç»§ç»­å¤„ç†**
   - åªå¤„ç†æœªå®Œæˆçš„é…å¯¹
   - ç»“æœè¿½åŠ åˆ°å·²å­˜åœ¨ç»“æœä¸­

### å¤šGPUæ¨¡å¼

1. **æ£€æŸ¥æœ€ç»ˆåˆå¹¶ç»“æœ**
   - å¦‚æœæœ€ç»ˆè¾“å‡ºæ–‡ä»¶å­˜åœ¨ï¼Œä»ä¸­æå–å·²å¤„ç†çš„é…å¯¹
   - è¿‡æ»¤æ‰å·²å¤„ç†çš„é…å¯¹

2. **æ£€æŸ¥æ¯ä¸ªGPU/Workerçš„ç»“æœ**
   - æ¯ä¸ªGPU/Workerç‹¬ç«‹æ£€æŸ¥è‡ªå·±çš„ç»“æœæ–‡ä»¶
   - åªå¤„ç†æœªå®Œæˆçš„é…å¯¹

3. **å®šæœŸä¿å­˜**
   - æ¯å¤„ç† `SAVE_INTERVAL` ä¸ªé…å¯¹ä¿å­˜ä¸€æ¬¡
   - ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­å¯¼è‡´æ•°æ®ä¸¢å¤±

### é…å¯¹å”¯ä¸€æ ‡è¯†

ä½¿ç”¨ä¸‰å…ƒç»„ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼š
```python
pair_key = (image_id, subject, object)
```

---

## Promptæ„å»º

### Promptæ ¼å¼

```
è¿™æ˜¯ä¸€å¼ å›¾ï¼Œç‰©ä½“<|object_ref_start|>{subject_name}<|object_ref_end|>å’Œ
<|object_ref_start|>{object_name}<|object_ref_end|>åœ¨ä½ç½®
({subj_center_x}, {subj_center_y})å’Œ({obj_center_x}, {obj_center_y})ï¼Œ
<|object_ref_start|>{subject_name}<|object_ref_end|>ä½äº
<|box_start|>({subject_bbox_str})<|box_end|>ï¼Œ
<|object_ref_start|>{object_name}<|object_ref_end|>ä½äº
<|box_start|>({object_bbox_str})<|box_end|>ï¼Œ
{subject_name} is roughly {top_predicate} {object_name}ï¼Œ
è¯·ç»™å‡º{subject_name}å’Œ{object_name}ç»†èŠ‚çš„ç›¸å¯¹å…³ç³»
```

### Bboxå½’ä¸€åŒ–

- å°†åŸå§‹åæ ‡å½’ä¸€åŒ–åˆ°[0, 1000)èŒƒå›´
- æ ¼å¼ï¼š`"{x1}, {y1}, {x2}, {y2}"`

---

## å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `TOP_K_RELATIONS` | 10 | æ¯ä¸ªé…å¯¹é€‰æ‹©çš„é«˜ç½®ä¿¡åº¦relationæ•°é‡ |
| `BATCH_SIZE` | 8 | æ‰¹é‡æ¨ç†çš„batch size |
| `MAX_NEW_TOKENS` | 512 | ç”Ÿæˆçš„æœ€å¤§tokenæ•° |
| `TEMPERATURE` | 0.1 | ç”Ÿæˆæ¸©åº¦ï¼ˆè¶Šä½è¶Šç¡®å®šï¼‰ |
| `SAVE_INTERVAL` | 50 | æ¯å¤„ç†å¤šå°‘ä¸ªé…å¯¹ä¿å­˜ä¸€æ¬¡ |
| `MEMORY_CLEANUP_INTERVAL` | 20 | æ¯å¤„ç†å¤šå°‘ä¸ªé…å¯¹æ¸…ç†ä¸€æ¬¡æ˜¾å­˜ |
| `USE_IMAGE_CACHE` | False | æ˜¯å¦ä½¿ç”¨å›¾åƒç¼“å­˜ä¼˜åŒ– |

---

## æ‰§è¡Œç¤ºä¾‹

### å•GPUæ¨¡å¼
```bash
python generate_relation_stage2.py \
    --num_gpus 1 \
    --batch_size 8 \
    --top_k 10
```

### å¤šGPUå¤šWorkeræ¨¡å¼
```bash
python generate_relation_stage2.py \
    --num_gpus 4 \
    --workers_per_gpu 2 \
    --batch_size 6 \
    --top_k 10
```

---

## æ€»ç»“

æ•´ä¸ªPipelineçš„æ ¸å¿ƒæµç¨‹ï¼š

1. **æ•°æ®åŠ è½½** â†’ åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœå’ŒåŸå§‹è¾“å…¥æ•°æ®
2. **æ•°æ®å‡†å¤‡** â†’ åŒ¹é…æ•°æ®ã€åˆ†ç»„ã€é€‰æ‹©Top-Kï¼Œæ„å»ºpair_data
3. **æ¨ç†æ‰§è¡Œ** â†’ å•GPUæˆ–å¤šGPUæ¨¡å¼ï¼Œæ‰¹é‡ç”Ÿæˆè¯¦ç»†æè¿°
4. **ç»“æœä¿å­˜** â†’ å®šæœŸä¿å­˜ã€æ–­ç‚¹ç»­ä¼ ã€æœ€ç»ˆåˆå¹¶

å…³é”®ä¼˜åŒ–ï¼š
- âœ… æ‰¹é‡ç”Ÿæˆï¼ˆbatch processingï¼‰
- âœ… å¤šGPUå¤šWorkerå¹¶è¡Œ
- âœ… å›¾åƒç¼“å­˜ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
- âœ… æ–­ç‚¹ç»­ä¼ 
- âœ… å®šæœŸä¿å­˜å’Œæ˜¾å­˜æ¸…ç†

