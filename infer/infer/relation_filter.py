
import json
import torch
from PIL import Image
from tqdm import tqdm
import os
import sys
import warnings
from pathlib import Path
torch.cuda.empty_cache() 
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).resolve().parent  # embedding/infer/
project_root = current_dir.parent.parent  # VLM2Vec/
sys.path.insert(0, str(project_root))


def check_flash_attention_support():

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        if not torch.cuda.is_available():
            return False, "CUDAä¸å¯ç”¨"
        
        # è·å–GPUè®¡ç®—èƒ½åŠ›
        device_capability = torch.cuda.get_device_capability()
        major, minor = device_capability
        compute_capability = major * 10 + minor
        
        # Flash Attention 2éœ€è¦è®¡ç®—èƒ½åŠ› >= 8.0 (AmpereåŠä»¥ä¸Šæ¶æ„)
        # Flash Attention 1éœ€è¦è®¡ç®—èƒ½åŠ› >= 7.5 (TuringåŠä»¥ä¸Šæ¶æ„)
        if compute_capability >= 80:
            # å°è¯•å¯¼å…¥flash_attn
            try:
                import flash_attn
                return True, f"æ”¯æŒFlash Attention (GPUè®¡ç®—èƒ½åŠ›: {major}.{minor})"
            except ImportError:
                return False, f"GPUæ”¯æŒä½†æœªå®‰è£…flash_attnåŒ… (è®¡ç®—èƒ½åŠ›: {major}.{minor})"
        else:
            return False, f"GPUè®¡ç®—èƒ½åŠ›ä¸è¶³ (å½“å‰: {major}.{minor}, éœ€è¦: >= 8.0)"
            
    except Exception as e:
        return False, f"æ£€æµ‹å¤±è´¥: {str(e)}"


def configure_attention_backend():

    is_supported, message = check_flash_attention_support()
    
    print("\n" + "="*80)
    print("æ³¨æ„åŠ›æœºåˆ¶é…ç½®")
    print("="*80)
    
    if is_supported:
        print(f"âœ… {message}")
        print("   ä½¿ç”¨: Flash Attention (æœ€å¿«)")
        os.environ["ATTN_IMPLEMENTATION"] = "flash_attention_2"
        # åŒæ—¶è®¾ç½®transformersä½¿ç”¨çš„ç¯å¢ƒå˜é‡
        os.environ["USE_FLASH_ATTENTION"] = "1"
        return "flash_attn"
    else:
        print(f"âš ï¸  {message}")
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒSDPA
        pytorch_version = torch.__version__
        major, minor = map(int, pytorch_version.split('.')[:2])
        
        if major >= 2:  # PyTorch 2.0+æ”¯æŒSDPA
            print("   é™çº§ä½¿ç”¨: Scaled Dot Product Attention (SDPA)")
            print("   æ€§èƒ½: ä¸­ç­‰ï¼Œä½†æ¯”eageræ¨¡å¼å¿«")
            os.environ["ATTN_IMPLEMENTATION"] = "sdpa"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            return "sdpa"
        else:
            print("   é™çº§ä½¿ç”¨: Eager Attention (æ ‡å‡†å®ç°)")
            print("   æ€§èƒ½: è¾ƒæ…¢ï¼Œä½†å…¼å®¹æ€§æœ€å¥½")
            os.environ["ATTN_IMPLEMENTATION"] = "eager"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            return "eager"
    
    print("="*80 + "\n")


_attn_type = configure_attention_backend()

# ç°åœ¨æ‰å¯¼å…¥VLM2Vecæ¨¡å—
from src.model.model import MMEBModel
from src.arguments import ModelArguments, DataArguments
from src.model.processor import load_processor, QWEN2_VL, VLM_IMAGE_TOKENS


INPUT_FILE = "/public/home/xiaojw2025/Workspace/RAHP/DATASET/VG150/test_2000_images.json"
OUTPUT_FILE = "/public/home/xiaojw2025/Workspace/VLM2Vec/predict/recall_results_2000_mmmeb_filter.json"

# å…³ç³»åˆ¤æ–­ç±»åˆ«ï¼ˆäºŒåˆ†ç±»ï¼šæœ‰å…³ç³» vs æ— å…³ç³»ï¼‰
RELATION_CATEGORIES = ["has_relation", "no_relation"]


def format_bbox_as_special_token(bbox, normalize=True, original_width=1024, original_height=1024):
    """å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸ºQwen2-VLçš„special tokenæ ¼å¼"""
    if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        
        if normalize:
            x1_norm = int((x1 / original_width) * 1000)
            y1_norm = int((y1 / original_height) * 1000)
            x2_norm = int((x2 / original_width) * 1000)
            y2_norm = int((y2 / original_height) * 1000)
            
            x1_norm = max(0, min(x1_norm, 999))
            y1_norm = max(0, min(y1_norm, 999))
            x2_norm = max(0, min(x2_norm, 999))
            y2_norm = max(0, min(y2_norm, 999))
            
            x1_norm, x2_norm = min(x1_norm, x2_norm), max(x1_norm, x2_norm)
            y1_norm, y2_norm = min(y1_norm, y2_norm), max(y1_norm, y2_norm)
            
            if x1_norm == x2_norm:
                x2_norm = min(x1_norm + 1, 999)
            if y1_norm == y2_norm:
                y2_norm = min(y1_norm + 1, 999)
            
            return f"<|box_start|>({x1_norm}, {y1_norm}), ({x2_norm}, {y2_norm})<|box_end|>"
    return ""

def format_object_with_ref(object_label):
    return f"<|object_ref_start|>{object_label}<|object_ref_end|>"


def precompute_relation_vectors(model, processor):
    """
    é¢„è®¡ç®—å…³ç³»ç±»åˆ«å‘é‡ï¼ˆäºŒåˆ†ç±»ï¼šæœ‰å…³ç³» vs æ— å…³ç³»ï¼‰
    
    Args:
        model: VLM2Vecæ¨¡å‹
        processor: æ–‡æœ¬å¤„ç†å™¨
    
    Returns:
        relation_vectors: dict, {'has_relation': tensor, 'no_relation': tensor}
    """
    print("ğŸ”§ é¢„è®¡ç®—å…³ç³»ç±»åˆ«å‘é‡...")
    relation_vectors = {}
    
    # æœ‰å…³ç³»çš„æè¿°
    has_relation_text = "The subject and object have a relationship."
    inputs = processor(text=has_relation_text, images=None, return_tensors="pt")
    inputs = {key: value.to('cuda') for key, value in inputs.items()}
    with torch.no_grad():
        relation_vectors['has_relation'] = model(tgt=inputs)["tgt_reps"]
    
    # æ— å…³ç³»çš„æè¿°
    no_relation_text = "The subject and object have no relationship."
    inputs = processor(text=no_relation_text, images=None, return_tensors="pt")
    inputs = {key: value.to('cuda') for key, value in inputs.items()}
    with torch.no_grad():
        relation_vectors['no_relation'] = model(tgt=inputs)["tgt_reps"]
    
    print(f"âœ… å…³ç³»ç±»åˆ«å‘é‡é¢„è®¡ç®—å®Œæˆ")
    print(f"   has_relation shape: {relation_vectors['has_relation'].shape}")
    print(f"   no_relation shape: {relation_vectors['no_relation'].shape}")
    
    return relation_vectors


def predict_relation_binary(model, processor, image_path, subject_obj, object_obj, 
                            original_width, original_height, relation_vectors=None):
    """
    äºŒåˆ†ç±»é¢„æµ‹ï¼šåˆ¤æ–­ä¸¤ä¸ªç‰©ä½“ä¹‹é—´æ˜¯å¦æœ‰å…³ç³»
    
    Args:
        relation_vectors: é¢„è®¡ç®—çš„å…³ç³»å‘é‡ dict {'has_relation': tensor, 'no_relation': tensor}
    
    Returns:
        dict: {'has_relation': similarity_score, 'no_relation': similarity_score, 'predicted_category': str}
    """
    # æ„å»ºsubjectå’Œobjectçš„ç‰¹æ®Štoken
    subj_bbox_token = format_bbox_as_special_token(
        subject_obj['bbox'], True, original_width, original_height
    )
    obj_bbox_token = format_bbox_as_special_token(
        object_obj['bbox'], True, original_width, original_height
    )
    subj_ref = format_object_with_ref(subject_obj['class_name'])
    obj_ref = format_object_with_ref(object_obj['class_name'])
    
    # ä¿®æ”¹æŸ¥è¯¢æ–‡æœ¬ï¼Œèšç„¦äºåˆ¤æ–­æ˜¯å¦æœ‰å…³ç³»
    query_text = f"{VLM_IMAGE_TOKENS[QWEN2_VL]} In the given image, the subject {subj_ref} is located at {subj_bbox_token}, the object {obj_ref} is located at {obj_bbox_token}. Do they have any relationship?"
    
    inputs = processor(
        text=query_text,
        images=Image.open(image_path),
        return_tensors="pt"
    )
    inputs = {key: value.to('cuda') for key, value in inputs.items()}
    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)
    inputs['image_grid_thw'] = inputs['image_grid_thw'].unsqueeze(0)
    
    try:
        with torch.no_grad():
            qry_output = model(qry=inputs)["qry_reps"]
    except RuntimeError as e:
        if "FlashAttention only supports Ampere" in str(e):
            raise RuntimeError(
                "æ£€æµ‹åˆ°Flash Attentionè¿è¡Œæ—¶é”™è¯¯ï¼šæ‚¨çš„GPUä¸æ”¯æŒFlash Attentionã€‚\n"
                "è¯·åœ¨è¿è¡Œè„šæœ¬å‰è®¾ç½®ç¯å¢ƒå˜é‡: export USE_FLASH_ATTENTION=0\n"
                f"åŸå§‹é”™è¯¯: {str(e)}"
            )
        else:
            raise
    
    # è®¡ç®—ä¸ä¸¤ä¸ªå…³ç³»ç±»åˆ«çš„ç›¸ä¼¼åº¦
    scores = {}
    
    if relation_vectors is not None:
        # ä½¿ç”¨é¢„è®¡ç®—çš„å…³ç³»å‘é‡
        with torch.no_grad():
            for category in RELATION_CATEGORIES:
                similarity = model.compute_similarity(
                    qry_output, 
                    relation_vectors[category]
                )
                scores[category] = similarity.item()
    else:
        # å®æ—¶è®¡ç®—ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        for category in RELATION_CATEGORIES:
            if category == "has_relation":
                text = "The subject and object have a relationship."
            else:
                text = "The subject and object have no relationship."
            
            inputs = processor(text=text, images=None, return_tensors="pt")
            inputs = {key: value.to('cuda') for key, value in inputs.items()}
            
            with torch.no_grad():
                tgt_output = model(tgt=inputs)["tgt_reps"]
                similarity = model.compute_similarity(qry_output, tgt_output)
                scores[category] = similarity.item()
    
    # åˆ¤æ–­é¢„æµ‹ç±»åˆ«ï¼ˆç›¸ä¼¼åº¦æ›´é«˜çš„ï¼‰
    predicted_category = max(scores, key=scores.get)
    
    return {
        'has_relation_similarity': scores['has_relation'],
        'no_relation_similarity': scores['no_relation'],
        'predicted_category': predicted_category,
        'confidence': scores[predicted_category]
    }


def calculate_binary_classification_metrics(image_pair_predictions):
    """
    è®¡ç®—äºŒåˆ†ç±»ï¼ˆæœ‰å…³ç³» vs æ— å…³ç³»ï¼‰çš„è¯„ä¼°æŒ‡æ ‡
    
    Args:
        image_pair_predictions: list of dicts, æ¯ä¸ªdictåŒ…å«ä¸€ä¸ªç‰©ä½“å¯¹çš„é¢„æµ‹ç»“æœ
    
    Returns:
        dict: åŒ…å«å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ç­‰æŒ‡æ ‡
    """
    true_positives = 0  # æ­£ç¡®é¢„æµ‹æœ‰å…³ç³»
    false_positives = 0  # é”™è¯¯é¢„æµ‹æœ‰å…³ç³»ï¼ˆå®é™…æ— å…³ç³»ï¼‰
    true_negatives = 0  # æ­£ç¡®é¢„æµ‹æ— å…³ç³»
    false_negatives = 0  # é”™è¯¯é¢„æµ‹æ— å…³ç³»ï¼ˆå®é™…æœ‰å…³ç³»ï¼‰
    
    total_pairs = len(image_pair_predictions)
    gt_pairs_count = 0  # å®é™…æœ‰å…³ç³»çš„é…å¯¹æ•°
    
    for pred in image_pair_predictions:
        has_gt = pred['has_gt']  # Ground Truth: æ˜¯å¦æœ‰å…³ç³»
        predicted_has_relation = (pred['predicted_category'] == 'has_relation')
        
        if has_gt:
            gt_pairs_count += 1
            if predicted_has_relation:
                true_positives += 1
            else:
                false_negatives += 1
        else:
            if predicted_has_relation:
                false_positives += 1
            else:
                true_negatives += 1
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    accuracy = (true_positives + true_negatives) / total_pairs if total_pairs > 0 else 0.0
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'true_positives': true_positives,
        'false_positives': false_positives,
        'true_negatives': true_negatives,
        'false_negatives': false_negatives,
        'total_pairs': total_pairs,
        'gt_pairs': gt_pairs_count,
        'non_gt_pairs': total_pairs - gt_pairs_count
    }


def calculate_per_image_metrics(per_image_pairs):
    """
    è®¡ç®—æ¯å¼ å›¾ç‰‡çš„äºŒåˆ†ç±»æŒ‡æ ‡å¹¶æ±‡æ€»
    
    Args:
        per_image_pairs: dict, keyä¸ºimage_id, valueä¸ºè¯¥å›¾ç‰‡çš„ç‰©ä½“å¯¹é¢„æµ‹åˆ—è¡¨
    
    Returns:
        dict: åŒ…å«æ€»ä½“å’Œå„å›¾ç‰‡çš„è¯„ä¼°æŒ‡æ ‡
    """
    per_image_results = []
    
    overall_tp = 0
    overall_fp = 0
    overall_tn = 0
    overall_fn = 0
    
    for image_id, pairs in per_image_pairs.items():
        # è®¡ç®—è¯¥å›¾ç‰‡çš„æŒ‡æ ‡
        metrics = calculate_binary_classification_metrics(pairs)
        metrics['image_id'] = image_id
        per_image_results.append(metrics)
        
        # ç´¯ç§¯æ€»ä½“ç»Ÿè®¡
        overall_tp += metrics['true_positives']
        overall_fp += metrics['false_positives']
        overall_tn += metrics['true_negatives']
        overall_fn += metrics['false_negatives']
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    total_pairs = overall_tp + overall_fp + overall_tn + overall_fn
    overall_accuracy = (overall_tp + overall_tn) / total_pairs if total_pairs > 0 else 0.0
    overall_precision = overall_tp / (overall_tp + overall_fp) if (overall_tp + overall_fp) > 0 else 0.0
    overall_recall = overall_tp / (overall_tp + overall_fn) if (overall_tp + overall_fn) > 0 else 0.0
    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0
    
    # è®¡ç®—å¹³å‡å›¾ç‰‡çº§åˆ«æŒ‡æ ‡
    avg_accuracy = sum(r['accuracy'] for r in per_image_results) / len(per_image_results) if per_image_results else 0.0
    avg_precision = sum(r['precision'] for r in per_image_results) / len(per_image_results) if per_image_results else 0.0
    avg_recall = sum(r['recall'] for r in per_image_results) / len(per_image_results) if per_image_results else 0.0
    avg_f1 = sum(r['f1_score'] for r in per_image_results) / len(per_image_results) if per_image_results else 0.0
    
    return {
        'overall_metrics': {
            'accuracy': overall_accuracy,
            'precision': overall_precision,
            'recall': overall_recall,
            'f1_score': overall_f1,
            'true_positives': overall_tp,
            'false_positives': overall_fp,
            'true_negatives': overall_tn,
            'false_negatives': overall_fn,
            'total_pairs': total_pairs
        },
        'average_per_image_metrics': {
            'accuracy': avg_accuracy,
            'precision': avg_precision,
            'recall': avg_recall,
            'f1_score': avg_f1
        },
        'total_images': len(per_image_results),
        'per_image_results': per_image_results
    }



def main():
    print("="*80)
    print("åœºæ™¯å›¾å…³ç³»äºŒåˆ†ç±»é¢„æµ‹ï¼ˆåˆ¤æ–­ç‰©ä½“å¯¹æ˜¯å¦æœ‰å…³ç³»ï¼‰")
    print("="*80)

    # åŠ è½½æ•°æ®
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {INPUT_FILE}")
    with open(INPUT_FILE, 'r') as f:
        data = json.load(f)
    
    total_images = len(data)
    total_relations = sum(len(img['relations']) for img in data)
    print(f"   åŠ è½½äº† {total_images} å¼ å›¾ç‰‡ï¼Œå…± {total_relations} ä¸ªå…³ç³»")
    
    #  åŠ è½½æ¨¡å‹
    print("\nğŸ”§ æ­£åœ¨åŠ è½½VLM2Vecæ¨¡å‹...")
    

    model_args = ModelArguments(
        model_name='/public/home/xiaojw2025/Workspace/VLM2Vec/models/qwen_vl/Qwen2-VL-2B-Instruct',
        # checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/qwen_vl/Qwen2-VL-2B-Instruct',
        checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/VLM2Vec-Qwen2VL-2B',
        # checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/final',
        pooling='last',
        normalize=True,
        model_backbone='qwen2_vl',
        lora=True  # ä½¿ç”¨ LoRA æ¨¡å‹
    )
    
    data_args = DataArguments(
        resize_min_pixels=56 * 56,
        resize_max_pixels=28 * 28 * 1280
    )
    
    processor = load_processor(model_args, data_args)
    
    # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœflash attentionå¤±è´¥åˆ™é™çº§
    try:
        model = MMEBModel.load(model_args)
        model = model.to('cuda', dtype=torch.bfloat16)
        model.eval()
        print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        error_msg = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯Flash Attentionç›¸å…³é”™è¯¯
        if ("flash" in error_msg.lower() or 
            "ampere" in error_msg.lower() or 
            "attention" in error_msg.lower() and "support" in error_msg.lower()):
            print(f"\nâš ï¸  æ¨¡å‹åŠ è½½/è¿è¡Œå¤±è´¥: {error_msg[:200]}")
            print("   æ£€æµ‹åˆ°Flash Attentionå…¼å®¹æ€§é—®é¢˜")
            print("   å°è¯•é™çº§åˆ°eageræ¨¡å¼...")
            
            # å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
            os.environ["ATTN_IMPLEMENTATION"] = "eager"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            
            # éœ€è¦é‡æ–°å¯¼å…¥æ¨¡å—ä»¥åº”ç”¨æ–°çš„ç¯å¢ƒå˜é‡
            import importlib
            import src.model.model
            importlib.reload(src.model.model)
            from src.model.model import MMEBModel as MMEBModelReloaded
            
            try:
                # é‡æ–°åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
                processor = load_processor(model_args, data_args)
                model = MMEBModelReloaded.load(model_args)
                model = model.to('cuda', dtype=torch.bfloat16)
                model.eval()
                print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (ä½¿ç”¨eageræ¨¡å¼)")
            except Exception as e2:
                print(f"\nâŒ é™çº§åä»ç„¶å¤±è´¥: {e2}")
                raise
        else:
            print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}")
            raise
    
    # 3. é¢„è®¡ç®—å…³ç³»ç±»åˆ«å‘é‡ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
    print("\nğŸš€ é¢„è®¡ç®—å…³ç³»ç±»åˆ«å‘é‡ï¼ˆåŠ é€Ÿæ¨ç†ï¼‰...\n")
    relation_vectors = precompute_relation_vectors(model, processor)
    
    # 4. æ‰¹é‡é¢„æµ‹ï¼ˆäºŒåˆ†ç±»ï¼šåˆ¤æ–­æ˜¯å¦æœ‰å…³ç³»ï¼‰
    print("\nğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹...\n")
    
    per_image_pairs = {}  # æŒ‰å›¾ç‰‡ç»„ç»‡çš„ç‰©ä½“å¯¹é¢„æµ‹ {image_id: [pair_predictions]}
    
    for img_idx, img_data in enumerate(tqdm(data, desc="å¤„ç†å›¾ç‰‡")):
        image_id = img_data['image_id']
        image_path = img_data['image_path']
        objects = img_data['objects']
        relations = img_data['relations']
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âš ï¸  è­¦å‘Š: å›¾åƒä¸å­˜åœ¨ {image_path}")
            continue
        
        # è·å–å›¾åƒå°ºå¯¸
        with Image.open(image_path) as img:
            original_width, original_height = img.size
        
        # åˆ›å»ºç‰©ä½“IDåˆ°ç‰©ä½“ä¿¡æ¯çš„æ˜ å°„
        obj_dict = {obj['id']: obj for obj in objects}
        
        # åˆ›å»ºGTå…³ç³»æ˜ å°„ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æœ‰å…³ç³»ï¼ˆä¸å…³å¿ƒå…·ä½“è°“è¯ï¼‰
        gt_relations_set = set()
        for relation in relations:
            subject_id = relation['subject_id']
            object_id = relation['object_id']
            # åªè®°å½•æœ‰å…³ç³»çš„é…å¯¹ï¼Œä¸å…³å¿ƒå…·ä½“è°“è¯
            gt_relations_set.add((subject_id, object_id))
        
        # å¯¹æ‰€æœ‰ç‰©ä½“è¿›è¡Œä¸¤ä¸¤é…å¯¹é¢„æµ‹
        image_pair_predictions = []
        object_ids = list(obj_dict.keys())
        
        for i, subject_id in enumerate(object_ids):
            for j, object_id in enumerate(object_ids):
                # è·³è¿‡è‡ªå·±ä¸è‡ªå·±é…å¯¹
                if i == j:
                    continue
                
                subject_obj = obj_dict[subject_id]
                object_obj = obj_dict[object_id]
                
                # äºŒåˆ†ç±»é¢„æµ‹ï¼šåˆ¤æ–­æ˜¯å¦æœ‰å…³ç³»
                prediction = predict_relation_binary(
                    model, processor, image_path,
                    subject_obj, object_obj,
                    original_width, original_height,
                    relation_vectors=relation_vectors
                )
                
                # åˆ¤æ–­è¯¥é…å¯¹æ˜¯å¦æœ‰GTå…³ç³»ï¼ˆä¸å…³å¿ƒå…·ä½“è°“è¯ï¼‰
                has_gt = (subject_id, object_id) in gt_relations_set
                
                # ä¿å­˜è¯¥ç‰©ä½“å¯¹çš„é¢„æµ‹ç»“æœ
                image_pair_predictions.append({
                    'image_id': image_id,
                    'subject_id': subject_id,
                    'object_id': object_id,
                    'subject': subject_obj['class_name'],
                    'object': object_obj['class_name'],
                    'has_gt': has_gt,  # Ground Truth: æ˜¯å¦æœ‰å…³ç³»
                    'predicted_category': prediction['predicted_category'],  # has_relation æˆ– no_relation
                    'has_relation_similarity': prediction['has_relation_similarity'],
                    'no_relation_similarity': prediction['no_relation_similarity'],
                    'confidence': prediction['confidence']
                })
        
        # ä¿å­˜è¯¥å›¾ç‰‡çš„æ‰€æœ‰ç‰©ä½“å¯¹é¢„æµ‹
        per_image_pairs[image_id] = image_pair_predictions
    
    print(f"\nâœ… é¢„æµ‹å®Œæˆï¼")
    print(f"   æ€»å›¾ç‰‡æ•°: {len(per_image_pairs)}")
    
    # ç»Ÿè®¡é…å¯¹ä¿¡æ¯
    total_pairs = sum(len(pairs) for pairs in per_image_pairs.values())
    total_gt_pairs = sum(sum(1 for p in pairs if p['has_gt']) for pairs in per_image_pairs.values())
    
    print(f"   æ€»ç‰©ä½“å¯¹æ•°: {total_pairs}")
    print(f"   æœ‰GTå…³ç³»çš„é…å¯¹æ•°: {total_gt_pairs}")
    print(f"   æ— GTå…³ç³»çš„é…å¯¹æ•°: {total_pairs - total_gt_pairs}")
    
    # 5. è®¡ç®—äºŒåˆ†ç±»è¯„ä¼°æŒ‡æ ‡
    print("\nğŸ“Š è®¡ç®—äºŒåˆ†ç±»è¯„ä¼°æŒ‡æ ‡...")
    metrics_results = calculate_per_image_metrics(per_image_pairs)
    
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœ - æ€»ä½“æŒ‡æ ‡ï¼ˆæ‰€æœ‰ç‰©ä½“å¯¹ï¼‰")
    print("="*80)
    overall = metrics_results['overall_metrics']
    print(f"å‡†ç¡®ç‡ (Accuracy):  {overall['accuracy']:.4f} ({overall['accuracy']*100:.2f}%)")
    print(f"ç²¾ç¡®ç‡ (Precision): {overall['precision']:.4f} ({overall['precision']*100:.2f}%)")
    print(f"å¬å›ç‡ (Recall):    {overall['recall']:.4f} ({overall['recall']*100:.2f}%)")
    print(f"F1åˆ†æ•° (F1-Score):  {overall['f1_score']:.4f} ({overall['f1_score']*100:.2f}%)")
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(f"  çœŸæ­£ä¾‹ (TP): {overall['true_positives']:6d}  (æ­£ç¡®è¯†åˆ«æœ‰å…³ç³»)")
    print(f"  å‡æ­£ä¾‹ (FP): {overall['false_positives']:6d}  (é”™è¯¯è¯†åˆ«æœ‰å…³ç³»)")
    print(f"  çœŸè´Ÿä¾‹ (TN): {overall['true_negatives']:6d}  (æ­£ç¡®è¯†åˆ«æ— å…³ç³»)")
    print(f"  å‡è´Ÿä¾‹ (FN): {overall['false_negatives']:6d}  (é”™è¯¯è¯†åˆ«æ— å…³ç³»)")
    print(f"  æ€»é…å¯¹æ•°:    {overall['total_pairs']:6d}")
    print("="*80)
    
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœ - å¹³å‡å›¾ç‰‡çº§åˆ«æŒ‡æ ‡")
    print("="*80)
    avg = metrics_results['average_per_image_metrics']
    print(f"å¹³å‡å‡†ç¡®ç‡:  {avg['accuracy']:.4f} ({avg['accuracy']*100:.2f}%)")
    print(f"å¹³å‡ç²¾ç¡®ç‡:  {avg['precision']:.4f} ({avg['precision']*100:.2f}%)")
    print(f"å¹³å‡å¬å›ç‡:  {avg['recall']:.4f} ({avg['recall']*100:.2f}%)")
    print(f"å¹³å‡F1åˆ†æ•°:  {avg['f1_score']:.4f} ({avg['f1_score']*100:.2f}%)")
    print(f"æ€»å›¾ç‰‡æ•°:    {metrics_results['total_images']}")
    print("="*80)
    
    # 6. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {OUTPUT_FILE}")
    output_data = {
        'summary': {
            'evaluation_method': 'binary_classification',  # äºŒåˆ†ç±»æ–¹æ³•
            'task': 'relation_detection',  # å…³ç³»æ£€æµ‹ï¼ˆä¸æ˜¯è°“è¯åˆ†ç±»ï¼‰
            'total_images': len(per_image_pairs),
            'total_pairs': total_pairs,
            'total_gt_pairs': total_gt_pairs,
            'total_non_gt_pairs': total_pairs - total_gt_pairs,
            'avg_pairs_per_image': total_pairs / len(per_image_pairs) if len(per_image_pairs) > 0 else 0,
            'avg_gt_pairs_per_image': total_gt_pairs / len(per_image_pairs) if len(per_image_pairs) > 0 else 0
        },
        'overall_metrics': metrics_results['overall_metrics'],
        'average_per_image_metrics': metrics_results['average_per_image_metrics'],
        'per_image_results': metrics_results['per_image_results'],
        # æ³¨æ„ï¼šä¸ä¿å­˜æ‰€æœ‰ç‰©ä½“å¯¹çš„é¢„æµ‹ç»†èŠ‚ï¼ˆå¤ªå¤§ï¼‰ï¼Œåªä¿å­˜æ±‡æ€»ç»Ÿè®¡
    }
    
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print("âœ… ç»“æœå·²ä¿å­˜ï¼")
    
    # 7. æ˜¾ç¤ºä¸€äº›æ ·ä¾‹å›¾ç‰‡
    print("\n" + "="*80)
    print("å›¾ç‰‡çº§åˆ«è¯„ä¼°æ ·ä¾‹ï¼ˆå‰5å¼ å›¾ç‰‡ï¼‰")
    print("="*80)
    for i, img_result in enumerate(metrics_results['per_image_results'][:5], 1):
        print(f"\n{i}. å›¾ç‰‡ #{img_result['image_id']}")
        print(f"   å‡†ç¡®ç‡: {img_result['accuracy']:.4f}, ç²¾ç¡®ç‡: {img_result['precision']:.4f}, "
              f"å¬å›ç‡: {img_result['recall']:.4f}, F1: {img_result['f1_score']:.4f}")
        print(f"   TP: {img_result['true_positives']}, FP: {img_result['false_positives']}, "
              f"TN: {img_result['true_negatives']}, FN: {img_result['false_negatives']}")
        print(f"   æ€»é…å¯¹: {img_result['total_pairs']}, æœ‰GT: {img_result['gt_pairs']}, "
              f"æ— GT: {img_result['non_gt_pairs']}")
    
    # 8. æ˜¾ç¤ºä¸€äº›é¢„æµ‹æ ·ä¾‹
    print("\n" + "="*80)
    print("é¢„æµ‹æ ·ä¾‹ï¼ˆå‰10å¯¹ï¼ŒæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰")
    print("="*80)
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹ï¼ŒæŒ‰ç½®ä¿¡åº¦æ’åº
    all_predictions = []
    for pairs in per_image_pairs.values():
        all_predictions.extend(pairs)
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    sorted_predictions = sorted(all_predictions, key=lambda x: x['confidence'], reverse=True)
    
    for i, pred in enumerate(sorted_predictions[:10], 1):
        gt_label = "æœ‰å…³ç³»" if pred['has_gt'] else "æ— å…³ç³»"
        pred_label = "æœ‰å…³ç³»" if pred['predicted_category'] == 'has_relation' else "æ— å…³ç³»"
        status = "âœ…" if (pred['has_gt'] == (pred['predicted_category'] == 'has_relation')) else "âŒ"
        
        print(f"\n{i}. {status} å›¾ç‰‡#{pred['image_id']}: {pred['subject']} <-> {pred['object']}")
        print(f"   GT: {gt_label} | é¢„æµ‹: {pred_label} (ç½®ä¿¡åº¦: {pred['confidence']:.4f})")
        print(f"   æœ‰å…³ç³»ç›¸ä¼¼åº¦: {pred['has_relation_similarity']:.4f}")
        print(f"   æ— å…³ç³»ç›¸ä¼¼åº¦: {pred['no_relation_similarity']:.4f}")


if __name__ == "__main__":
    main()

