import json
import torch
from PIL import Image
from tqdm import tqdm
import os
import sys
import warnings
from multiprocessing import Process, Queue, Manager
import math
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥srcæ¨¡å—
# è„šæœ¬ä½äº embedding/infer/ ç›®å½•ä¸‹ï¼Œéœ€è¦å‘ä¸Šä¸¤çº§æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)


def check_flash_attention_support():

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        if not torch.cuda.is_available():
            return False, "CUDAä¸å¯ç”¨"
        
        # è·å–GPUè®¡ç®—èƒ½åŠ›
        device_capability = torch.cuda.get_device_capability()
        major, minor = device_capability
        compute_capability = major * 10 + minor
        
        # Flash Attention 2éœ€è¦è®¡ç®—èƒ½åŠ› >= 8.0 (AmpereåŠä»¥ä¸Šæ¶æ„)
        # Flash Attention 1éœ€è¦è®¡ç®—èƒ½åŠ› >= 7.5 (TuringåŠä»¥ä¸Šæ¶æ„)
        if compute_capability >= 80:
            # å°è¯•å¯¼å…¥flash_attn
            try:
                import flash_attn
                return True, f"æ”¯æŒFlash Attention (GPUè®¡ç®—èƒ½åŠ›: {major}.{minor})"
            except ImportError:
                return False, f"GPUæ”¯æŒä½†æœªå®‰è£…flash_attnåŒ… (è®¡ç®—èƒ½åŠ›: {major}.{minor})"
        else:
            return False, f"GPUè®¡ç®—èƒ½åŠ›ä¸è¶³ (å½“å‰: {major}.{minor}, éœ€è¦: >= 8.0)"
            
    except Exception as e:
        return False, f"æ£€æµ‹å¤±è´¥: {str(e)}"


def configure_attention_backend():

    is_supported, message = check_flash_attention_support()
    
    print("\n" + "="*80)
    print("æ³¨æ„åŠ›æœºåˆ¶é…ç½®")
    print("="*80)
    
    if is_supported:
        print(f"âœ… {message}")
        print("   ä½¿ç”¨: Flash Attention (æœ€å¿«)")
        os.environ["ATTN_IMPLEMENTATION"] = "flash_attention_2"
        # åŒæ—¶è®¾ç½®transformersä½¿ç”¨çš„ç¯å¢ƒå˜é‡
        os.environ["USE_FLASH_ATTENTION"] = "1"
        return "flash_attn"
    else:
        print(f"âš ï¸  {message}")
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒSDPA
        pytorch_version = torch.__version__
        major, minor = map(int, pytorch_version.split('.')[:2])
        
        if major >= 2:  # PyTorch 2.0+æ”¯æŒSDPA
            print("   é™çº§ä½¿ç”¨: Scaled Dot Product Attention (SDPA)")
            print("   æ€§èƒ½: ä¸­ç­‰ï¼Œä½†æ¯”eageræ¨¡å¼å¿«")
            os.environ["ATTN_IMPLEMENTATION"] = "sdpa"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            return "sdpa"
        else:
            print("   é™çº§ä½¿ç”¨: Eager Attention (æ ‡å‡†å®ç°)")
            print("   æ€§èƒ½: è¾ƒæ…¢ï¼Œä½†å…¼å®¹æ€§æœ€å¥½")
            os.environ["ATTN_IMPLEMENTATION"] = "eager"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            return "eager"
    
    print("="*80 + "\n")


_attn_type = configure_attention_backend()

# ç°åœ¨æ‰å¯¼å…¥VLM2Vecæ¨¡å—
from src.model.model import MMEBModel
from src.arguments import ModelArguments, DataArguments
from src.model.processor import load_processor, QWEN2_VL, VLM_IMAGE_TOKENS


# Stage2è¾“å‡ºæ–‡ä»¶ï¼ˆåŒ…å«CoTæ•°æ®ï¼‰
STAGE2_OUTPUT_FILE = "/public/home/xiaojw2025/Data/stage2/stage2_generated_results.json"
INPUT_FILE = "/public/home/xiaojw2025/Workspace/RAHP/DATASET/VG150/test_2000_images.json"
OUTPUT_FILE = "/public/home/xiaojw2025/Data/stage3/recall_results_2000_stage3.json"

# é»˜è®¤ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨GPUï¼‰
# ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•° --num_gpus æˆ–ç¯å¢ƒå˜é‡ NUM_GPUS æŒ‡å®š
NUM_GPUS = None  # è®¾ç½®ä¸º None ä½¿ç”¨æ‰€æœ‰GPUï¼Œæˆ–è®¾ç½®ä¸ºå…·ä½“æ•°å­—å¦‚ 2 è¡¨ç¤ºåªä½¿ç”¨2ä¸ªGPU

# 50ä¸ªè°“è¯åˆ—è¡¨
PREDICATES = [
    "above", "across", "against", "along", "and", "at", "attached to", "behind",
    "belonging to", "between", "carrying", "covered in", "covering", "eating",
    "flying in", "for", "from", "growing on", "hanging from", "has", "holding",
    "in", "in front of", "laying on", "looking at", "lying on", "made of",
    "mounted on", "near", "of", "on", "on back of", "over", "painted on",
    "parked on", "part of", "playing", "riding", "says", "sitting on",
    "standing on", "to", "under", "using", "walking in", "walking on",
    "watching", "wearing", "wears", "with"
]


def format_bbox_as_special_token(bbox, normalize=True, original_width=1024, original_height=1024):
    """å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸ºQwen2-VLçš„special tokenæ ¼å¼"""
    if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        
        if normalize:
            x1_norm = int((x1 / original_width) * 1000)
            y1_norm = int((y1 / original_height) * 1000)
            x2_norm = int((x2 / original_width) * 1000)
            y2_norm = int((y2 / original_height) * 1000)
            
            x1_norm = max(0, min(x1_norm, 999))
            y1_norm = max(0, min(y1_norm, 999))
            x2_norm = max(0, min(x2_norm, 999))
            y2_norm = max(0, min(y2_norm, 999))
            
            x1_norm, x2_norm = min(x1_norm, x2_norm), max(x1_norm, x2_norm)
            y1_norm, y2_norm = min(y1_norm, y2_norm), max(y1_norm, y2_norm)
            
            if x1_norm == x2_norm:
                x2_norm = min(x1_norm + 1, 999)
            if y1_norm == y2_norm:
                y2_norm = min(y1_norm + 1, 999)
            
            return f"<|box_start|>({x1_norm}, {y1_norm}), ({x2_norm}, {y2_norm})<|box_end|>"
    return ""

def format_object_with_ref(object_label):
    return f"<|object_ref_start|>{object_label}<|object_ref_end|>"


def load_stage2_cot_data(stage2_file):
    """
    åŠ è½½stage2çš„è¾“å‡ºæ–‡ä»¶ï¼Œå»ºç«‹(image_id, subject_id, object_id) -> CoTæè¿°çš„æ˜ å°„
    åŒæ—¶æ”¯æŒä½¿ç”¨ç±»åˆ«åçš„å‘åå…¼å®¹æ ¼å¼
    
    Returns:
        dict: {(image_id, subject_id, object_id): cot_description} æˆ– {(image_id, subject, object): cot_description}
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½Stage2 CoTæ•°æ®: {stage2_file}")
    with open(stage2_file, 'r', encoding='utf-8') as f:
        stage2_data = json.load(f)
    
    cot_map = {}
    results = stage2_data.get('results', [])
    
    for result in results:
        image_id = result['image_id']
        subject_id = result.get('subject_id', None)
        object_id = result.get('object_id', None)
        subject = result['subject'].strip()  # å»é™¤å¯èƒ½çš„ç©ºæ ¼
        object_name = result['object'].strip()  # å»é™¤å¯èƒ½çš„ç©ºæ ¼
        cot_description = result.get('stage2_generated_description', '')
        
        # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDåˆ›å»ºkeyï¼ˆåŒºåˆ†åŒåç‰©ä½“ï¼‰
        if subject_id is not None and object_id is not None:
            # ä½¿ç”¨ç‰©ä½“IDä½œä¸ºkey
            key_formats = [
                (str(image_id), subject_id, object_id),  # å­—ç¬¦ä¸²image_id
                (image_id, subject_id, object_id),  # åŸå§‹image_idæ ¼å¼
            ]
            # å¦‚æœimage_idæ˜¯å­—ç¬¦ä¸²ä¸”å¯è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¹Ÿæ·»åŠ æ•´æ•°key
            if isinstance(image_id, str) and image_id.isdigit():
                key_formats.append((int(image_id), subject_id, object_id))
            # å¦‚æœimage_idæ˜¯æ•´æ•°ï¼Œä¹Ÿæ·»åŠ å­—ç¬¦ä¸²keyï¼ˆä¸Šé¢å·²æ·»åŠ ï¼‰
            
            for key_format in key_formats:
                cot_map[key_format] = cot_description
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨ç±»åˆ«åä½œä¸ºkey
            key_formats = [
                (str(image_id), subject, object_name),  # å­—ç¬¦ä¸²image_id
                (image_id, subject, object_name),  # åŸå§‹image_idæ ¼å¼
            ]
            if isinstance(image_id, str) and image_id.isdigit():
                key_formats.append((int(image_id), subject, object_name))
            
            for key_format in key_formats:
                cot_map[key_format] = cot_description
    
    unique_entries = len(results)
    total_keys = len(cot_map)
    print(f"   âœ“ åŠ è½½äº† {unique_entries} ä¸ªCoTæè¿°ï¼ˆå…± {total_keys} ä¸ªkeyæ ¼å¼ï¼‰")
    return cot_map


def precompute_predicate_vectors(model, processor, predicates, device='cuda', progress_queue=None, gpu_id=None):
    """
    é¢„è®¡ç®—æ‰€æœ‰è°“è¯çš„å‘é‡è¡¨ç¤ºï¼ˆåªéœ€è¦è®¡ç®—ä¸€æ¬¡ï¼‰
    
    Args:
        model: VLM2Vecæ¨¡å‹
        processor: æ–‡æœ¬å¤„ç†å™¨
        predicates: è°“è¯åˆ—è¡¨
        device: è®¾å¤‡åç§°ï¼Œå¦‚ 'cuda:0'
        progress_queue: è¿›åº¦é˜Ÿåˆ—ï¼ˆå¯é€‰ï¼Œç”¨äºå¤šè¿›ç¨‹ç¯å¢ƒï¼‰
        gpu_id: GPU IDï¼ˆå¯é€‰ï¼Œç”¨äºå¤šè¿›ç¨‹ç¯å¢ƒï¼‰
    
    Returns:
        predicate_vectors: [num_predicates, hidden_dim] çš„tensor
    """
    if progress_queue is not None and gpu_id is not None:
        progress_queue.put((gpu_id, f"GPU{gpu_id}: å¼€å§‹é¢„è®¡ç®—è°“è¯å‘é‡..."))
    
    predicate_vectors = []
    
    # åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œä¸ä½¿ç”¨tqdmï¼ˆè¾“å‡ºä¼šè¢«ç¼“å†²ï¼‰ï¼Œæ”¹ç”¨ç®€å•æ‰“å°
    for idx, predicate in enumerate(predicates):
        predicate_text = f"The subject is {predicate} the object."
        inputs = processor(text=predicate_text, images=None, return_tensors="pt")
        inputs = {key: value.to(device) for key, value in inputs.items()}
        
        with torch.no_grad():
            tgt_output = model(tgt=inputs)["tgt_reps"]
            predicate_vectors.append(tgt_output)
        
        # æ¯10ä¸ªè°“è¯æ›´æ–°ä¸€æ¬¡è¿›åº¦
        if (idx + 1) % 10 == 0 and progress_queue is not None and gpu_id is not None:
            progress_queue.put((gpu_id, f"GPU{gpu_id}: é¢„è®¡ç®—è°“è¯å‘é‡è¿›åº¦: {idx + 1}/{len(predicates)}"))
    
    # å †å æˆä¸€ä¸ªtensor: [num_predicates, hidden_dim]
    predicate_vectors = torch.cat(predicate_vectors, dim=0)
    
    if progress_queue is not None and gpu_id is not None:
        progress_queue.put((gpu_id, f"GPU{gpu_id}: è°“è¯å‘é‡é¢„è®¡ç®—å®Œæˆï¼Œshape: {predicate_vectors.shape}"))
    
    return predicate_vectors


# å…¨å±€æ ‡å¿—ï¼Œç”¨äºè·Ÿè¸ªæ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡æ¨ç†
_first_inference_printed = False

def predict_relation(model, processor, image_path, subject_obj, object_obj, 
                     original_width, original_height, cot_description, predicate_vectors=None, device='cuda',
                     use_original_query=False, use_image=False):
    """
    é¢„æµ‹å…³ç³»ï¼Œä½¿ç”¨stage2çš„CoTæè¿°ä»£æ›¿åŸå§‹query
    
    Args:
        model: VLM2Vecæ¨¡å‹
        processor: æ–‡æœ¬å¤„ç†å™¨
        image_path: å›¾ç‰‡è·¯å¾„
        subject_obj: ä¸»ä½“å¯¹è±¡ä¿¡æ¯
        object_obj: å®¢ä½“å¯¹è±¡ä¿¡æ¯
        original_width: å›¾ç‰‡å®½åº¦
        original_height: å›¾ç‰‡é«˜åº¦
        cot_description: stage2ç”Ÿæˆçš„CoTæè¿°æ–‡æœ¬
        predicate_vectors: é¢„è®¡ç®—çš„è°“è¯å‘é‡ [num_predicates, hidden_dim]ï¼Œå¦‚æœä¸ºNoneåˆ™å®æ—¶è®¡ç®—
        device: è®¾å¤‡åç§°ï¼Œå¦‚ 'cuda:0'
        use_original_query: æ˜¯å¦åœ¨cot_descriptionå‰åŠ ä¸ŠåŸå§‹query
        use_image: æ˜¯å¦è°ƒç”¨å›¾åƒ
    """
    global _first_inference_printed
    
    # åªä½¿ç”¨CoTæè¿°ï¼Œå¦‚æœä¸ºç©ºåˆ™è¿”å›Noneï¼ˆè·³è¿‡è¯¥æ ·æœ¬ï¼‰
    if not cot_description or not cot_description.strip():
        return None
    
    # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
    query_text = cot_description.strip()
    
    # å¦‚æœéœ€è¦æ·»åŠ åŸå§‹query
    if use_original_query:
        # æ„å»ºåŸå§‹query
        subj_ref = format_object_with_ref(subject_obj['class_name'])
        obj_ref = format_object_with_ref(object_obj['class_name'])
        subj_bbox_token = format_bbox_as_special_token(
            subject_obj['bbox'], 
            normalize=True, 
            original_width=original_width, 
            original_height=original_height
        )
        obj_bbox_token = format_bbox_as_special_token(
            object_obj['bbox'], 
            normalize=True, 
            original_width=original_width, 
            original_height=original_height
        )
        # æ„å»ºåæ ‡ä¿¡æ¯å‰ç¼€ï¼ˆä¸predict_scene_graph_recall_stage3.pyä¿æŒä¸€è‡´ï¼‰
        coordinate_prefix = f"In the given image, the subject {subj_ref} is located at {subj_bbox_token},the object{obj_ref} is located at {obj_bbox_token}. Please describe the predicate relationship between the subject and the object as the subject is *predicate* the object.Besides,"
        
        # å¦‚æœä½¿ç”¨å›¾åƒï¼Œæ·»åŠ å›¾åƒtokenï¼ˆå›¾åƒtokenåœ¨æœ€å‰é¢ï¼‰
        image_token = ""
        if use_image:
            image_token = VLM_IMAGE_TOKENS[QWEN2_VL]
            if image_token:
                image_token = f"{image_token} "
        
        # æ‹¼æ¥ï¼šå›¾åƒtoken + åæ ‡ä¿¡æ¯ + CoTæè¿°ï¼ˆç›´æ¥æ‹¼æ¥ï¼Œä¸ä½¿ç”¨æ¢è¡Œç¬¦ï¼‰
        query_text = f"{image_token}{coordinate_prefix}{query_text}"
    
    # å†³å®šæ˜¯å¦åŠ è½½å›¾åƒ
    image = None
    if use_image:
        # å¦‚æœä½¿ç”¨å›¾åƒï¼Œä½†æ–‡æœ¬ä¸­æ²¡æœ‰å›¾åƒtokenï¼ˆä¸”use_original_query=Falseï¼‰ï¼Œéœ€è¦æ·»åŠ å›¾åƒtoken
        if not use_original_query:
            image_token = VLM_IMAGE_TOKENS[QWEN2_VL]
            if image_token and image_token not in query_text:
                query_text = f"{image_token} {query_text}"
        
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•åŠ è½½å›¾åƒ {image_path}: {str(e)}")
            image = None
    
    inputs = processor(
        text=query_text,
        images=image,  # æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦ä¼ å…¥å›¾ç‰‡
        return_tensors="pt"
    )
    inputs = {key: value.to(device) for key, value in inputs.items()}
    
    # éªŒè¯è¾“å…¥åºåˆ—é•¿åº¦æ˜¯å¦å¤§äº0
    if 'input_ids' in inputs:
        seq_len = inputs['input_ids'].shape[-1]
        if seq_len == 0:
            # å¦‚æœåºåˆ—é•¿åº¦ä¸º0ï¼Œä¹Ÿè·³è¿‡è¯¥æ ·æœ¬
            return None
    
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œè¿™äº›å­—æ®µå¯èƒ½ä¸å­˜åœ¨ï¼Œéœ€è¦æ£€æŸ¥
    if 'pixel_values' in inputs:
        inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)
    if 'image_grid_thw' in inputs:
        inputs['image_grid_thw'] = inputs['image_grid_thw'].unsqueeze(0)
    
    try:
        with torch.no_grad():
            qry_output = model(qry=inputs)["qry_reps"]
            
            # æ‰“å°ç¬¬ä¸€æ¡æ¨ç†çš„è¾“å…¥å’Œä¸­é—´è¾“å‡º
            if not _first_inference_printed:
                print("\n" + "="*80)
                print("ç¬¬ä¸€æ¡æ¨ç†çš„è¾“å…¥å’Œè¾“å‡º")
                print("="*80)
                print(f"\nã€è¾“å…¥ä¿¡æ¯ã€‘")
                print(f"  image_path: {image_path}")
                print(f"  subject: {subject_obj['class_name']} (bbox: {subject_obj['bbox']})")
                print(f"  object: {object_obj['class_name']} (bbox: {object_obj['bbox']})")
                print(f"  image_size: {original_width}x{original_height}")
                print(f"\nã€CoTæè¿°æ–‡æœ¬ã€‘")
                print(f"  {query_text[:500]}..." if len(query_text) > 500 else f"  {query_text}")
                print(f"\nã€è¾“å…¥tensorä¿¡æ¯ã€‘")
                for key, value in inputs.items():
                    if isinstance(value, torch.Tensor):
                        print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
                    else:
                        print(f"  {key}: {value}")
                print(f"\nã€ä¸­é—´è¾“å‡ºï¼ˆqry_outputï¼‰ã€‘")
                print(f"  qry_output shape: {qry_output.shape}")
                print(f"  qry_output dtype: {qry_output.dtype}")
                print(f"  qry_output sample (first 10 values): {qry_output[0, :10].cpu().tolist()}")
                print("="*80 + "\n")
    except RuntimeError as e:
        # æ•è·Flash Attentionè¿è¡Œæ—¶é”™è¯¯
        if "FlashAttention only supports Ampere" in str(e):
            raise RuntimeError(
                "æ£€æµ‹åˆ°Flash Attentionè¿è¡Œæ—¶é”™è¯¯ï¼šæ‚¨çš„GPUä¸æ”¯æŒFlash Attentionã€‚\n"
                "è¯·åœ¨è¿è¡Œè„šæœ¬å‰è®¾ç½®ç¯å¢ƒå˜é‡: export USE_FLASH_ATTENTION=0\n"
                f"åŸå§‹é”™è¯¯: {str(e)}"
            )
        else:
            raise
    
    # è®¡ç®—ä¸æ‰€æœ‰è°“è¯çš„ç›¸ä¼¼åº¦
    predicate_scores = []
    
    if predicate_vectors is not None:
        # ä½¿ç”¨é¢„è®¡ç®—çš„è°“è¯å‘é‡ï¼ˆé€ä¸ªè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        with torch.no_grad():
            # qry_output: [1, hidden_dim]
            # predicate_vectors: [num_predicates, hidden_dim]
            for i, predicate in enumerate(PREDICATES):
                similarity = model.compute_similarity(
                    qry_output, 
                    predicate_vectors[i:i+1]  # å–å•ä¸ªè°“è¯å‘é‡ [1, hidden_dim]
                )
                predicate_scores.append({
                    'predicate': predicate,
                    'similarity': similarity.item()
                })
    else:
        # åŸå§‹æ–¹æ³•ï¼šé€ä¸ªç¼–ç è°“è¯ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        for predicate in PREDICATES:
            inputs = processor(text=predicate, images=None, return_tensors="pt")
            inputs = {key: value.to(device) for key, value in inputs.items()}
            
            with torch.no_grad():
                tgt_output = model(tgt=inputs)["tgt_reps"]
                similarity = model.compute_similarity(qry_output, tgt_output)
            
            predicate_scores.append({
                'predicate': predicate,
                'similarity': similarity.item()
            })
    
    # æ‰“å°ç¬¬ä¸€æ¡æ¨ç†çš„æœ€ç»ˆè¾“å‡ºï¼ˆpredicate_scoresï¼‰
    if not _first_inference_printed:
        print("\n" + "="*80)
        print("ç¬¬ä¸€æ¡æ¨ç†çš„æœ€ç»ˆè¾“å‡ºï¼ˆTop-10è°“è¯ï¼‰")
        print("="*80)
        sorted_scores = sorted(predicate_scores, key=lambda x: x['similarity'], reverse=True)
        for i, score in enumerate(sorted_scores[:10], 1):
            print(f"  {i:2d}. {score['predicate']:20s}: {score['similarity']:.6f}")
        print("="*80 + "\n")
        _first_inference_printed = True  # è®¾ç½®æ ‡å¿—ï¼Œé¿å…åç»­é‡å¤æ‰“å°
    
    return predicate_scores


def calculate_recall_at_k_per_image(image_candidate_predictions, k=50):
    """
    è®¡ç®—å•å¼ å›¾ç‰‡çš„recall@k
    ç°åœ¨æ”¯æŒæ‰€æœ‰ç‰©ä½“ä¸¤ä¸¤é…å¯¹çš„é¢„æµ‹ç»“æœ
    
    ä¿®æ”¹ï¼šå…ˆè¿‡æ»¤no relationï¼Œå†å–top-kï¼ˆä¸evaluate_results.pyå¯¹é½ï¼‰
    """
    # è·å–è¯¥å›¾ç‰‡ä¸­æ‰€æœ‰GTå…³ç³»ï¼ˆåªç»Ÿè®¡relation_idx >= 0çš„ï¼Œæ’é™¤-1ï¼‰
    gt_relations = set()
    for pred in image_candidate_predictions:
        if pred['has_gt'] and pred['relation_idx'] >= 0:
            gt_relations.add(pred['relation_idx'])
    
    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰no relationçš„é¢„æµ‹ï¼ˆä»æ‰€æœ‰å€™é€‰ä¸­ï¼‰
    non_bg_candidates = []
    for pred in image_candidate_predictions:
        if pred.get('predicted_predicate') != 'no relation':
            non_bg_candidates.append(pred)
    
    # ç¬¬äºŒæ­¥ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
    predictions_sorted = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
    actual_k = min(k, len(predictions_sorted))  # å¦‚æœå€™é€‰æ•°ä¸è¶³kï¼Œå–å…¨éƒ¨
    top_k_predictions = predictions_sorted[:actual_k]
    
    # ç¬¬ä¸‰æ­¥ï¼šåœ¨top-kä¸­ï¼Œåªå¯¹GTå…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼Œç»Ÿè®¡å¬å›çš„å…³ç³»ï¼ˆå»é‡ï¼‰
    recalled_relations = set()
    for pred in top_k_predictions:
        # åªç»Ÿè®¡GTå…³ç³»å¯¹ä¸­é¢„æµ‹æ­£ç¡®çš„
        if pred['relation_idx'] in gt_relations and pred['is_correct']:
            recalled_relations.add(pred['relation_idx'])
    
    # æ€»GTå…³ç³»æ•°
    total_gt_relations = len(gt_relations)
    
    recall = len(recalled_relations) / total_gt_relations if total_gt_relations > 0 else 0.0
    
    # ç»Ÿè®¡æ€»é¢„æµ‹å¯¹æ•°ï¼ˆåŒ…æ‹¬æ— GTçš„é…å¯¹ï¼‰
    total_pairs = len(set((pred['subject'], pred['object']) for pred in image_candidate_predictions))
    gt_pairs = len(set((pred['subject'], pred['object']) for pred in image_candidate_predictions if pred['has_gt']))
    
    return {
        'recall@k': recall,
        'k': k,
        'actual_k': actual_k,  # å®é™…å–çš„æ•°é‡ï¼ˆè¿‡æ»¤no relationåï¼‰
        'recalled_relations': len(recalled_relations),
        'total_gt_relations': total_gt_relations,
        'total_candidates': len(image_candidate_predictions),
        'top_k_candidates': len(top_k_predictions),
        'total_pairs': total_pairs,  # æ€»é¢„æµ‹å¯¹æ•°
        'gt_pairs': gt_pairs,  # æœ‰GTçš„é…å¯¹å¯¹æ•°
        'non_gt_pairs': total_pairs - gt_pairs  # æ— GTçš„é…å¯¹å¯¹æ•°
    }


def calculate_mean_recall_per_predicate(per_image_candidates, predicates, k=50):
    """
    è®¡ç®—æ¯ä¸ªè°“è¯ç±»åˆ«çš„mean recall@50
    
    Args:
        per_image_candidates: dict, keyä¸ºimage_id, valueä¸ºè¯¥å›¾ç‰‡çš„æ‰€æœ‰å€™é€‰é¢„æµ‹åˆ—è¡¨
        predicates: æ‰€æœ‰è°“è¯ç±»åˆ«åˆ—è¡¨
        k: top-k
    
    Returns:
        dict: æ¯ä¸ªè°“è¯çš„recallå’Œæ•´ä½“mean recall
    
    ä¿®æ”¹ï¼šå…ˆè¿‡æ»¤no relationï¼Œå†å–top-kï¼ˆä¸evaluate_results.pyå¯¹é½ï¼‰
    """
    # åˆå§‹åŒ–æ¯ä¸ªè°“è¯çš„ç»Ÿè®¡
    predicate_stats = {pred: {'hit': 0, 'total': 0} for pred in predicates}
    
    for image_id, candidates in per_image_candidates.items():
        # è·å–è¯¥å›¾ç‰‡ä¸­æ‰€æœ‰GTå…³ç³»ï¼ˆåªç»Ÿè®¡relation_idx >= 0çš„ï¼Œæ’é™¤-1ï¼‰
        gt_relations = set()
        for cand in candidates:
            if cand['relation_idx'] >= 0:
                gt_relations.add(cand['relation_idx'])
        
        # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰no relationçš„é¢„æµ‹ï¼ˆä»æ‰€æœ‰å€™é€‰ä¸­ï¼‰
        non_bg_candidates = []
        for cand in candidates:
            if cand.get('predicted_predicate') != 'no relation':
                non_bg_candidates.append(cand)
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
        predictions_sorted = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
        actual_k = min(k, len(predictions_sorted))
        top_k_predictions = predictions_sorted[:actual_k]
        
        # ç»Ÿè®¡è¯¥å›¾ç‰‡ä¸­æ¯ä¸ªè°“è¯ç±»åˆ«çš„GT
        gt_predicates_in_image = {}
        recalled_predicates_in_image = {}
        
        for cand in candidates:
            if not cand['has_gt'] or cand['relation_idx'] == -1:
                continue  # è·³è¿‡æ²¡æœ‰GTçš„é…å¯¹
                
            gt_pred = cand['gt_predicate']
            relation_idx = cand['relation_idx']
            
            # ç»Ÿè®¡GTï¼ˆæ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡ï¼‰
            if relation_idx not in gt_predicates_in_image:
                gt_predicates_in_image[relation_idx] = gt_pred
                predicate_stats[gt_pred]['total'] += 1
        
        # ç¬¬ä¸‰æ­¥ï¼šåœ¨top-kä¸­ï¼Œåªå¯¹GTå…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼Œç»Ÿè®¡å¬å›çš„è°“è¯
        for cand in top_k_predictions:
            # åªç»Ÿè®¡GTå…³ç³»å¯¹ä¸­é¢„æµ‹æ­£ç¡®çš„
            if cand['relation_idx'] in gt_relations and cand['is_correct']:
                relation_idx = cand['relation_idx']
                gt_pred = cand['gt_predicate']
                
                # æ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡å¬å›
                if relation_idx not in recalled_predicates_in_image:
                    recalled_predicates_in_image[relation_idx] = gt_pred
                    predicate_stats[gt_pred]['hit'] += 1
    
    # è®¡ç®—æ¯ä¸ªè°“è¯çš„recall
    per_predicate_recall = {}
    valid_predicates = []
    
    for pred in predicates:
        total = predicate_stats[pred]['total']
        hit = predicate_stats[pred]['hit']
        
        if total > 0:
            recall = hit / total
            per_predicate_recall[pred] = {
                'recall': recall,
                'hit': hit,
                'total': total
            }
            valid_predicates.append(recall)
        else:
            per_predicate_recall[pred] = {
                'recall': 0.0,
                'hit': 0,
                'total': 0
            }
    
    # è®¡ç®—mean recallï¼ˆåªå¯¹æœ‰GTçš„ç±»åˆ«è®¡ç®—ï¼‰
    mean_recall = sum(valid_predicates) / len(valid_predicates) if valid_predicates else 0.0
    
    return {
        'mean_recall@k': mean_recall,
        'k': k,
        'per_predicate_recall': per_predicate_recall,
        'num_valid_predicates': len(valid_predicates),
        'total_predicates': len(predicates)
    }


def calculate_average_recall_at_k(per_image_candidates, k=50):

    per_image_results = []
    total_recall = 0.0
    valid_images = 0
    
    for image_id, candidates in per_image_candidates.items():
        # è®¡ç®—è¯¥å›¾ç‰‡çš„recall
        img_result = calculate_recall_at_k_per_image(candidates, k)
        img_result['image_id'] = image_id
        per_image_results.append(img_result)
        
        total_recall += img_result['recall@k']
        valid_images += 1
    
    # è®¡ç®—å¹³å‡recall
    avg_recall = total_recall / valid_images if valid_images > 0 else 0.0
    
    # ç»Ÿè®¡æ€»ä½“ä¿¡æ¯
    total_gt_relations = sum(r['total_gt_relations'] for r in per_image_results)
    total_recalled_relations = sum(r['recalled_relations'] for r in per_image_results)
    
    # ç»Ÿè®¡å€™é€‰æ•°ä¸è¶³kçš„å›¾ç‰‡æ•°é‡
    images_with_insufficient_candidates = sum(1 for r in per_image_results if r['actual_k'] < k)
    
    return {
        'avg_recall@k': avg_recall,
        'k': k,
        'total_images': valid_images,
        'total_gt_relations': total_gt_relations,
        'total_recalled_relations': total_recalled_relations,
        'images_with_insufficient_candidates': images_with_insufficient_candidates,
        'per_image_results': per_image_results
    }


def process_data_shard(gpu_id, data_shard, model_args, data_args, predicate_vectors_dict, cot_map, result_queue, progress_queue, use_original_query=False, use_image=False):
    """
    åœ¨æŒ‡å®šGPUä¸Šå¤„ç†æ•°æ®åˆ†ç‰‡
    
    Args:
        gpu_id: GPU ID (0, 1, 2, ...)
        data_shard: è¯¥GPUè¦å¤„ç†çš„æ•°æ®åˆ†ç‰‡ï¼ˆå›¾ç‰‡åˆ—è¡¨ï¼‰
        model_args: æ¨¡å‹å‚æ•°
        data_args: æ•°æ®å‚æ•°
        predicate_vectors_dict: å…±äº«çš„è°“è¯å‘é‡å­—å…¸ï¼ˆé€šè¿‡Manageråˆ›å»ºï¼‰
        cot_map: CoTæè¿°æ˜ å°„å­—å…¸ {(image_id, subject, object): cot_description}
        result_queue: ç»“æœé˜Ÿåˆ—
        progress_queue: è¿›åº¦é˜Ÿåˆ—
        use_original_query: æ˜¯å¦åœ¨cot_descriptionå‰åŠ ä¸ŠåŸå§‹query
        use_image: æ˜¯å¦è°ƒç”¨å›¾åƒ
    """
    device = f'cuda:{gpu_id}'
    torch.cuda.set_device(gpu_id)
    
    try:
        # åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ï¼‰
        processor = load_processor(model_args, data_args)
        
        # å°è¯•åŠ è½½æ¨¡å‹
        try:
            model = MMEBModel.load(model_args, is_trainable=False)
            model = model.to(device, dtype=torch.bfloat16)
            model.eval()
        except Exception as e:
            error_msg = str(e)
            if ("flash" in error_msg.lower() or 
                "ampere" in error_msg.lower() or 
                "attention" in error_msg.lower() and "support" in error_msg.lower()):
                # å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼
                os.environ["ATTN_IMPLEMENTATION"] = "eager"
                os.environ["USE_FLASH_ATTENTION"] = "0"
                
                import importlib
                import src.model.model
                importlib.reload(src.model.model)
                from src.model.model import MMEBModel as MMEBModelReloaded
                
                processor = load_processor(model_args, data_args)
                model = MMEBModelReloaded.load(model_args, is_trainable=False)
                model = model.to(device, dtype=torch.bfloat16)
                model.eval()
            else:
                raise
        
        # è·å–æˆ–é¢„è®¡ç®—è°“è¯å‘é‡
        if gpu_id not in predicate_vectors_dict:
            # å¦‚æœè¯¥GPUè¿˜æ²¡æœ‰è°“è¯å‘é‡ï¼Œåˆ™é¢„è®¡ç®—
            predicate_vectors = precompute_predicate_vectors(
                model, processor, PREDICATES, device=device,
                progress_queue=progress_queue, gpu_id=gpu_id
            )
            predicate_vectors_dict[gpu_id] = predicate_vectors.cpu()  # ä¿å­˜åˆ°CPUä»¥ä¾¿å…±äº«
        else:
            # ä½¿ç”¨å…±äº«çš„è°“è¯å‘é‡ï¼ˆéœ€è¦ç§»å›GPUï¼‰
            predicate_vectors = predicate_vectors_dict[gpu_id].to(device)
            progress_queue.put((gpu_id, f"GPU{gpu_id}: ä½¿ç”¨å…±äº«è°“è¯å‘é‡ï¼Œå¼€å§‹å¤„ç†å›¾ç‰‡..."))
        
        # å¤„ç†è¯¥GPUçš„æ•°æ®åˆ†ç‰‡
        per_image_candidates = {}
        all_relations_info = []
        processed_images = 0
        total_images = len(data_shard)
        missing_cot_count = 0
        total_pairs_checked = 0  # ç»Ÿè®¡æ£€æŸ¥çš„é…å¯¹æ€»æ•°
        
        # å‘é€å¼€å§‹å¤„ç†çš„æ¶ˆæ¯
        progress_queue.put((gpu_id, f"GPU{gpu_id}: å¼€å§‹å¤„ç† {total_images} å¼ å›¾ç‰‡"))
        
        for img_idx, img_data in enumerate(data_shard):
            image_id = img_data['image_id']
            image_path = img_data['image_path']
            objects = img_data['objects']
            relations = img_data['relations']
            
            # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                progress_queue.put((gpu_id, f"âš ï¸  GPU{gpu_id}: å›¾åƒä¸å­˜åœ¨ {image_path}"))
                continue
            
            # è·å–å›¾åƒå°ºå¯¸
            with Image.open(image_path) as img:
                original_width, original_height = img.size
            
            # åˆ›å»ºç‰©ä½“IDåˆ°ç‰©ä½“ä¿¡æ¯çš„æ˜ å°„
            obj_dict = {obj['id']: obj for obj in objects}
            
            # åˆå§‹åŒ–è¯¥å›¾ç‰‡çš„å€™é€‰åˆ—è¡¨
            image_candidates = []
            image_relation_idx = 0
            
            # åˆ›å»ºGTå…³ç³»æ˜ å°„
            gt_relations_map = {}
            for relation in relations:
                subject_id = relation['subject_id']
                object_id = relation['object_id']
                gt_predicate = relation['predicate']
                if (subject_id, object_id) not in gt_relations_map:
                    gt_relations_map[(subject_id, object_id)] = []
                gt_relations_map[(subject_id, object_id)].append(gt_predicate)
            
            # å¯¹æ‰€æœ‰ç‰©ä½“è¿›è¡Œä¸¤ä¸¤é…å¯¹é¢„æµ‹
            object_ids = list(obj_dict.keys())
            for i, subject_id in enumerate(object_ids):
                for j, object_id in enumerate(object_ids):
                    if i == j:
                        continue
                    
                    subject_obj = obj_dict[subject_id]
                    object_obj = obj_dict[object_id]
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„CoTæè¿°ï¼ˆä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼‰
                    subject_name = subject_obj['class_name'].strip()  # å»é™¤å¯èƒ½çš„ç©ºæ ¼
                    object_name = object_obj['class_name'].strip()  # å»é™¤å¯èƒ½çš„ç©ºæ ¼
                    total_pairs_checked += 1
                    
                    # å°è¯•å¤šç§keyæ ¼å¼ä»¥ç¡®ä¿åŒ¹é…ï¼ˆä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼‰
                    cot_description = None
                    key_formats = []
                    
                    # ä¼˜å…ˆå°è¯•ä½¿ç”¨ç‰©ä½“IDçš„keyæ ¼å¼
                    key_formats.extend([
                        (str(image_id), subject_id, object_id),  # å­—ç¬¦ä¸²image_id + ç‰©ä½“ID
                        (image_id, subject_id, object_id),  # åŸå§‹image_idæ ¼å¼ + ç‰©ä½“ID
                    ])
                    if isinstance(image_id, str) and image_id.isdigit():
                        key_formats.append((int(image_id), subject_id, object_id))
                    
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨ç±»åˆ«åçš„keyæ ¼å¼
                    key_formats.extend([
                        (str(image_id), subject_name, object_name),  # å­—ç¬¦ä¸²image_id + ç±»åˆ«å
                        (image_id, subject_name, object_name),  # åŸå§‹image_idæ ¼å¼ + ç±»åˆ«å
                    ])
                    if isinstance(image_id, str) and image_id.isdigit():
                        key_formats.append((int(image_id), subject_name, object_name))
                    
                    for key_format in key_formats:
                        if key_format and key_format in cot_map:
                            cot_description = cot_map[key_format]
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°CoTæè¿°ï¼Œç»Ÿè®¡å¹¶è·³è¿‡è¯¥æ ·æœ¬
                    if not cot_description or not cot_description.strip():
                        missing_cot_count += 1
                        continue  # è·³è¿‡è¯¥é…å¯¹
                    
                    # é¢„æµ‹å…³ç³»ï¼ˆä½¿ç”¨CoTæè¿°ï¼‰
                    predicate_scores = predict_relation(
                        model, processor, image_path,
                        subject_obj, object_obj,
                        original_width, original_height,
                        cot_description=cot_description,
                        predicate_vectors=predicate_vectors,
                        device=device,
                        use_original_query=use_original_query,
                        use_image=use_image
                    )
                    
                    # å¦‚æœpredict_relationè¿”å›Noneï¼ˆCoTæè¿°ä¸ºç©ºæˆ–å¤„ç†å¤±è´¥ï¼‰ï¼Œè·³è¿‡è¯¥æ ·æœ¬
                    if predicate_scores is None:
                        missing_cot_count += 1
                        continue
                    
                    # åˆ¤æ–­è¯¥é…å¯¹æ˜¯å¦æœ‰GTå…³ç³»
                    has_gt = (subject_id, object_id) in gt_relations_map
                    gt_predicates = gt_relations_map.get((subject_id, object_id), [])
                    
                    # è®°å½•å…³ç³»ä¿¡æ¯
                    if has_gt:
                        for gt_predicate in gt_predicates:
                            all_relations_info.append({
                                'relation_idx': -1,  # å°†åœ¨ä¸»è¿›ç¨‹é‡æ–°åˆ†é…
                                'image_id': image_id,
                                'image_relation_idx': image_relation_idx,
                                'subject_id': subject_id,  # æ·»åŠ subject_idä»¥åŒºåˆ†åŒåç‰©ä½“
                                'object_id': object_id,  # æ·»åŠ object_idä»¥åŒºåˆ†åŒåç‰©ä½“
                                'subject': subject_obj['class_name'],
                                'object': object_obj['class_name'],
                                'gt_predicate': gt_predicate
                            })
                            image_relation_idx += 1
                    
                    # å°†è¯¥é…å¯¹çš„50ä¸ªè°“è¯å€™é€‰åŠ å…¥å€™é€‰æ± 
                    # è®¡ç®—è¯¥é…å¯¹å¯¹åº”çš„å…³ç³»ç´¢å¼•èµ·å§‹å€¼
                    relation_idx_start = image_relation_idx - len(gt_predicates) if has_gt else -1
                    
                    for pred_score in predicate_scores:
                        is_correct = False
                        if has_gt and pred_score['predicate'] in gt_predicates:
                            is_correct = True
                        
                        # å¦‚æœé¢„æµ‹æ­£ç¡®ï¼Œæ‰¾åˆ°å¯¹åº”çš„å…³ç³»ç´¢å¼•
                        relation_idx = -1
                        if is_correct and has_gt:
                            # æ‰¾åˆ°è¯¥è°“è¯åœ¨gt_predicatesä¸­çš„ä½ç½®
                            for idx, gt_pred in enumerate(gt_predicates):
                                if gt_pred == pred_score['predicate']:
                                    relation_idx = relation_idx_start + idx
                                    break
                        
                        image_candidates.append({
                            'relation_idx': relation_idx,
                            'global_relation_idx': -1,  # å°†åœ¨ä¸»è¿›ç¨‹é‡æ–°åˆ†é…
                            'image_id': image_id,
                            'subject_id': subject_id,  # æ·»åŠ subject_idä»¥åŒºåˆ†åŒåç‰©ä½“
                            'object_id': object_id,  # æ·»åŠ object_idä»¥åŒºåˆ†åŒåç‰©ä½“
                            'subject': subject_obj['class_name'],
                            'object': object_obj['class_name'],
                            'gt_predicate': gt_predicates[0] if gt_predicates else None,
                            'gt_predicates': gt_predicates,
                            'predicted_predicate': pred_score['predicate'],
                            'similarity': pred_score['similarity'],
                            'is_correct': is_correct,
                            'has_gt': has_gt
                        })
            
            per_image_candidates[image_id] = image_candidates
            processed_images += 1
            
            # æ›´æ–°è¿›åº¦ï¼ˆæ¯5å¼ å›¾ç‰‡æ›´æ–°ä¸€æ¬¡ï¼Œå¹³è¡¡å®æ—¶æ€§å’Œæ€§èƒ½ï¼‰
            if processed_images % 5 == 0 or processed_images == total_images:
                progress_queue.put((gpu_id, f"GPU{gpu_id}: å·²å¤„ç† {processed_images}/{total_images} å¼ å›¾ç‰‡ ({processed_images*100//total_images}%)"))
        
        matched_count = total_pairs_checked - missing_cot_count
        match_rate = (matched_count / total_pairs_checked * 100) if total_pairs_checked > 0 else 0
        if missing_cot_count > 0:
            progress_queue.put((gpu_id, f"âš ï¸  GPU{gpu_id}: è­¦å‘Šï¼š{missing_cot_count}/{total_pairs_checked} ä¸ªé…å¯¹å› CoTæè¿°ä¸ºç©ºè€Œè¢«è·³è¿‡ï¼ˆåŒ¹é…ç‡: {match_rate:.1f}%ï¼‰"))
        else:
            progress_queue.put((gpu_id, f"âœ… GPU{gpu_id}: æ‰€æœ‰ {total_pairs_checked} ä¸ªé…å¯¹éƒ½æˆåŠŸåŒ¹é…åˆ°CoTæè¿°"))
        
        # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
        result_queue.put({
            'gpu_id': gpu_id,
            'per_image_candidates': per_image_candidates,
            'all_relations_info': all_relations_info,
            'missing_cot_count': missing_cot_count,  # ä¼ é€’è·³è¿‡çš„æ ·æœ¬æ•°é‡
            'total_pairs_checked': total_pairs_checked  # ä¼ é€’æ£€æŸ¥çš„é…å¯¹æ€»æ•°
        })
        
        progress_queue.put((gpu_id, f"âœ… GPU{gpu_id}: å®Œæˆå¤„ç† {processed_images} å¼ å›¾ç‰‡"))
        
    except Exception as e:
        import traceback
        error_msg = f"GPU{gpu_id}å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        result_queue.put({
            'gpu_id': gpu_id,
            'error': error_msg
        })
        progress_queue.put((gpu_id, f"âŒ {error_msg}"))



def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Stage3: ä½¿ç”¨Stage2çš„CoTæ•°æ®è¿›è¡Œåœºæ™¯å›¾å…³ç³»é¢„æµ‹ä¸Per-Image Recall@50è®¡ç®—')
    parser.add_argument('--num_gpus', type=int, default=None,
                        help='æŒ‡å®šä½¿ç”¨çš„GPUæ•°é‡ï¼ˆé»˜è®¤ï¼šä½¿ç”¨æ‰€æœ‰å¯ç”¨GPUï¼Œæˆ–ä»NUM_GPUSç¯å¢ƒå˜é‡/é…ç½®å˜é‡è¯»å–ï¼‰')
    parser.add_argument('--input_file', type=str, default=None,
                        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„INPUT_FILEï¼‰')
    parser.add_argument('--stage2_file', type=str, default=None,
                        help='Stage2è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„STAGE2_OUTPUT_FILEï¼‰')
    parser.add_argument('--output_file', type=str, default=None,
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„OUTPUT_FILEï¼‰')
    parser.add_argument('--use_original_query', action='store_true', default=True,
                        help='æ˜¯å¦åœ¨cot_descriptionå‰åŠ ä¸ŠåŸå§‹queryï¼ˆé»˜è®¤ï¼šFalseï¼‰')
    parser.add_argument('--use_image', action='store_true', default=True,
                        help='æ˜¯å¦è°ƒç”¨å›¾åƒï¼ˆé»˜è®¤ï¼šFalseï¼‰')
    args = parser.parse_args()
    
    # ç¡®å®šä½¿ç”¨çš„GPUæ•°é‡ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®å˜é‡ > æ‰€æœ‰GPUï¼‰
    num_gpus_to_use = args.num_gpus
    if num_gpus_to_use is None:
        num_gpus_to_use = os.environ.get('NUM_GPUS')
        if num_gpus_to_use is not None:
            num_gpus_to_use = int(num_gpus_to_use)
        else:
            num_gpus_to_use = NUM_GPUS
    
    # ç¡®å®šè¾“å…¥è¾“å‡ºæ–‡ä»¶
    input_file = args.input_file if args.input_file else INPUT_FILE
    stage2_file = args.stage2_file if args.stage2_file else STAGE2_OUTPUT_FILE
    output_file = args.output_file if args.output_file else OUTPUT_FILE
    
    print("="*80)
    print("Stage3: ä½¿ç”¨Stage2çš„CoTæ•°æ®è¿›è¡Œåœºæ™¯å›¾å…³ç³»é¢„æµ‹ä¸Per-Image Recall@50è®¡ç®—")
    print("="*80)
    print(f"\né…ç½®é€‰é¡¹:")
    print(f"  ä½¿ç”¨åŸå§‹query: {'æ˜¯' if args.use_original_query else 'å¦'}")
    print(f"  ä½¿ç”¨å›¾åƒ: {'æ˜¯' if args.use_image else 'å¦'}")
    print("="*80)

    # æ£€æµ‹å¯ç”¨GPUæ•°é‡
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
        return
    
    total_gpus = torch.cuda.device_count()
    print(f"\nğŸ” æ£€æµ‹åˆ° {total_gpus} ä¸ªGPUè®¾å¤‡")
    for i in range(total_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        print(f"   GPU {i}: {gpu_name}")
    
    # ç¡®å®šå®é™…ä½¿ç”¨çš„GPUæ•°é‡
    if num_gpus_to_use is None:
        num_gpus = total_gpus
        print(f"\nâœ… ä½¿ç”¨æ‰€æœ‰ {num_gpus} ä¸ªGPU")
    else:
        num_gpus = min(num_gpus_to_use, total_gpus)
        if num_gpus_to_use > total_gpus:
            print(f"\nâš ï¸  è­¦å‘Š: è¯·æ±‚ä½¿ç”¨ {num_gpus_to_use} ä¸ªGPUï¼Œä½†åªæœ‰ {total_gpus} ä¸ªå¯ç”¨ï¼Œå°†ä½¿ç”¨ {num_gpus} ä¸ªGPU")
        else:
            print(f"\nâœ… ä½¿ç”¨æŒ‡å®šçš„ {num_gpus} ä¸ªGPU (GPU 0-{num_gpus-1})")
    
    # åŠ è½½Stage2çš„CoTæ•°æ®
    cot_map = load_stage2_cot_data(stage2_file)
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    total_images = len(data)
    total_relations = sum(len(img['relations']) for img in data)
    print(f"   åŠ è½½äº† {total_images} å¼ å›¾ç‰‡ï¼Œå…± {total_relations} ä¸ªå…³ç³»")
    
    # å‡†å¤‡æ¨¡å‹å‚æ•°
    model_args = ModelArguments(
        model_name='/public/home/xiaojw2025/Workspace/VLM2Vec/models/qwen_vl/Qwen2-VL-2B-Instruct',
        # checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/qwen_vl/Qwen2-VL-2B-Instruct',
        checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/VLM2Vec-Qwen2VL-2B',
        # checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/train_5k_balance',
        pooling='last',
        normalize=True,
        model_backbone='qwen2_vl',
        lora=True
    )
    
    data_args = DataArguments(
        resize_min_pixels=56 * 56,
        resize_max_pixels=28 * 28 * 1280
    )
    
    # æ•°æ®åˆ†ç‰‡ï¼šå°†æ•°æ®å‡åŒ€åˆ†é…åˆ°å„ä¸ªGPU
    print(f"\nğŸ“Š æ•°æ®åˆ†ç‰‡: å°† {total_images} å¼ å›¾ç‰‡åˆ†é…åˆ° {num_gpus} ä¸ªGPU")
    data_shards = []
    images_per_gpu = math.ceil(total_images / num_gpus)
    for i in range(num_gpus):
        start_idx = i * images_per_gpu
        end_idx = min((i + 1) * images_per_gpu, total_images)
        shard = data[start_idx:end_idx]
        data_shards.append(shard)
        print(f"   GPU {i}: {len(shard)} å¼ å›¾ç‰‡ (ç´¢å¼• {start_idx}-{end_idx-1})")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œå¤šGPUå¹¶è¡Œæ¨ç†
    print(f"\nğŸš€ å¼€å§‹å¤šGPUå¹¶è¡Œæ¨ç†...\n")
    
    # åˆ›å»ºå…±äº«å­—å…¸å’Œé˜Ÿåˆ—
    manager = Manager()
    predicate_vectors_dict = manager.dict()  # å…±äº«çš„è°“è¯å‘é‡å­—å…¸
    result_queue = Queue()  # ç»“æœé˜Ÿåˆ—
    progress_queue = Queue()  # è¿›åº¦é˜Ÿåˆ—
    
    # å°†cot_mapè½¬æ¢ä¸ºæ™®é€šdictï¼ˆmultiprocessingéœ€è¦å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
    # æ³¨æ„ï¼šManager().dict()ä¸æ”¯æŒåµŒå¥—dictï¼Œæ‰€ä»¥ç›´æ¥ä¼ é€’æ™®é€šdict
    # ç”±äºcot_mapæ˜¯åªè¯»çš„ï¼Œå¯ä»¥åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­ç›´æ¥ä½¿ç”¨
    
    # å¯åŠ¨å¤šä¸ªè¿›ç¨‹
    processes = []
    for gpu_id in range(num_gpus):
        if len(data_shards[gpu_id]) > 0:  # åªå¯åŠ¨æœ‰æ•°æ®çš„GPUè¿›ç¨‹
            p = Process(
                target=process_data_shard,
                args=(gpu_id, data_shards[gpu_id], model_args, data_args, 
                      predicate_vectors_dict, cot_map, result_queue, progress_queue,
                      args.use_original_query, args.use_image)
            )
            p.start()
            processes.append(p)
            print(f"   âœ… å¯åŠ¨GPU {gpu_id}è¿›ç¨‹")
    
    # ç›‘æ§è¿›åº¦
    completed_gpus = set()
    progress_messages = {}
    
    def print_progress():
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        updated = False
        while not progress_queue.empty():
            try:
                gpu_id, message = progress_queue.get_nowait()
                progress_messages[gpu_id] = message
                updated = True
            except:
                break
        
        # å¦‚æœæœ‰æ›´æ–°ï¼Œæ‰“å°æ‰€æœ‰GPUçš„æœ€æ–°è¿›åº¦
        if updated:
            # æ¸…å±å¹¶æ‰“å°æ‰€æœ‰GPUçš„è¿›åº¦ï¼ˆä½¿ç”¨\rå®ç°è¦†ç›–ï¼‰
            for gpu_id in range(num_gpus):
                if gpu_id in progress_messages:
                    print(f"   {progress_messages[gpu_id]}", flush=True)
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆå¹¶æ”¶é›†ç»“æœ
    print("\nğŸ“ˆ æ¨ç†è¿›åº¦:")
    all_results = {}
    import time
    last_print_time = time.time()
    print_interval = 0.5  # æ¯0.5ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
    
    while len(completed_gpus) < len(processes):
        current_time = time.time()
        
        # å®šæœŸæ‰“å°è¿›åº¦ï¼ˆæ¯0.5ç§’æˆ–é˜Ÿåˆ—ä¸ä¸ºç©ºæ—¶ï¼‰
        if current_time - last_print_time >= print_interval or not progress_queue.empty():
            print_progress()
            last_print_time = current_time
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç»“æœ
        try:
            result = result_queue.get(timeout=0.1)  # å‡å°‘è¶…æ—¶æ—¶é—´ï¼Œæ›´é¢‘ç¹æ£€æŸ¥è¿›åº¦
            if 'error' in result:
                print(f"\nâŒ {result['error']}", flush=True)
                completed_gpus.add(result['gpu_id'])
            else:
                all_results[result['gpu_id']] = result
                completed_gpus.add(result['gpu_id'])
                print(f"\n   âœ… GPU {result['gpu_id']} å®Œæˆ", flush=True)
        except:
            # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­å¾ªç¯æ£€æŸ¥è¿›åº¦
            pass
    
    # æœ€åæ‰“å°ä¸€æ¬¡æ‰€æœ‰GPUçš„æœ€ç»ˆè¿›åº¦
    print_progress()
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ç»“æŸ
    for p in processes:
        p.join()
        p.terminate()
    
    print("\nâœ… æ‰€æœ‰GPUå¤„ç†å®Œæˆï¼Œæ­£åœ¨åˆå¹¶ç»“æœ...")
    
    # åˆå¹¶æ‰€æœ‰GPUçš„ç»“æœ
    per_image_candidates = {}
    all_relations_info = []
    total_missing_cot_count = 0  # ç»Ÿè®¡æ€»çš„è·³è¿‡æ ·æœ¬æ•°é‡
    total_pairs_checked_all = 0  # ç»Ÿè®¡æ€»çš„æ£€æŸ¥é…å¯¹æ•°é‡
    
    # æŒ‰GPU IDé¡ºåºåˆå¹¶ç»“æœ
    for gpu_id in sorted(all_results.keys()):
        result = all_results[gpu_id]
        if 'error' in result:
            continue
        
        # åˆå¹¶æ¯å¼ å›¾ç‰‡çš„å€™é€‰
        for image_id, candidates in result['per_image_candidates'].items():
            per_image_candidates[image_id] = candidates
        
        # ç´¯è®¡è·³è¿‡çš„æ ·æœ¬æ•°é‡å’Œæ£€æŸ¥çš„é…å¯¹æ•°é‡
        if 'missing_cot_count' in result:
            total_missing_cot_count += result['missing_cot_count']
        if 'total_pairs_checked' in result:
            total_pairs_checked_all += result['total_pairs_checked']
    
    # é‡æ–°åˆ†é…å…³ç³»ç´¢å¼•ï¼ˆåŸºäºåˆå¹¶åçš„æ•°æ®ï¼‰
    print("   é‡æ–°åˆ†é…å…³ç³»ç´¢å¼•...")
    global_relation_idx = 0
    
    # æŒ‰å›¾ç‰‡IDæ’åºå¤„ç†ï¼Œç¡®ä¿ä¸€è‡´æ€§
    for image_id in sorted(per_image_candidates.keys()):
        candidates = per_image_candidates[image_id]
        image_relation_idx = 0
        
        # æ”¶é›†è¯¥å›¾ç‰‡çš„æ‰€æœ‰GTå…³ç³»ï¼ˆå»é‡ï¼Œä½¿ç”¨ç‰©ä½“IDåŒºåˆ†åŒåç‰©ä½“ï¼‰
        gt_relations_set = set()
        gt_relations_list = []
        for cand in candidates:
            if cand['has_gt'] and cand['gt_predicate']:
                # ä½¿ç”¨ç‰©ä½“IDä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œä»¥åŒºåˆ†åŒåç‰©ä½“
                subject_id = cand.get('subject_id', None)
                object_id = cand.get('object_id', None)
                if subject_id is not None and object_id is not None:
                    key = (subject_id, object_id, cand['gt_predicate'])
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ç‰©ä½“IDï¼Œåˆ™ä½¿ç”¨ç±»åˆ«å
                    key = (cand['subject'], cand['object'], cand['gt_predicate'])
                if key not in gt_relations_set:
                    gt_relations_set.add(key)
                    gt_relations_list.append({
                        'subject_id': subject_id,
                        'object_id': object_id,
                        'subject': cand['subject'],
                        'object': cand['object'],
                        'gt_predicate': cand['gt_predicate']
                    })
        
        # ä¸ºæ¯ä¸ªGTå…³ç³»åˆ†é…å…¨å±€ç´¢å¼•
        image_relation_idx_map = {}  # (subject_id, object_id, gt_predicate) æˆ– (subject, object, gt_predicate) -> relation_idx
        for rel_info in gt_relations_list:
            if rel_info['subject_id'] is not None and rel_info['object_id'] is not None:
                key = (rel_info['subject_id'], rel_info['object_id'], rel_info['gt_predicate'])
            else:
                key = (rel_info['subject'], rel_info['object'], rel_info['gt_predicate'])
            image_relation_idx_map[key] = global_relation_idx
            
            all_relations_info.append({
                'relation_idx': global_relation_idx,
                'image_id': image_id,
                'image_relation_idx': image_relation_idx,
                'subject_id': rel_info['subject_id'],
                'object_id': rel_info['object_id'],
                'subject': rel_info['subject'],
                'object': rel_info['object'],
                'gt_predicate': rel_info['gt_predicate']
            })
            global_relation_idx += 1
            image_relation_idx += 1
        
        # æ›´æ–°å€™é€‰ä¸­çš„å…³ç³»ç´¢å¼•
        for cand in candidates:
            if cand['has_gt'] and cand['gt_predicate']:
                # ä½¿ç”¨ç‰©ä½“IDè¿›è¡ŒåŒ¹é…
                subject_id = cand.get('subject_id', None)
                object_id = cand.get('object_id', None)
                if subject_id is not None and object_id is not None:
                    key = (subject_id, object_id, cand['gt_predicate'])
                else:
                    # å‘åå…¼å®¹
                    key = (cand['subject'], cand['object'], cand['gt_predicate'])
                if key in image_relation_idx_map:
                    rel_idx = image_relation_idx_map[key]
                    cand['relation_idx'] = rel_idx
                    cand['global_relation_idx'] = rel_idx
                else:
                    cand['relation_idx'] = -1
                    cand['global_relation_idx'] = -1
            else:
                cand['relation_idx'] = -1
                cand['global_relation_idx'] = -1
    
    # å¤šGPUæ¨¡å¼ä¸‹ï¼Œç»“æœå·²ç»åœ¨process_data_shardä¸­å¤„ç†å®Œæˆå¹¶åˆå¹¶
    # ç°åœ¨ç›´æ¥è¿›å…¥ç»“æœç»Ÿè®¡å’Œä¿å­˜é˜¶æ®µ
    
    print(f"\nâœ… é¢„æµ‹å®Œæˆï¼")
    print(f"   æ€»å›¾ç‰‡æ•°: {len(per_image_candidates)}")
    print(f"   æ€»GTå…³ç³»æ•°: {len(all_relations_info)}")
    total_candidates = sum(len(candidates) for candidates in per_image_candidates.values())
    print(f"   æ€»å€™é€‰é¢„æµ‹æ•°: {total_candidates}")
    if total_pairs_checked_all > 0:
        matched_count_all = total_pairs_checked_all - total_missing_cot_count
        match_rate_all = (matched_count_all / total_pairs_checked_all * 100) if total_pairs_checked_all > 0 else 0
        print(f"   CoTåŒ¹é…ç»Ÿè®¡: {matched_count_all}/{total_pairs_checked_all} ä¸ªé…å¯¹æˆåŠŸåŒ¹é…ï¼ˆåŒ¹é…ç‡: {match_rate_all:.1f}%ï¼‰")
        if total_missing_cot_count > 0:
            print(f"   âš ï¸  è·³è¿‡çš„æ ·æœ¬æ•°ï¼ˆCoTæè¿°ä¸ºç©ºï¼‰: {total_missing_cot_count}")
    
    # ç»Ÿè®¡é…å¯¹ä¿¡æ¯
    total_pairs = 0
    total_gt_pairs = 0
    for candidates in per_image_candidates.values():
        pairs_in_image = set((cand['subject'], cand['object']) for cand in candidates)
        gt_pairs_in_image = set((cand['subject'], cand['object']) for cand in candidates if cand['has_gt'])
        total_pairs += len(pairs_in_image)
        total_gt_pairs += len(gt_pairs_in_image)
    
    print(f"   æ€»é¢„æµ‹é…å¯¹å¯¹æ•°: {total_pairs}")
    print(f"   æœ‰GTçš„é…å¯¹å¯¹æ•°: {total_gt_pairs}")
    print(f"   æ— GTçš„é…å¯¹å¯¹æ•°: {total_pairs - total_gt_pairs}")
    
    # 5. è®¡ç®—Per-Image Recall@50å¹¶å–å¹³å‡
    print("\nğŸ“Š è®¡ç®—Per-Image Recall@50ï¼ˆæ¯å¼ å›¾ç‰‡ç‹¬ç«‹è®¡ç®—å†å¹³å‡ï¼‰...")
    recall_results = calculate_average_recall_at_k(per_image_candidates, k=50)
    
    # 5.1 è®¡ç®—Mean Recall@50ï¼ˆé’ˆå¯¹æ¯ä¸ªè°“è¯ç±»åˆ«ï¼‰
    print("\nğŸ“Š è®¡ç®—Mean Recall@50ï¼ˆé’ˆå¯¹æ‰€æœ‰è°“è¯ç±»åˆ«ï¼‰...")
    mean_recall_results = calculate_mean_recall_per_predicate(per_image_candidates, PREDICATES, k=50)
    
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœ (Per-Image Recall@50)")
    print("="*80)
    print(f"å¹³å‡ Recall@{recall_results['k']}: {recall_results['avg_recall@k']:.4f} ({recall_results['avg_recall@k']*100:.2f}%)")
    print(f"æ€»å›¾ç‰‡æ•°: {recall_results['total_images']}")
    print(f"æ€»å¬å›å…³ç³»æ•°: {recall_results['total_recalled_relations']}/{recall_results['total_gt_relations']}")
    
    # è®¡ç®—å¹³å‡é…å¯¹ç»Ÿè®¡
    if len(recall_results['per_image_results']) > 0:
        avg_total_pairs = sum(r.get('total_pairs', 0) for r in recall_results['per_image_results']) / len(recall_results['per_image_results'])
        avg_gt_pairs = sum(r.get('gt_pairs', 0) for r in recall_results['per_image_results']) / len(recall_results['per_image_results'])
        avg_non_gt_pairs = sum(r.get('non_gt_pairs', 0) for r in recall_results['per_image_results']) / len(recall_results['per_image_results'])
        
        print(f"å¹³å‡æ¯å¼ å›¾ç‰‡é¢„æµ‹é…å¯¹å¯¹æ•°: {avg_total_pairs:.1f}")
        print(f"å¹³å‡æ¯å¼ å›¾ç‰‡æœ‰GTçš„é…å¯¹å¯¹æ•°: {avg_gt_pairs:.1f}")
        print(f"å¹³å‡æ¯å¼ å›¾ç‰‡æ— GTçš„é…å¯¹å¯¹æ•°: {avg_non_gt_pairs:.1f}")
    else:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰å¤„ç†ä»»ä½•å›¾ç‰‡ï¼Œæ— æ³•è®¡ç®—å¹³å‡é…å¯¹ç»Ÿè®¡")
        print(f"å¹³å‡æ¯å¼ å›¾ç‰‡é¢„æµ‹é…å¯¹å¯¹æ•°: 0.0")
        print(f"å¹³å‡æ¯å¼ å›¾ç‰‡æœ‰GTçš„é…å¯¹å¯¹æ•°: 0.0")
        print(f"å¹³å‡æ¯å¼ å›¾ç‰‡æ— GTçš„é…å¯¹å¯¹æ•°: 0.0")
    
    if recall_results['images_with_insufficient_candidates'] > 0:
        print(f"å€™é€‰æ•°ä¸è¶³{recall_results['k']}çš„å›¾ç‰‡: {recall_results['images_with_insufficient_candidates']}/{recall_results['total_images']}")
    print("="*80)
    
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœ (Mean Recall@50 - æ‰€æœ‰è°“è¯ç±»åˆ«)")
    print("="*80)
    print(f"Mean Recall@{mean_recall_results['k']}: {mean_recall_results['mean_recall@k']:.4f} ({mean_recall_results['mean_recall@k']*100:.2f}%)")
    print(f"æœ‰æ•ˆè°“è¯ç±»åˆ«æ•°: {mean_recall_results['num_valid_predicates']}/{mean_recall_results['total_predicates']}")
    print("="*80)
    
    # æ˜¾ç¤ºæ¯ä¸ªè°“è¯çš„recallï¼ˆå‰10ä¸ªå’Œå10ä¸ªï¼‰
    print("\nè°“è¯ç±»åˆ«Recallè¯¦æƒ…ï¼ˆæŒ‰recallæ’åºï¼‰:")
    sorted_predicates = sorted(
        mean_recall_results['per_predicate_recall'].items(),
        key=lambda x: x[1]['recall'],
        reverse=True
    )
    
    # åªæ˜¾ç¤ºæœ‰GTçš„è°“è¯
    predicates_with_gt = [(pred, stats) for pred, stats in sorted_predicates if stats['total'] > 0]
    
    if len(predicates_with_gt) > 0:
        print("\n  Top-10 è¡¨ç°æœ€å¥½çš„è°“è¯:")
        for i, (pred, stats) in enumerate(predicates_with_gt[:10], 1):
            print(f"    {i:2d}. {pred:20s}: R={stats['recall']:.4f} ({stats['hit']:3d}/{stats['total']:3d})")
        
        if len(predicates_with_gt) > 10:
            print("\n  Bottom-10 è¡¨ç°æœ€å·®çš„è°“è¯:")
            for i, (pred, stats) in enumerate(predicates_with_gt[-10:], 1):
                print(f"    {i:2d}. {pred:20s}: R={stats['recall']:.4f} ({stats['hit']:3d}/{stats['total']:3d})")
    
    # 6. æ˜¾ç¤ºæ¯å¼ å›¾ç‰‡çš„recallåˆ†å¸ƒ
    per_image_recalls = [r['recall@k'] for r in recall_results['per_image_results']]
    if per_image_recalls:
        print(f"\nRecallåˆ†å¸ƒç»Ÿè®¡:")
        print(f"  æœ€å¤§å€¼: {max(per_image_recalls):.4f}")
        print(f"  æœ€å°å€¼: {min(per_image_recalls):.4f}")
        print(f"  ä¸­ä½æ•°: {sorted(per_image_recalls)[len(per_image_recalls)//2]:.4f}")
    
    # 7. æ”¶é›†æ‰€æœ‰å€™é€‰ç”¨äºå±•ç¤ºï¼ˆå¯é€‰ï¼‰
    all_candidate_predictions = []
    for candidates in per_image_candidates.values():
        all_candidate_predictions.extend(candidates)
    
    candidates_sorted = sorted(all_candidate_predictions, key=lambda x: x['similarity'], reverse=True)
    top50_global_candidates = candidates_sorted[:100]
    
    # 7.1 ä¸ºæ¯å¼ å›¾ç‰‡æ”¶é›†Top-100å€™é€‰ç»“æœ
    print("\nğŸ“¦ æ­£åœ¨æ•´ç†æ¯å¼ å›¾ç‰‡çš„Top-100å€™é€‰ç»“æœ...")
    per_image_top100_candidates = {}
    total_top100_candidates = 0
    
    for image_id, candidates in per_image_candidates.items():
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        sorted_candidates = sorted(candidates, key=lambda x: x['similarity'], reverse=True)
        # å–Top-100
        top100 = sorted_candidates[:min(100, len(sorted_candidates))]
        per_image_top100_candidates[image_id] = top100
        total_top100_candidates += len(top100)
    
    print(f"   æ”¶é›†äº† {len(per_image_top100_candidates)} å¼ å›¾ç‰‡çš„Top-100å€™é€‰")
    print(f"   æ€»å€™é€‰æ•°: {total_top100_candidates}")
    
    # 8. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_file}")
    output_data = {
        'summary': {
            'evaluation_method': 'per-image-all-pairs-stage3',
            'stage2_file': stage2_file,
            'total_images': len(per_image_candidates),
            'total_gt_relations': len(all_relations_info),
            'total_candidates': total_candidates,
            'total_top100_candidates': total_top100_candidates,  # æ–°å¢ï¼šTop-100å€™é€‰æ€»æ•°
            'avg_recall@50': recall_results['avg_recall@k'],
            'mean_recall@50': mean_recall_results['mean_recall@k'],
            'total_recalled_relations': recall_results['total_recalled_relations'],
            'total_gt_relations': recall_results['total_gt_relations'],
            'num_valid_predicates': mean_recall_results['num_valid_predicates'],
            'images_with_insufficient_candidates': recall_results['images_with_insufficient_candidates'],
            # æ–°å¢é…å¯¹ç»Ÿè®¡
            'total_pairs': total_pairs,
            'total_gt_pairs': total_gt_pairs,
            'total_non_gt_pairs': total_pairs - total_gt_pairs,
            'avg_pairs_per_image': total_pairs / len(per_image_candidates) if len(per_image_candidates) > 0 else 0,
            'avg_gt_pairs_per_image': total_gt_pairs / len(per_image_candidates) if len(per_image_candidates) > 0 else 0,
            # è·³è¿‡çš„æ ·æœ¬ç»Ÿè®¡
            'skipped_samples_missing_cot': total_missing_cot_count,
            # é…ç½®é€‰é¡¹
            'use_original_query': args.use_original_query,
            'use_image': args.use_image
        },
        'per_image_results': recall_results['per_image_results'],
        'mean_recall_per_predicate': mean_recall_results['per_predicate_recall'],
        'all_relations': all_relations_info,
        'per_image_top100_candidates': per_image_top100_candidates,  # æ–°å¢ï¼šæ¯å¼ å›¾ç‰‡çš„Top-100å€™é€‰
        'top50_global_candidates': top50_global_candidates,  # å…¨å±€æ’åºçš„top50ï¼ˆå‚è€ƒç”¨ï¼‰
        # 'all_candidates': all_candidate_predictions  # å®Œæ•´çš„å€™é€‰åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¯èƒ½å¾ˆå¤§ï¼‰
    }
    
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print("âœ… ç»“æœå·²ä¿å­˜ï¼")
    
    # 9. æ˜¾ç¤ºä¸€äº›æ ·ä¾‹
    print("\n" + "="*80)
    print("Per-Image Recallæ ·ä¾‹ï¼ˆå‰5å¼ å›¾ç‰‡ï¼‰")
    print("="*80)
    for i, img_result in enumerate(recall_results['per_image_results'][:5], 1):
        print(f"\n{i}. å›¾ç‰‡#{img_result['image_id']}")
        print(f"   Recall@50: {img_result['recall@k']:.4f} ({img_result['recall@k']*100:.2f}%)")
        print(f"   å¬å›: {img_result['recalled_relations']}/{img_result['total_gt_relations']} å…³ç³»")
        actual_k_info = f" (å®é™…å–{img_result['actual_k']}ä¸ª)" if img_result['actual_k'] < img_result['k'] else ""
        print(f"   å€™é€‰æ•°: {img_result['total_candidates']} (Top-{img_result['k']}ä¸­å–{img_result['top_k_candidates']}ä¸ª{actual_k_info})")
        print(f"   é…å¯¹ç»Ÿè®¡: æ€»é…å¯¹{img_result.get('total_pairs', 0)}å¯¹, æœ‰GTé…å¯¹{img_result.get('gt_pairs', 0)}å¯¹, æ— GTé…å¯¹{img_result.get('non_gt_pairs', 0)}å¯¹")
    
    print("\n" + "="*80)
    print("å…¨å±€Top-50å€™é€‰é¢„æµ‹æ ·ä¾‹ï¼ˆå‰10ä¸ªï¼Œä»…ä¾›å‚è€ƒï¼‰")
    print("="*80)
    for i, pred in enumerate(top50_global_candidates[:10], 1):
        status = "âœ…" if pred['is_correct'] else "âŒ"
        print(f"\n{i}. {status} æ’å#{i} (ç›¸ä¼¼åº¦: {pred['similarity']:.4f})")
        print(f"   å›¾ç‰‡#{pred['image_id']}, å…³ç³»#{pred['relation_idx']}: {pred['subject']} --[{pred['predicted_predicate']}]--> {pred['object']}")
        print(f"   GTè°“è¯: {pred['gt_predicate']}")


if __name__ == "__main__":
    # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼Œä»¥æ”¯æŒCUDAå¤šè¿›ç¨‹
    # Linuxç³»ç»Ÿé»˜è®¤ä½¿ç”¨'fork'ï¼Œä½†CUDAä¸æ”¯æŒåœ¨forkçš„å­è¿›ç¨‹ä¸­é‡æ–°åˆå§‹åŒ–
    # å¿…é¡»åœ¨å¯¼å…¥multiprocessingåã€åˆ›å»ºä»»ä½•è¿›ç¨‹ä¹‹å‰è®¾ç½®
    try:
        import multiprocessing
        if multiprocessing.get_start_method(allow_none=True) != 'spawn':
            multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        # å¦‚æœå·²ç»è®¾ç½®è¿‡å¯åŠ¨æ–¹æ³•ï¼Œå¿½ç•¥é”™è¯¯
        pass
    
    main()

