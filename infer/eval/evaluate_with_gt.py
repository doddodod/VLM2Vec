"""
åŸºäºå®Œæ•´GTæ•°æ®çš„è¯„ä¼°è„šæœ¬
ä»åŸå§‹GTæ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰GT pairï¼ŒåŸºäºå®Œæ•´çš„GT pairè®¡ç®—å¬å›ç‡
"""

import json
import argparse
from collections import defaultdict
import numpy as np
from typing import Dict, List, Tuple, Set

# Baseå’ŒNovelç±»è°“è¯åˆ†ç±»æ˜ å°„
PREDICATE_CATEGORY_MAPPING = {
    "above": "base", "across": "novel", "against": "base", "along": "novel", "and": "novel",
    "at": "base", "attached to": "base", "behind": "base", "belonging to": "base", "between": "base",
    "carrying": "base", "covered in": "base", "covering": "base", "eating": "novel", "flying in": "novel",
    "for": "base", "from": "base", "growing on": "novel", "hanging from": "base", "has": "base",
    "holding": "base", "in": "base", "in front of": "base", "laying on": "novel", "looking at": "base",
    "lying on": "novel", "made of": "base", "mounted on": "novel", "near": "base", "of": "base",
    "on": "base", "on back of": "novel", "over": "base", "painted on": "novel", "parked on": "base",
    "part of": "novel", "playing": "base", "riding": "base", "says": "novel", "sitting on": "base",
    "standing on": "base", "to": "base", "under": "base", "using": "novel", "walking in": "novel",
    "walking on": "base", "watching": "base", "wearing": "base", "wears": "base", "with": "base",
    "no relation": "base"  # æ·»åŠ no relationï¼Œè™½ç„¶é€šå¸¸ä¸å‚ä¸ç»Ÿè®¡
}


def load_gt_data(gt_file: str) -> Dict:
    """
    ä»GTæ–‡ä»¶ä¸­åŠ è½½æ‰€æœ‰GT pair
    
    Args:
        gt_file: GTæ–‡ä»¶è·¯å¾„
        
    Returns:
        å­—å…¸ï¼Œkeyä¸ºimage_idï¼Œvalueä¸ºè¯¥å›¾ç‰‡çš„æ‰€æœ‰GT pairé›†åˆ
        GT pairæ ¼å¼: (subject_id, object_id, predicate) ç”¨äºåŒºåˆ†åŒåç‰©ä½“
        åŒæ—¶ä¿å­˜ç±»åˆ«åæ˜ å°„: {image_id: {subject_id: class_name, ...}}
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½GTæ–‡ä»¶: {gt_file}")
    with open(gt_file, 'r', encoding='utf-8') as f:
        gt_data = json.load(f)
    
    gt_pairs_per_image = {}
    object_id_to_class = {}  # {image_id: {object_id: class_name}}
    
    for item in gt_data:
        image_id = item['image_id']
        objects = {obj['id']: obj['class_name'] for obj in item['objects']}
        relations = item['relations']
        object_id_to_class[image_id] = objects
        
        # æ„å»ºè¯¥å›¾ç‰‡çš„æ‰€æœ‰GT pairï¼Œä½¿ç”¨ç‰©ä½“IDåŒºåˆ†åŒåç‰©ä½“
        gt_pairs = set()
        for rel in relations:
            subject_id = rel['subject_id']
            object_id = rel['object_id']
            predicate = rel['predicate']
            
            if subject_id in objects and object_id in objects:
                # ä½¿ç”¨ (subject_id, object_id, predicate) ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œä»¥åŒºåˆ†åŒåç‰©ä½“
                gt_pairs.add((subject_id, object_id, predicate))
        
        gt_pairs_per_image[image_id] = gt_pairs
    
    print(f"âœ… GTæ–‡ä»¶åŠ è½½å®Œæˆï¼Œå…± {len(gt_pairs_per_image)} å¼ å›¾ç‰‡")
    total_gt_pairs = sum(len(pairs) for pairs in gt_pairs_per_image.values())
    print(f"   æ€»GT pairæ•°: {total_gt_pairs}\n")
    
    # è¿”å›GT pairså’Œç‰©ä½“IDåˆ°ç±»åˆ«åçš„æ˜ å°„
    return gt_pairs_per_image, object_id_to_class


def load_results(json_path: str) -> Dict:
    """åŠ è½½é¢„æµ‹ç»“æœJSONæ–‡ä»¶"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½ç»“æœæ–‡ä»¶: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… åŠ è½½å®Œæˆ\n")
    return data


def get_per_image_candidates(data: Dict) -> Dict:
    """
    è·å–æŒ‰å›¾ç‰‡åˆ†ç»„çš„å€™é€‰åˆ—è¡¨
    
    Returns:
        å­—å…¸ï¼Œkeyä¸ºimage_idï¼Œvalueä¸ºè¯¥å›¾ç‰‡çš„æ‰€æœ‰å€™é€‰åˆ—è¡¨
    """
    per_image_candidates = {}
    
    if 'per_image_top100_candidates' in data:
        print("   ä½¿ç”¨ per_image_top100_candidates å­—æ®µ...")
        per_image_candidates = data['per_image_top100_candidates']
    elif 'all_candidates' in data:
        print("   ä½¿ç”¨ all_candidates å­—æ®µ...")
        all_candidates = data['all_candidates']
        # æŒ‰ image_id åˆ†ç»„
        per_image_candidates_list = defaultdict(list)
        for cand in all_candidates:
            per_image_candidates_list[cand['image_id']].append(cand)
        per_image_candidates = dict(per_image_candidates_list)
    else:
        print("âš ï¸  JSONä¸­æ²¡æœ‰ä¿å­˜å€™é€‰åˆ—è¡¨ (per_image_top100_candidates æˆ– all_candidates å­—æ®µç¼ºå¤±)")
        return None
    
    return per_image_candidates


def calculate_recall_with_full_gt(
    data: Dict, 
    gt_pairs_per_image: Dict,
    object_id_to_class: Dict,
    k_values: List[int] = [50, 100]
) -> Dict:
    """
    åŸºäºå®Œæ•´GTæ•°æ®è®¡ç®—å¬å›ç‡
    
    Args:
        data: é¢„æµ‹ç»“æœæ•°æ®
        gt_pairs_per_image: æ¯å¼ å›¾ç‰‡çš„å®Œæ•´GT pairé›†åˆ
        k_values: Kå€¼åˆ—è¡¨
        
    Returns:
        åŒ…å«å¬å›ç‡ç»Ÿè®¡çš„å­—å…¸
    """
    print(f"ğŸ“Š åŸºäºå®Œæ•´GTæ•°æ®è®¡ç®—Top-Kå¬å›ç‡ (K={k_values})...")
    
    per_image_candidates = get_per_image_candidates(data)
    if per_image_candidates is None:
        return None
    
    results = {}
    
    for k in k_values:
        print(f"\n   è®¡ç®— Recall@{k}...")
        
        total_gt_pairs = 0  # æ‰€æœ‰GTä¸­çš„pairæ€»æ•°ï¼ˆå®Œæ•´GTï¼‰
        total_recalled_pairs = 0  # åœ¨top-kä¸­è¢«å¬å›çš„pairæ•°
        total_gt_pairs_in_candidates = 0  # åœ¨å€™é€‰åˆ—è¡¨ä¸­çš„GT pairæ•°
        total_recalled_pairs_in_candidates = 0  # åœ¨å€™é€‰åˆ—è¡¨ä¸­ä¸”è¢«å¬å›çš„pairæ•°
        
        # Baseå’ŒNovelç±»åˆ†åˆ«ç»Ÿè®¡
        total_gt_pairs_base = 0
        total_recalled_pairs_base = 0
        total_gt_pairs_novel = 0
        total_recalled_pairs_novel = 0
        
        # ç”¨äºç»Ÿè®¡æ¯å¼ å›¾ç‰‡çš„æƒ…å†µ
        image_recalls = []
        image_recalls_in_candidates = []  # åŸºäºå€™é€‰åˆ—è¡¨ä¸­çš„GT pairçš„å¬å›ç‡
        image_recalls_base = []  # Baseç±»å¬å›ç‡
        image_recalls_novel = []  # Novelç±»å¬å›ç‡
        
        for image_id_str, candidates in per_image_candidates.items():
            # ç»Ÿä¸€image_idç±»å‹ï¼ˆè½¬æ¢ä¸ºæ•´æ•°ï¼‰
            try:
                image_id = int(image_id_str)
            except (ValueError, TypeError):
                image_id = image_id_str
            
            # è·å–è¯¥å›¾ç‰‡çš„å®Œæ•´GT pairé›†åˆ
            full_gt_pairs = gt_pairs_per_image.get(image_id, set())
            
            if len(full_gt_pairs) == 0:
                continue
            
            # è·å–è¯¥å›¾ç‰‡çš„ç‰©ä½“IDåˆ°ç±»åˆ«åæ˜ å°„
            image_object_map = object_id_to_class.get(image_id, {})
            
            # æ„å»ºå€™é€‰åˆ—è¡¨ä¸­çš„GT pairé›†åˆï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            gt_pairs_in_candidates = set()
            for cand in candidates:
                has_gt = cand.get('has_gt', False)
                if has_gt and cand.get('relation_idx', -1) >= 0:
                    # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»åˆ«å
                    subject_id = cand.get('subject_id', None)
                    object_id = cand.get('object_id', None)
                    predicate = cand.get('gt_predicate', '')
                    if subject_id is not None and object_id is not None and predicate:
                        gt_pairs_in_candidates.add((subject_id, object_id, predicate))
                    else:
                        # å‘åå…¼å®¹ï¼šä½¿ç”¨ç±»åˆ«å
                        subject = cand.get('subject', '')
                        object_name = cand.get('object', '')
                        if subject and object_name and predicate:
                            gt_pairs_in_candidates.add((subject, object_name, predicate))
            
            # è¿‡æ»¤æ‰no relationçš„é¢„æµ‹
            non_bg_candidates = []
            for cand in candidates:
                if cand.get('predicted_predicate') != 'no relation':
                    non_bg_candidates.append(cand)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
            sorted_candidates = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
            top_k = sorted_candidates[:min(k, len(sorted_candidates))]
            
            # ç»Ÿè®¡åœ¨top-kä¸­ç¡®å®å­˜åœ¨äºå®Œæ•´GTä¸­çš„pair
            recalled_pairs = set()
            for cand in top_k:
                # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDè¿›è¡ŒåŒ¹é…
                subject_id = cand.get('subject_id', None)
                object_id = cand.get('object_id', None)
                predicate = cand.get('predicted_predicate', '')
                
                if subject_id is not None and object_id is not None and predicate:
                    pair_key = (subject_id, object_id, predicate)
                    if pair_key in full_gt_pairs:
                        recalled_pairs.add(pair_key)
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ç‰©ä½“IDï¼Œå°è¯•ä½¿ç”¨ç±»åˆ«ååŒ¹é…
                    # éœ€è¦å°†ç±»åˆ«åè½¬æ¢ä¸ºç‰©ä½“IDï¼ˆå¯èƒ½ä¸å‡†ç¡®ï¼Œå› ä¸ºå¯èƒ½æœ‰å¤šä¸ªåŒåç‰©ä½“ï¼‰
                    subject = cand.get('subject', '')
                    object_name = cand.get('object', '')
                    if subject and object_name and predicate:
                        # å°è¯•æ‰¾åˆ°åŒ¹é…çš„ç‰©ä½“IDå¯¹
                        found_match = False
                        for gt_subj_id, gt_obj_id, gt_pred in full_gt_pairs:
                            if (gt_pred == predicate and 
                                image_object_map.get(gt_subj_id) == subject and 
                                image_object_map.get(gt_obj_id) == object_name):
                                recalled_pairs.add((gt_subj_id, gt_obj_id, predicate))
                                found_match = True
                                break
            
            # åˆ†åˆ«ç»Ÿè®¡Baseå’ŒNovelç±»çš„GT pairs
            gt_pairs_base = set()
            gt_pairs_novel = set()
            for pair in full_gt_pairs:
                predicate = pair[2]  # pairæ ¼å¼: (subject_id, object_id, predicate)
                category = PREDICATE_CATEGORY_MAPPING.get(predicate, "base")  # é»˜è®¤base
                if category == "base":
                    gt_pairs_base.add(pair)
                elif category == "novel":
                    gt_pairs_novel.add(pair)
            
            # åˆ†åˆ«ç»Ÿè®¡Baseå’ŒNovelç±»çš„å¬å›
            recalled_pairs_base = recalled_pairs & gt_pairs_base
            recalled_pairs_novel = recalled_pairs & gt_pairs_novel
            
            # è®¡ç®—åŸºäºå®Œæ•´GTçš„å¬å›ç‡
            recalled_count = len(recalled_pairs)
            gt_count = len(full_gt_pairs)
            recall = recalled_count / gt_count if gt_count > 0 else 0.0
            
            # è®¡ç®—Baseå’ŒNovelç±»çš„å¬å›ç‡
            gt_count_base = len(gt_pairs_base)
            recalled_count_base = len(recalled_pairs_base)
            recall_base = recalled_count_base / gt_count_base if gt_count_base > 0 else 0.0
            
            gt_count_novel = len(gt_pairs_novel)
            recalled_count_novel = len(recalled_pairs_novel)
            recall_novel = recalled_count_novel / gt_count_novel if gt_count_novel > 0 else 0.0
            
            image_recalls.append(recall)
            if gt_count_base > 0:
                image_recalls_base.append(recall_base)
            if gt_count_novel > 0:
                image_recalls_novel.append(recall_novel)
            
            total_gt_pairs += gt_count
            total_recalled_pairs += recalled_count
            total_gt_pairs_base += gt_count_base
            total_recalled_pairs_base += recalled_count_base
            total_gt_pairs_novel += gt_count_novel
            total_recalled_pairs_novel += recalled_count_novel
            
            # è®¡ç®—åŸºäºå€™é€‰åˆ—è¡¨ä¸­GT pairçš„å¬å›ç‡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            recalled_in_candidates = len(recalled_pairs & gt_pairs_in_candidates)
            gt_in_candidates_count = len(gt_pairs_in_candidates)
            recall_in_candidates = recalled_in_candidates / gt_in_candidates_count if gt_in_candidates_count > 0 else 0.0
            
            image_recalls_in_candidates.append(recall_in_candidates)
            total_gt_pairs_in_candidates += gt_in_candidates_count
            total_recalled_pairs_in_candidates += recalled_in_candidates
        
        # è®¡ç®—å¹³å‡å¬å›ç‡å’Œæ•´ä½“å¬å›ç‡
        avg_recall = np.mean(image_recalls) if image_recalls else 0.0
        overall_recall = total_recalled_pairs / total_gt_pairs if total_gt_pairs > 0 else 0.0
        
        # Baseå’ŒNovelç±»çš„å¹³å‡å¬å›ç‡å’Œæ•´ä½“å¬å›ç‡
        avg_recall_base = np.mean(image_recalls_base) if image_recalls_base else 0.0
        overall_recall_base = total_recalled_pairs_base / total_gt_pairs_base if total_gt_pairs_base > 0 else 0.0
        avg_recall_novel = np.mean(image_recalls_novel) if image_recalls_novel else 0.0
        overall_recall_novel = total_recalled_pairs_novel / total_gt_pairs_novel if total_gt_pairs_novel > 0 else 0.0
        
        # åŸºäºå€™é€‰åˆ—è¡¨ä¸­çš„GT pairçš„å¬å›ç‡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        avg_recall_in_candidates = np.mean(image_recalls_in_candidates) if image_recalls_in_candidates else 0.0
        overall_recall_in_candidates = total_recalled_pairs_in_candidates / total_gt_pairs_in_candidates if total_gt_pairs_in_candidates > 0 else 0.0
        
        # Stage1è¦†ç›–ç‡ï¼šæœ‰å¤šå°‘GT pairè¿›å…¥äº†å€™é€‰åˆ—è¡¨
        stage1_coverage = total_gt_pairs_in_candidates / total_gt_pairs if total_gt_pairs > 0 else 0.0
        
        results[f'recall@{k}'] = {
            'avg_recall': avg_recall,
            'overall_recall': overall_recall,
            'total_gt_pairs': total_gt_pairs,  # å®Œæ•´GT pairæ•°
            'total_recalled_pairs': total_recalled_pairs,
            'num_images': len(image_recalls),
            'image_recalls': image_recalls,
            # Baseå’ŒNovelç±»ç»Ÿè®¡
            'avg_recall_base': avg_recall_base,
            'overall_recall_base': overall_recall_base,
            'total_gt_pairs_base': total_gt_pairs_base,
            'total_recalled_pairs_base': total_recalled_pairs_base,
            'avg_recall_novel': avg_recall_novel,
            'overall_recall_novel': overall_recall_novel,
            'total_gt_pairs_novel': total_gt_pairs_novel,
            'total_recalled_pairs_novel': total_recalled_pairs_novel,
            # å¯¹æ¯”æŒ‡æ ‡ï¼šåŸºäºå€™é€‰åˆ—è¡¨ä¸­çš„GT pair
            'avg_recall_in_candidates': avg_recall_in_candidates,
            'overall_recall_in_candidates': overall_recall_in_candidates,
            'total_gt_pairs_in_candidates': total_gt_pairs_in_candidates,
            'total_recalled_pairs_in_candidates': total_recalled_pairs_in_candidates,
            # Stage1è¦†ç›–ç‡
            'stage1_coverage': stage1_coverage
        }
        
        print(f"   åŸºäºå®Œæ•´GTæ•°æ®:")
        print(f"     å¹³å‡å¬å›ç‡: {avg_recall:.4f} ({avg_recall*100:.2f}%)")
        print(f"     æ•´ä½“å¬å›ç‡: {overall_recall:.4f} ({overall_recall*100:.2f}%)")
        print(f"     ç»Ÿè®¡: {total_recalled_pairs}/{total_gt_pairs} pairsè¢«å¬å›ï¼Œå…±{len(image_recalls)}å¼ å›¾ç‰‡")
        print(f"   Baseç±»è°“è¯:")
        print(f"     å¹³å‡å¬å›ç‡: {avg_recall_base:.4f} ({avg_recall_base*100:.2f}%)")
        print(f"     æ•´ä½“å¬å›ç‡: {overall_recall_base:.4f} ({overall_recall_base*100:.2f}%)")
        print(f"     ç»Ÿè®¡: {total_recalled_pairs_base}/{total_gt_pairs_base} pairsè¢«å¬å›")
        print(f"   Novelç±»è°“è¯:")
        print(f"     å¹³å‡å¬å›ç‡: {avg_recall_novel:.4f} ({avg_recall_novel*100:.2f}%)")
        print(f"     æ•´ä½“å¬å›ç‡: {overall_recall_novel:.4f} ({overall_recall_novel*100:.2f}%)")
        print(f"     ç»Ÿè®¡: {total_recalled_pairs_novel}/{total_gt_pairs_novel} pairsè¢«å¬å›")
        print(f"   åŸºäºå€™é€‰åˆ—è¡¨ä¸­çš„GT pair (å¯¹æ¯”):")
        print(f"     å¹³å‡å¬å›ç‡: {avg_recall_in_candidates:.4f} ({avg_recall_in_candidates*100:.2f}%)")
        print(f"     æ•´ä½“å¬å›ç‡: {overall_recall_in_candidates:.4f} ({overall_recall_in_candidates*100:.2f}%)")
        print(f"     ç»Ÿè®¡: {total_recalled_pairs_in_candidates}/{total_gt_pairs_in_candidates} pairsè¢«å¬å›")
        print(f"   Stage1è¦†ç›–ç‡: {stage1_coverage:.4f} ({stage1_coverage*100:.2f}%) - {total_gt_pairs_in_candidates}/{total_gt_pairs} GT pairsè¿›å…¥å€™é€‰åˆ—è¡¨")
    
    print()
    return results


def calculate_confusion_matrix_with_full_gt(
    data: Dict,
    gt_pairs_per_image: Dict,
    object_id_to_class: Dict,
    k_values: List[int] = [50, 100]
) -> Dict:
    """
    åŸºäºå®Œæ•´GTæ•°æ®è®¡ç®—æ··æ·†çŸ©é˜µ
    
    Args:
        data: é¢„æµ‹ç»“æœæ•°æ®
        gt_pairs_per_image: æ¯å¼ å›¾ç‰‡çš„å®Œæ•´GT pairé›†åˆ
        k_values: Kå€¼åˆ—è¡¨
        
    Returns:
        åŒ…å«æ··æ·†çŸ©é˜µçš„å­—å…¸
    """
    print(f"ğŸ“Š åŸºäºå®Œæ•´GTæ•°æ®è®¡ç®—æ··æ·†çŸ©é˜µ (K={k_values})...")
    
    per_image_candidates = get_per_image_candidates(data)
    if per_image_candidates is None:
        return None
    
    results = {}
    
    for k in k_values:
        print(f"\n   è®¡ç®— K={k} çš„æ··æ·†çŸ©é˜µ...")
        
        tp = 0  # True Positive: å®Œæ•´GTä¸­çš„pairè¢«å¬å›äº†
        fn = 0  # False Negative: å®Œæ•´GTä¸­çš„pairæ²¡æœ‰è¢«å¬å›
        fp = 0  # False Positive: top-kä¸­çš„pairä¸åœ¨å®Œæ•´GTä¸­
        tn = 0  # True Negative: top-kä¸­ä¸åœ¨å®Œæ•´GTä¸­çš„pairï¼ˆrelation_idx == -1ï¼‰
        
        # æ¯å¼ å›¾ç‰‡å†…çš„æŒ‡æ ‡ç»Ÿè®¡
        image_recalls = []
        image_precisions = []
        image_f1s = []
        
        for image_id_str, candidates in per_image_candidates.items():
            # ç»Ÿä¸€image_idç±»å‹ï¼ˆè½¬æ¢ä¸ºæ•´æ•°ï¼‰
            try:
                image_id = int(image_id_str)
            except (ValueError, TypeError):
                image_id = image_id_str
            
            # è·å–è¯¥å›¾ç‰‡çš„å®Œæ•´GT pairé›†åˆ
            full_gt_pairs = gt_pairs_per_image.get(image_id, set())
            
            if len(full_gt_pairs) == 0:
                continue
            
            # è·å–è¯¥å›¾ç‰‡çš„ç‰©ä½“IDåˆ°ç±»åˆ«åæ˜ å°„
            image_object_map = object_id_to_class.get(image_id, {})
            
            # è¿‡æ»¤æ‰no relationçš„é¢„æµ‹
            non_bg_candidates = []
            for cand in candidates:
                if cand.get('predicted_predicate') != 'no relation':
                    non_bg_candidates.append(cand)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
            sorted_candidates = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
            top_k = sorted_candidates[:min(k, len(sorted_candidates))]
            
            # ç»Ÿè®¡top-kä¸­çš„pairï¼ˆä½¿ç”¨ç‰©ä½“IDï¼‰
            top_k_pairs = set()
            top_k_non_gt_pairs = set()
            
            for cand in top_k:
                # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“ID
                subject_id = cand.get('subject_id', None)
                object_id = cand.get('object_id', None)
                predicate = cand.get('predicted_predicate', '')
                
                if subject_id is not None and object_id is not None and predicate:
                    pair_key = (subject_id, object_id, predicate)
                    
                    relation_idx = cand.get('relation_idx', -1)
                    if relation_idx >= 0:  # åœ¨å€™é€‰åˆ—è¡¨çš„GTä¸­
                        top_k_pairs.add(pair_key)
                    else:
                        top_k_non_gt_pairs.add(pair_key)
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ç‰©ä½“IDï¼Œå°è¯•ä½¿ç”¨ç±»åˆ«å
                    subject = cand.get('subject', '')
                    object_name = cand.get('object', '')
                    if subject and object_name and predicate:
                        # å°è¯•æ‰¾åˆ°åŒ¹é…çš„ç‰©ä½“IDå¯¹
                        found_match = False
                        for gt_subj_id, gt_obj_id, gt_pred in full_gt_pairs:
                            if (gt_pred == predicate and 
                                image_object_map.get(gt_subj_id) == subject and 
                                image_object_map.get(gt_obj_id) == object_name):
                                pair_key = (gt_subj_id, gt_obj_id, predicate)
                                relation_idx = cand.get('relation_idx', -1)
                                if relation_idx >= 0:
                                    top_k_pairs.add(pair_key)
                                else:
                                    top_k_non_gt_pairs.add(pair_key)
                                found_match = True
                                break
                        if not found_match:
                            # å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…ï¼Œä½¿ç”¨ç±»åˆ«åï¼ˆå¯èƒ½ä¸å‡†ç¡®ï¼‰
                            pair_key = (subject, object_name, predicate)
                            relation_idx = cand.get('relation_idx', -1)
                            if relation_idx >= 0:
                                top_k_pairs.add(pair_key)
                            else:
                                top_k_non_gt_pairs.add(pair_key)
            
            # è®¡ç®—å½“å‰å›¾ç‰‡çš„TPã€FNã€FPã€TN
            # TP: å®Œæ•´GTä¸­çš„pairåœ¨top-kä¸­å‡ºç°äº†
            image_tp = len(full_gt_pairs & top_k_pairs)
            # FN: å®Œæ•´GTä¸­çš„pairåœ¨top-kä¸­æ²¡æœ‰å‡ºç°
            image_fn = len(full_gt_pairs - top_k_pairs)
            # FP: top-kä¸­çš„pairä¸åœ¨å®Œæ•´GTä¸­
            image_fp = len(top_k_pairs - full_gt_pairs)
            # TN: top-kä¸­ä¸åœ¨å®Œæ•´GTä¸­çš„pair
            image_tn = len(top_k_non_gt_pairs)
            
            # ç´¯è®¡åˆ°å…¨å±€ç»Ÿè®¡
            tp += image_tp
            fn += image_fn
            fp += image_fp
            tn += image_tn
            
            # è®¡ç®—å½“å‰å›¾ç‰‡çš„æŒ‡æ ‡
            image_total_gt = image_tp + image_fn
            
            image_recall = 0.0
            image_precision = 0.0
            
            if image_total_gt > 0:
                image_recall = image_tp / image_total_gt
                image_recalls.append(image_recall)
            
            image_total_topk = image_tp + image_fp
            if image_total_topk > 0:
                image_precision = image_tp / image_total_topk
                image_precisions.append(image_precision)
            
            # è®¡ç®—F1åˆ†æ•°
            if image_precision > 0 and image_recall > 0:
                image_f1 = 2 * image_precision * image_recall / (image_precision + image_recall)
                image_f1s.append(image_f1)
        
        # æ„å»º2x2æ··æ·†çŸ©é˜µ
        cm = np.array([[tp, fn],
                       [fp, tn]])
        
        # è®¡ç®—æŒ‡æ ‡
        total_gt_pairs = tp + fn  # å®Œæ•´GTä¸­çš„pairæ€»æ•°
        total_top_k_pairs = tp + fp  # top-kä¸­åœ¨å®Œæ•´GTä¸­çš„pairæ€»æ•°
        
        # æ•´ä½“æŒ‡æ ‡
        recall_overall = tp / total_gt_pairs if total_gt_pairs > 0 else 0.0
        precision_overall = tp / total_top_k_pairs if total_top_k_pairs > 0 else 0.0
        f1_overall = 2 * precision_overall * recall_overall / (precision_overall + recall_overall) if (precision_overall + recall_overall) > 0 else 0.0
        
        # å¹³å‡æŒ‡æ ‡
        recall_avg = np.mean(image_recalls) if image_recalls else 0.0
        precision_avg = np.mean(image_precisions) if image_precisions else 0.0
        f1_avg = np.mean(image_f1s) if image_f1s else 0.0
        
        results[f'confusion_matrix@{k}'] = {
            'matrix': cm.tolist(),
            'tp': int(tp),
            'fp': int(fp),
            'fn': int(fn),
            'tn': int(tn),
            'total_gt_pairs': int(total_gt_pairs),
            'total_top_k_pairs': int(total_top_k_pairs),
            'recall_overall': recall_overall,
            'precision_overall': precision_overall,
            'f1_overall': f1_overall,
            'recall_avg': recall_avg,
            'precision_avg': precision_avg,
            'f1_avg': f1_avg
        }
        
        print(f"   æ··æ·†çŸ©é˜µ (2x2):")
        print(f"               é¢„æµ‹ä¸ºæ­£ä¾‹(åœ¨top-k)  é¢„æµ‹ä¸ºè´Ÿä¾‹(ä¸åœ¨top-k)")
        print(f"   å®é™…ä¸ºæ­£ä¾‹(åœ¨GT)    TP={tp:6d}        FN={fn:6d}")
        print(f"   å®é™…ä¸ºè´Ÿä¾‹(ä¸åœ¨GT)  FP={fp:6d}        TN={tn:6d}")
        print(f"   æŒ‡æ ‡ (æ¯å¼ å›¾ç‰‡å¹³å‡):")
        print(f"     å¬å›ç‡ (Recall): {recall_avg:.4f} ({recall_avg*100:.2f}%)")
        print(f"     ç²¾ç¡®ç‡ (Precision): {precision_avg:.4f} ({precision_avg*100:.2f}%)")
        print(f"     F1åˆ†æ•°: {f1_avg:.4f}")
        print(f"   æŒ‡æ ‡ (æ•´ä½“ç´¯è®¡):")
        print(f"     å¬å›ç‡ (Recall): {recall_overall:.4f} ({recall_overall*100:.2f}%)")
        print(f"     ç²¾ç¡®ç‡ (Precision): {precision_overall:.4f} ({precision_overall*100:.2f}%)")
        print(f"     F1åˆ†æ•°: {f1_overall:.4f}")
        print(f"   ç»Ÿè®¡: å®Œæ•´GTä¸­æœ‰{total_gt_pairs}ä¸ªpair, top-kä¸­æœ‰{total_top_k_pairs}ä¸ªGT pair")
    
    print()
    return results


def calculate_mean_recall_with_full_gt(
    data: Dict,
    gt_pairs_per_image: Dict,
    object_id_to_class: Dict,
    k_values: List[int] = [50, 100]
) -> Dict:
    """
    åŸºäºå®Œæ•´GTæ•°æ®è®¡ç®—æ¯ä¸ªè°“è¯ç±»åˆ«çš„Mean Recall
    
    Args:
        data: é¢„æµ‹ç»“æœæ•°æ®
        gt_pairs_per_image: æ¯å¼ å›¾ç‰‡çš„å®Œæ•´GT pairé›†åˆ
        k_values: Kå€¼åˆ—è¡¨
        
    Returns:
        åŒ…å«mean recallç»Ÿè®¡çš„å­—å…¸
    """
    print(f"ğŸ“Š åŸºäºå®Œæ•´GTæ•°æ®è®¡ç®—è°“è¯çº§åˆ« Mean Recall (K={k_values})...")
    
    per_image_candidates = get_per_image_candidates(data)
    if per_image_candidates is None:
        return None
    
    # ä»GTæ•°æ®ä¸­è·å–æ‰€æœ‰è°“è¯ç±»åˆ«
    all_predicates = set()
    for gt_pairs in gt_pairs_per_image.values():
        for pair in gt_pairs:
            predicate = pair[2]  # (subject, object, predicate)
            all_predicates.add(predicate)
    predicates = sorted(list(all_predicates))
    
    print(f"   å‘ç° {len(predicates)} ä¸ªè°“è¯ç±»åˆ«\n")
    
    results = {}
    
    for k in k_values:
        print(f"   è®¡ç®— Mean Recall@{k}...")
        
        # åˆå§‹åŒ–æ¯ä¸ªè°“è¯çš„ç»Ÿè®¡
        predicate_stats = {pred: {'hit': 0, 'total': 0} for pred in predicates}
        
        for image_id_str, candidates in per_image_candidates.items():
            # ç»Ÿä¸€image_idç±»å‹ï¼ˆè½¬æ¢ä¸ºæ•´æ•°ï¼‰
            try:
                image_id = int(image_id_str)
            except (ValueError, TypeError):
                image_id = image_id_str
            
            # è·å–è¯¥å›¾ç‰‡çš„å®Œæ•´GT pairé›†åˆ
            full_gt_pairs = gt_pairs_per_image.get(image_id, set())
            
            if len(full_gt_pairs) == 0:
                continue
            
            # è·å–è¯¥å›¾ç‰‡çš„ç‰©ä½“IDåˆ°ç±»åˆ«åæ˜ å°„
            image_object_map = object_id_to_class.get(image_id, {})
            
            # ç»Ÿè®¡è¯¥å›¾ç‰‡ä¸­æ¯ä¸ªè°“è¯ç±»åˆ«çš„GTæ€»æ•°
            gt_predicates_in_image = defaultdict(set)  # predicate -> set of pairs
            for pair in full_gt_pairs:
                predicate = pair[2]  # pairæ ¼å¼: (subject_id, object_id, predicate)
                gt_predicates_in_image[predicate].add(pair)
                predicate_stats[predicate]['total'] += 1
            
            # è¿‡æ»¤æ‰no relationçš„é¢„æµ‹
            non_bg_candidates = []
            for cand in candidates:
                if cand.get('predicted_predicate') != 'no relation':
                    non_bg_candidates.append(cand)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
            sorted_candidates = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
            top_k = sorted_candidates[:min(k, len(sorted_candidates))]
            
            # ç»Ÿè®¡åœ¨top-kä¸­è¢«å¬å›çš„è°“è¯pairï¼ˆä½¿ç”¨ç‰©ä½“IDï¼‰
            recalled_predicates_in_image = defaultdict(set)  # predicate -> set of recalled pairs
            
            for cand in top_k:
                # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“ID
                subject_id = cand.get('subject_id', None)
                object_id = cand.get('object_id', None)
                predicate = cand.get('predicted_predicate', '')
                
                if subject_id is not None and object_id is not None and predicate:
                    pair_key = (subject_id, object_id, predicate)
                    if pair_key in full_gt_pairs:
                        recalled_predicates_in_image[predicate].add(pair_key)
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ç‰©ä½“IDï¼Œå°è¯•ä½¿ç”¨ç±»åˆ«ååŒ¹é…
                    subject = cand.get('subject', '')
                    object_name = cand.get('object', '')
                    if subject and object_name and predicate:
                        # å°è¯•æ‰¾åˆ°åŒ¹é…çš„ç‰©ä½“IDå¯¹
                        for gt_subj_id, gt_obj_id, gt_pred in full_gt_pairs:
                            if (gt_pred == predicate and 
                                image_object_map.get(gt_subj_id) == subject and 
                                image_object_map.get(gt_obj_id) == object_name):
                                pair_key = (gt_subj_id, gt_obj_id, predicate)
                                recalled_predicates_in_image[predicate].add(pair_key)
                                break
            
            # ç»Ÿè®¡æ¯ä¸ªè°“è¯çš„å¬å›æ•°ï¼ˆç»Ÿè®¡æ‰€æœ‰è¢«å¬å›çš„pairï¼‰
            for predicate, recalled_pairs in recalled_predicates_in_image.items():
                if predicate in predicate_stats:
                    # ç»Ÿè®¡è¯¥è°“è¯åœ¨GTä¸­çš„pairè¢«å¬å›çš„æ•°é‡
                    gt_pairs_for_predicate = gt_predicates_in_image.get(predicate, set())
                    recalled_count = len(recalled_pairs & gt_pairs_for_predicate)
                    predicate_stats[predicate]['hit'] += recalled_count
        
        # è®¡ç®—æ¯ä¸ªè°“è¯çš„recall
        per_predicate_recall = {}
        valid_recalls = []
        valid_recalls_base = []  # Baseç±»è°“è¯çš„recall
        valid_recalls_novel = []  # Novelç±»è°“è¯çš„recall
        
        for pred in predicates:
            total = predicate_stats[pred]['total']
            hit = predicate_stats[pred]['hit']
            
            if total > 0:
                recall = hit / total
                category = PREDICATE_CATEGORY_MAPPING.get(pred, "base")  # é»˜è®¤base
                per_predicate_recall[pred] = {
                    'recall': recall,
                    'hit': hit,
                    'total': total,
                    'category': category
                }
                valid_recalls.append(recall)
                if category == "base":
                    valid_recalls_base.append(recall)
                elif category == "novel":
                    valid_recalls_novel.append(recall)
            else:
                category = PREDICATE_CATEGORY_MAPPING.get(pred, "base")
                per_predicate_recall[pred] = {
                    'recall': 0.0,
                    'hit': 0,
                    'total': 0,
                    'category': category
                }
        
        # è®¡ç®—mean recallï¼ˆåªå¯¹æœ‰GTçš„ç±»åˆ«è®¡ç®—ï¼‰
        mean_recall = np.mean(valid_recalls) if valid_recalls else 0.0
        mean_recall_base = np.mean(valid_recalls_base) if valid_recalls_base else 0.0
        mean_recall_novel = np.mean(valid_recalls_novel) if valid_recalls_novel else 0.0
        
        results[f'mean_recall@{k}'] = {
            'mean_recall': mean_recall,
            'num_valid_predicates': len(valid_recalls),
            'total_predicates': len(predicates),
            'per_predicate_recall': per_predicate_recall,
            # Baseå’ŒNovelç±»ç»Ÿè®¡
            'mean_recall_base': mean_recall_base,
            'num_valid_predicates_base': len(valid_recalls_base),
            'mean_recall_novel': mean_recall_novel,
            'num_valid_predicates_novel': len(valid_recalls_novel)
        }
        
        print(f"   Mean Recall@{k:3d}: {mean_recall:.4f} ({mean_recall*100:.2f}%), æœ‰æ•ˆè°“è¯: {len(valid_recalls)}/{len(predicates)}")
        print(f"   Baseç±» Mean Recall@{k:3d}: {mean_recall_base:.4f} ({mean_recall_base*100:.2f}%), æœ‰æ•ˆè°“è¯: {len(valid_recalls_base)}")
        print(f"   Novelç±» Mean Recall@{k:3d}: {mean_recall_novel:.4f} ({mean_recall_novel*100:.2f}%), æœ‰æ•ˆè°“è¯: {len(valid_recalls_novel)}")
    
    print()
    return results


def print_summary(recall_results: Dict, cm_results: Dict, mean_recall_results: Dict = None) -> None:
    """æ‰“å°æ€»ç»“ä¿¡æ¯"""
    print("="*80)
    print("ğŸ“‹ æ€»ç»“æŠ¥å‘Šï¼ˆåŸºäºå®Œæ•´GTæ•°æ®ï¼‰")
    print("="*80)
    
    print(f"\nå¬å›ç‡ç»Ÿè®¡ï¼ˆåŸºäºå®Œæ•´GTæ•°æ®ï¼‰:")
    for k in [50, 100]:
        key = f'recall@{k}'
        if key in recall_results:
            result = recall_results[key]
            print(f"  Recall@{k}:")
            print(f"    åŸºäºå®Œæ•´GTæ•°æ®:")
            print(f"      å¹³å‡å¬å›ç‡: {result['avg_recall']:.4f} ({result['avg_recall']*100:.2f}%)")
            print(f"      æ•´ä½“å¬å›ç‡: {result['overall_recall']:.4f} ({result['overall_recall']*100:.2f}%)")
            print(f"      ç»Ÿè®¡: {result['total_recalled_pairs']}/{result['total_gt_pairs']} pairs")
            print(f"    Baseç±»è°“è¯:")
            print(f"      å¹³å‡å¬å›ç‡: {result['avg_recall_base']:.4f} ({result['avg_recall_base']*100:.2f}%)")
            print(f"      æ•´ä½“å¬å›ç‡: {result['overall_recall_base']:.4f} ({result['overall_recall_base']*100:.2f}%)")
            print(f"      ç»Ÿè®¡: {result['total_recalled_pairs_base']}/{result['total_gt_pairs_base']} pairs")
            print(f"    Novelç±»è°“è¯:")
            print(f"      å¹³å‡å¬å›ç‡: {result['avg_recall_novel']:.4f} ({result['avg_recall_novel']*100:.2f}%)")
            print(f"      æ•´ä½“å¬å›ç‡: {result['overall_recall_novel']:.4f} ({result['overall_recall_novel']*100:.2f}%)")
            print(f"      ç»Ÿè®¡: {result['total_recalled_pairs_novel']}/{result['total_gt_pairs_novel']} pairs")
            print(f"    åŸºäºå€™é€‰åˆ—è¡¨ä¸­çš„GT pair (å¯¹æ¯”):")
            print(f"      å¹³å‡å¬å›ç‡: {result['avg_recall_in_candidates']:.4f} ({result['avg_recall_in_candidates']*100:.2f}%)")
            print(f"      æ•´ä½“å¬å›ç‡: {result['overall_recall_in_candidates']:.4f} ({result['overall_recall_in_candidates']*100:.2f}%)")
            print(f"      ç»Ÿè®¡: {result['total_recalled_pairs_in_candidates']}/{result['total_gt_pairs_in_candidates']} pairs")
            print(f"    Stage1è¦†ç›–ç‡: {result['stage1_coverage']:.4f} ({result['stage1_coverage']*100:.2f}%)")
            print(f"    å›¾ç‰‡æ•°: {result['num_images']}")
    
    print(f"\næ··æ·†çŸ©é˜µç»Ÿè®¡ (åŸºäºå®Œæ•´GTæ•°æ®):")
    for k in [50, 100]:
        key = f'confusion_matrix@{k}'
        if key in cm_results:
            result = cm_results[key]
            print(f"  K={k}:")
            print(f"    TP (True Positive): {result['tp']}")
            print(f"    FN (False Negative): {result['fn']}")
            print(f"    FP (False Positive): {result['fp']}")
            print(f"    TN (True Negative): {result['tn']}")
            print(f"    æŒ‡æ ‡ (æ¯å¼ å›¾ç‰‡å¹³å‡):")
            print(f"      å¬å›ç‡ (Recall): {result['recall_avg']:.4f} ({result['recall_avg']*100:.2f}%)")
            print(f"      ç²¾ç¡®ç‡ (Precision): {result['precision_avg']:.4f} ({result['precision_avg']*100:.2f}%)")
            print(f"      F1åˆ†æ•°: {result['f1_avg']:.4f}")
            print(f"    æŒ‡æ ‡ (æ•´ä½“ç´¯è®¡):")
            print(f"      å¬å›ç‡ (Recall): {result['recall_overall']:.4f} ({result['recall_overall']*100:.2f}%)")
            print(f"      ç²¾ç¡®ç‡ (Precision): {result['precision_overall']:.4f} ({result['precision_overall']*100:.2f}%)")
            print(f"      F1åˆ†æ•°: {result['f1_overall']:.4f}")
            print(f"    å®Œæ•´GTä¸­pairæ€»æ•°: {result['total_gt_pairs']}")
            print(f"    Top-kä¸­GT pairæ€»æ•°: {result['total_top_k_pairs']}")
    
    if mean_recall_results:
        print(f"\nMean Recallç»Ÿè®¡ (åŸºäºå®Œæ•´GTæ•°æ®):")
        for k in [50, 100]:
            key = f'mean_recall@{k}'
            if key in mean_recall_results:
                result = mean_recall_results[key]
                print(f"  Mean Recall@{k}:")
                print(f"    Mean Recall: {result['mean_recall']:.4f} ({result['mean_recall']*100:.2f}%)")
                print(f"    æœ‰æ•ˆè°“è¯æ•°: {result['num_valid_predicates']}/{result['total_predicates']}")
                print(f"    Baseç±» Mean Recall: {result['mean_recall_base']:.4f} ({result['mean_recall_base']*100:.2f}%)")
                print(f"    Baseç±»æœ‰æ•ˆè°“è¯æ•°: {result['num_valid_predicates_base']}")
                print(f"    Novelç±» Mean Recall: {result['mean_recall_novel']:.4f} ({result['mean_recall_novel']*100:.2f}%)")
                print(f"    Novelç±»æœ‰æ•ˆè°“è¯æ•°: {result['num_valid_predicates_novel']}")
                
                # æ˜¾ç¤ºTop-10å’ŒBottom-10è°“è¯
                per_predicate = result['per_predicate_recall']
                sorted_predicates = sorted(
                    [(pred, stats) for pred, stats in per_predicate.items() if stats['total'] > 0],
                    key=lambda x: x[1]['recall'],
                    reverse=True
                )
                
                if len(sorted_predicates) > 0:
                    print(f"    Top-10 è°“è¯:")
                    for i, (pred, stats) in enumerate(sorted_predicates[:10], 1):
                        category = stats.get('category', 'unknown')
                        print(f"      {i:2d}. {pred:<20} [{category:5s}] Recall: {stats['recall']:.4f} ({stats['hit']}/{stats['total']})")
                    
                    if len(sorted_predicates) > 10:
                        print(f"    Bottom-10 è°“è¯:")
                        for i, (pred, stats) in enumerate(sorted_predicates[-10:], len(sorted_predicates)-9):
                            category = stats.get('category', 'unknown')
                            print(f"      {i:2d}. {pred:<20} [{category:5s}] Recall: {stats['recall']:.4f} ({stats['hit']}/{stats['total']})")
    
    print()


def export_results(recall_results: Dict, cm_results: Dict, mean_recall_results: Dict = None, output_path: str = None) -> None:
    """å¯¼å‡ºç»“æœåˆ°JSONæ–‡ä»¶"""
    print(f"ğŸ’¾ æ­£åœ¨å¯¼å‡ºç»“æœåˆ°: {output_path}")
    
    export_data = {
        'recall_results': recall_results,
        'confusion_matrix_results': cm_results
    }
    
    if mean_recall_results:
        export_data['mean_recall_results'] = mean_recall_results
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç»“æœå·²å¯¼å‡º\n")


def main():
    parser = argparse.ArgumentParser(
        description="åŸºäºå®Œæ•´GTæ•°æ®è¯„ä¼°åœºæ™¯å›¾å…³ç³»é¢„æµ‹ç»“æœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬è¯„ä¼°ï¼ˆé»˜è®¤è®¡ç®—top50å’Œtop100ï¼‰
  python evaluate_with_gt.py --json_file results.json --gt_file gt.json
  
  # è‡ªå®šä¹‰Kå€¼
  python evaluate_with_gt.py --json_file results.json --gt_file gt.json --k-values 50 100 200
  
  # å¯¼å‡ºç»“æœ
  python evaluate_with_gt.py --json_file results.json --gt_file gt.json --export results.json
        """
    )
    # 
    # 
# INPUT_FILE = "/public/home/xiaojw2025/Workspace/RAHP/DATASET/VG150/test_case_20.json"
# OUTPUT_FILE = "/public/home/xiaojw2025/Data/embedding_similarity/vlm2vec_qwen2vl/result_recall_20_all.json"

    parser.add_argument('--json_file', type=str, 
                       default='/public/home/wangby2025/plusLab/outputs/test_2000_recall/best_eval_simi_37k_base.json',
                       help='é¢„æµ‹ç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gt_file', type=str,
                       default='/public/home/wangby2025/plusLab/VLM2Vec/infer/test_2000_images.json',
                       help='GTæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--k-values', type=int, nargs='+', default=[50, 100],
                       help='æŒ‡å®šè¦è®¡ç®—çš„Kå€¼åˆ—è¡¨ï¼ˆé»˜è®¤: 50 100ï¼‰')
    parser.add_argument('--export', type=str, default=None,
                       help='å¯¼å‡ºç»“æœåˆ°æŒ‡å®šJSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åŠ è½½GTæ•°æ®ï¼ˆè¿”å›GT pairså’Œç‰©ä½“IDåˆ°ç±»åˆ«åçš„æ˜ å°„ï¼‰
    gt_pairs_per_image, object_id_to_class = load_gt_data(args.gt_file)
    
    # åŠ è½½ç»“æœ
    data = load_results(args.json_file)
    
    # è®¡ç®—å¬å›ç‡
    recall_results = calculate_recall_with_full_gt(data, gt_pairs_per_image, object_id_to_class, args.k_values)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm_results = calculate_confusion_matrix_with_full_gt(data, gt_pairs_per_image, object_id_to_class, args.k_values)
    
    # è®¡ç®—Mean Recall
    mean_recall_results = calculate_mean_recall_with_full_gt(data, gt_pairs_per_image, object_id_to_class, args.k_values)
    
    # æ‰“å°æ€»ç»“
    print_summary(recall_results, cm_results, mean_recall_results)
    
    # å¯¼å‡ºç»“æœ
    if args.export:
        export_results(recall_results, cm_results, mean_recall_results, args.export)
    
    print("="*80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()

