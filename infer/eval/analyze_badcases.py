"""
Badcaseåˆ†æè„šæœ¬
ç”¨äºè¯¦ç»†åˆ†ææ¨ç†ç»“æœï¼Œç‰¹åˆ«æ˜¯æ¯ä¸ªGT pairçš„ç›¸ä¼¼åº¦åˆ†æ•°å’Œæ’å

ä¸»è¦åŠŸèƒ½ï¼š
1. å¯¹æ¯ä¸ªGT pairï¼Œæ˜¾ç¤ºGTè°“è¯åœ¨æ‰€æœ‰50ä¸ªè°“è¯ä¸­çš„ç›¸ä¼¼åº¦æ’å
2. æ˜¾ç¤ºGTè°“è¯çš„ç›¸ä¼¼åº¦åˆ†æ•°
3. åˆ†æbadcaseï¼ˆGTè°“è¯æ’åè¾ƒä½çš„æƒ…å†µï¼‰
4. æ”¯æŒæŒ‰å›¾ç‰‡ã€æŒ‰pairæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
"""

import json
import argparse
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import numpy as np

# 50ä¸ªè°“è¯åˆ—è¡¨ï¼ˆä¸æ¨ç†ä»£ç ä¿æŒä¸€è‡´ï¼‰
PREDICATES = [
    "above", "across", "against", "along", "and", "at", "attached to", "behind",
    "belonging to", "between", "carrying", "covered in", "covering", "eating",
    "flying in", "for", "from", "growing on", "hanging from", "has", "holding",
    "in", "in front of", "laying on", "looking at", "lying on", "made of",
    "mounted on", "near", "of", "on", "on back of", "over", "painted on",
    "parked on", "part of", "playing", "riding", "says", "sitting on",
    "standing on", "to", "under", "using", "walking in", "walking on",
    "watching", "wearing", "wears", "with", "no relation"
]


def load_results(json_path: str) -> Dict:
    """åŠ è½½é¢„æµ‹ç»“æœJSONæ–‡ä»¶"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½ç»“æœæ–‡ä»¶: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… åŠ è½½å®Œæˆ\n")
    return data


def get_all_candidates_for_pair(image_id, subject_id, object_id, subject, object_name, data: Dict) -> List[Dict]:
    """
    è·å–æŸä¸ªpairçš„æ‰€æœ‰50ä¸ªè°“è¯å€™é€‰ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
    
    Args:
        image_id: å›¾ç‰‡ID
        subject_id: ä¸»ä½“ç‰©ä½“ID
        object_id: å®¢ä½“ç‰©ä½“ID
        subject: ä¸»ä½“ç±»åˆ«å
        object_name: å®¢ä½“ç±»åˆ«å
        data: é¢„æµ‹ç»“æœæ•°æ®
    
    Returns:
        è¯¥pairçš„æ‰€æœ‰å€™é€‰åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
    """
    # å°è¯•å¤šç§image_idæ ¼å¼
    image_id_str = str(image_id)
    image_id_int = int(image_id) if isinstance(image_id, str) and image_id.isdigit() else image_id
    
    # ä¼˜å…ˆä½¿ç”¨per_image_all_candidatesï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨per_image_top100_candidates
    candidates = []
    if 'per_image_all_candidates' in data:
        candidates = data['per_image_all_candidates'].get(image_id_str, [])
        if not candidates:
            candidates = data['per_image_all_candidates'].get(image_id_int, [])
    elif 'per_image_top100_candidates' in data:
        candidates = data['per_image_top100_candidates'].get(image_id_str, [])
        if not candidates:
            candidates = data['per_image_top100_candidates'].get(image_id_int, [])
    
    if not candidates:
        return []
    
    # ç­›é€‰å‡ºè¯¥pairçš„æ‰€æœ‰å€™é€‰
    pair_candidates = []
    for cand in candidates:
        # ä¼˜å…ˆä½¿ç”¨ç‰©ä½“IDåŒ¹é…
        cand_subject_id = cand.get('subject_id', None)
        cand_object_id = cand.get('object_id', None)
        
        match = False
        if cand_subject_id is not None and cand_object_id is not None and subject_id is not None and object_id is not None:
            # ä½¿ç”¨ç‰©ä½“IDåŒ¹é…
            if cand_subject_id == subject_id and cand_object_id == object_id:
                match = True
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨ç±»åˆ«ååŒ¹é…
            if cand.get('subject') == subject and cand.get('object') == object_name:
                match = True
        
        if match:
            pair_candidates.append(cand)
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    pair_candidates.sort(key=lambda x: x.get('similarity', 0), reverse=True)
    
    return pair_candidates


def analyze_gt_pair_ranking(image_id, subject_id, object_id, subject, object_name, 
                           gt_predicate: str, data: Dict) -> Dict:
    """
    åˆ†æå•ä¸ªGT pairçš„ç›¸ä¼¼åº¦æ’åæƒ…å†µ
    
    Args:
        image_id: å›¾ç‰‡ID
        subject_id: ä¸»ä½“ç‰©ä½“ID
        object_id: å®¢ä½“ç‰©ä½“ID
        subject: ä¸»ä½“ç±»åˆ«å
        object_name: å®¢ä½“ç±»åˆ«å
        gt_predicate: GTè°“è¯
        data: é¢„æµ‹ç»“æœæ•°æ®
    
    Returns:
        åŒ…å«æ’åä¿¡æ¯çš„å­—å…¸
    """
    # è·å–è¯¥pairçš„æ‰€æœ‰å€™é€‰
    all_candidates = get_all_candidates_for_pair(
        image_id, subject_id, object_id, subject, object_name, data
    )
    
    if not all_candidates:
        return {
            'error': 'æœªæ‰¾åˆ°è¯¥pairçš„å€™é€‰æ•°æ®',
            'gt_predicate': gt_predicate,
            'gt_similarity': None,
            'gt_rank': None,
            'total_predicates': len(PREDICATES)
        }
    
    # æ‰¾åˆ°GTè°“è¯çš„ç›¸ä¼¼åº¦å’Œæ’å
    gt_similarity = None
    gt_rank = None
    
    # æ„å»ºè°“è¯åˆ°ç›¸ä¼¼åº¦çš„æ˜ å°„
    predicate_to_similarity = {}
    for cand in all_candidates:
        pred = cand.get('predicted_predicate', '')
        sim = cand.get('similarity', 0)
        if pred not in predicate_to_similarity:
            predicate_to_similarity[pred] = sim
        else:
            # å¦‚æœæœ‰å¤šä¸ªç›¸åŒè°“è¯ï¼Œå–æœ€å¤§ç›¸ä¼¼åº¦
            predicate_to_similarity[pred] = max(predicate_to_similarity[pred], sim)
    
    # è·å–GTè°“è¯çš„ç›¸ä¼¼åº¦
    if gt_predicate in predicate_to_similarity:
        gt_similarity = predicate_to_similarity[gt_predicate]
    else:
        # GTè°“è¯ä¸åœ¨å€™é€‰åˆ—è¡¨ä¸­
        return {
            'error': f'GTè°“è¯ "{gt_predicate}" ä¸åœ¨å€™é€‰åˆ—è¡¨ä¸­',
            'gt_predicate': gt_predicate,
            'gt_similarity': None,
            'gt_rank': None,
            'total_predicates': len(PREDICATES),
            'available_predicates': list(predicate_to_similarity.keys())
        }
    
    # è®¡ç®—æ’åï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰
    all_similarities = sorted(predicate_to_similarity.values(), reverse=True)
    gt_rank = all_similarities.index(gt_similarity) + 1  # æ’åä»1å¼€å§‹
    
    # è·å–Top-10è°“è¯
    top_predicates = sorted(
        predicate_to_similarity.items(),
        key=lambda x: x[1],
        reverse=True
    )[:10]
    
    return {
        'gt_predicate': gt_predicate,
        'gt_similarity': gt_similarity,
        'gt_rank': gt_rank,
        'total_predicates': len(predicate_to_similarity),
        'top_10_predicates': top_predicates,
        'all_predicates_ranked': sorted(
            predicate_to_similarity.items(),
            key=lambda x: x[1],
            reverse=True
        )
    }


def analyze_all_gt_pairs(data: Dict) -> List[Dict]:
    """
    åˆ†ææ‰€æœ‰GT pairçš„æ’åæƒ…å†µ
    
    Returns:
        æ¯ä¸ªGT pairçš„åˆ†æç»“æœåˆ—è¡¨
    """
    print("ğŸ“Š æ­£åœ¨åˆ†ææ‰€æœ‰GT pairçš„æ’åæƒ…å†µ...")
    
    # ä»all_relationsä¸­è·å–æ‰€æœ‰GTå…³ç³»
    all_relations = data.get('all_relations', [])
    
    results = []
    for rel in all_relations:
        image_id = rel['image_id']
        subject_id = rel.get('subject_id', None)
        object_id = rel.get('object_id', None)
        subject = rel['subject']
        object_name = rel['object']
        gt_predicate = rel['gt_predicate']
        
        analysis = analyze_gt_pair_ranking(
            image_id, subject_id, object_id, subject, object_name, gt_predicate, data
        )
        
        analysis['image_id'] = image_id
        analysis['subject_id'] = subject_id
        analysis['object_id'] = object_id
        analysis['subject'] = subject
        analysis['object'] = object_name
        
        results.append(analysis)
    
    print(f"âœ… åˆ†æäº† {len(results)} ä¸ªGT pair\n")
    return results


def print_pair_analysis(analysis: Dict, detailed: bool = False):
    """æ‰“å°å•ä¸ªpairçš„åˆ†æç»“æœ"""
    print(f"\n{'='*80}")
    print(f"å›¾ç‰‡ID: {analysis['image_id']}")
    print(f"é…å¯¹: {analysis['subject']} (ID:{analysis['subject_id']}) -> {analysis['object']} (ID:{analysis['object_id']})")
    print(f"GTè°“è¯: {analysis['gt_predicate']}")
    print(f"{'='*80}")
    
    if 'error' in analysis:
        print(f"âŒ é”™è¯¯: {analysis['error']}")
        if 'available_predicates' in analysis:
            print(f"   å¯ç”¨è°“è¯: {', '.join(analysis['available_predicates'][:10])}...")
        return
    
    print(f"GTè°“è¯ç›¸ä¼¼åº¦: {analysis['gt_similarity']:.6f}")
    print(f"GTè°“è¯æ’å: {analysis['gt_rank']}/{analysis['total_predicates']}")
    
    if analysis['gt_rank'] <= 10:
        print(f"âœ… GTè°“è¯æ’åè¾ƒé«˜ï¼ˆTop-10ï¼‰")
    elif analysis['gt_rank'] <= 20:
        print(f"âš ï¸  GTè°“è¯æ’åä¸­ç­‰ï¼ˆTop-20ï¼‰")
    else:
        print(f"âŒ GTè°“è¯æ’åè¾ƒä½ï¼ˆTop-{analysis['gt_rank']}ï¼‰")
    
    print(f"\nTop-10 è°“è¯æ’å:")
    for i, (pred, sim) in enumerate(analysis['top_10_predicates'], 1):
        marker = "âœ…" if pred == analysis['gt_predicate'] else "  "
        print(f"  {i:2d}. {marker} {pred:20s}: {sim:.6f}")
    
    if detailed:
        print(f"\næ‰€æœ‰è°“è¯æ’åï¼ˆå‰20ä¸ªï¼‰:")
        for i, (pred, sim) in enumerate(analysis['all_predicates_ranked'][:20], 1):
            marker = "âœ…" if pred == analysis['gt_predicate'] else "  "
            print(f"  {i:2d}. {marker} {pred:20s}: {sim:.6f}")


def analyze_badcases(all_analyses: List[Dict], rank_threshold: int = 20) -> Dict:
    """
    åˆ†æbadcaseï¼ˆGTè°“è¯æ’åè¾ƒä½çš„æƒ…å†µï¼‰
    
    Args:
        all_analyses: æ‰€æœ‰pairçš„åˆ†æç»“æœ
        rank_threshold: æ’åé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯badcase
    
    Returns:
        badcaseç»Ÿè®¡ä¿¡æ¯
    """
    print(f"\nğŸ“Š åˆ†æBadcaseï¼ˆæ’å > {rank_threshold} çš„æƒ…å†µï¼‰...")
    
    badcases = []
    good_cases = []
    missing_cases = []
    
    for analysis in all_analyses:
        if 'error' in analysis:
            missing_cases.append(analysis)
        elif analysis['gt_rank'] is None:
            missing_cases.append(analysis)
        elif analysis['gt_rank'] > rank_threshold:
            badcases.append(analysis)
        else:
            good_cases.append(analysis)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(all_analyses),
        'badcases': len(badcases),
        'good_cases': len(good_cases),
        'missing_cases': len(missing_cases),
        'badcase_rate': len(badcases) / len(all_analyses) if all_analyses else 0,
        'badcases_list': badcases
    }
    
    print(f"   æ€»GT pairæ•°: {stats['total']}")
    print(f"   Badcaseæ•°ï¼ˆæ’å>{rank_threshold}ï¼‰: {stats['badcases']} ({stats['badcase_rate']*100:.2f}%)")
    print(f"   æ­£å¸¸caseæ•°ï¼ˆæ’å<={rank_threshold}ï¼‰: {stats['good_cases']} ({(1-stats['badcase_rate'])*100:.2f}%)")
    print(f"   ç¼ºå¤±caseæ•°ï¼ˆGTè°“è¯ä¸åœ¨å€™é€‰åˆ—è¡¨ä¸­ï¼‰: {stats['missing_cases']}")
    
    return stats


def print_badcase_summary(badcases: List[Dict], top_n: int = 20):
    """æ‰“å°badcaseæ‘˜è¦"""
    if not badcases:
        print("\nâœ… æ²¡æœ‰å‘ç°badcaseï¼")
        return
    
    print(f"\n{'='*80}")
    print(f"Badcaseæ‘˜è¦ï¼ˆæ˜¾ç¤ºå‰{top_n}ä¸ªï¼‰")
    print(f"{'='*80}")
    
    # æŒ‰æ’åæ’åº
    sorted_badcases = sorted(badcases, key=lambda x: x.get('gt_rank', 999), reverse=True)
    
    for i, analysis in enumerate(sorted_badcases[:top_n], 1):
        print(f"\n{i}. å›¾ç‰‡#{analysis['image_id']}: {analysis['subject']} -> {analysis['object']}")
        print(f"   GTè°“è¯: {analysis['gt_predicate']}")
        print(f"   æ’å: {analysis['gt_rank']}/{analysis['total_predicates']}")
        print(f"   ç›¸ä¼¼åº¦: {analysis['gt_similarity']:.6f}")
        
        # æ˜¾ç¤ºTop-3è°“è¯
        top3 = analysis['top_10_predicates'][:3]
        print(f"   Top-3è°“è¯: {', '.join([f'{p}({s:.4f})' for p, s in top3])}")


def analyze_by_image(data: Dict, image_id: Optional[int] = None):
    """æŒ‰å›¾ç‰‡åˆ†æ"""
    all_analyses = analyze_all_gt_pairs(data)
    
    if image_id is not None:
        # åªåˆ†ææŒ‡å®šå›¾ç‰‡
        image_analyses = [a for a in all_analyses if a['image_id'] == image_id]
        print(f"\nğŸ“¸ å›¾ç‰‡ #{image_id} çš„åˆ†æç»“æœï¼ˆå…± {len(image_analyses)} ä¸ªGT pairï¼‰:")
        print("="*80)
        
        for analysis in image_analyses:
            print_pair_analysis(analysis, detailed=True)
    else:
        # åˆ†ææ‰€æœ‰å›¾ç‰‡
        print(f"\nğŸ“Š æ‰€æœ‰å›¾ç‰‡çš„åˆ†æç»“æœï¼ˆå…± {len(all_analyses)} ä¸ªGT pairï¼‰:")
        
        # ç»Ÿè®¡æ¯ä¸ªå›¾ç‰‡çš„badcaseæ•°
        image_stats = defaultdict(lambda: {'total': 0, 'badcases': 0, 'good_cases': 0})
        for analysis in all_analyses:
            img_id = analysis['image_id']
            image_stats[img_id]['total'] += 1
            if 'error' not in analysis and analysis.get('gt_rank') is not None:
                if analysis['gt_rank'] > 20:
                    image_stats[img_id]['badcases'] += 1
                else:
                    image_stats[img_id]['good_cases'] += 1
        
        print(f"\nå„å›¾ç‰‡ç»Ÿè®¡ï¼ˆæ’å>20ä¸ºbadcaseï¼‰:")
        print(f"{'å›¾ç‰‡ID':<12} {'æ€»pairæ•°':<10} {'Badcaseæ•°':<12} {'æ­£å¸¸caseæ•°':<12} {'Badcaseç‡':<10}")
        print("-"*60)
        for img_id in sorted(image_stats.keys()):
            stats = image_stats[img_id]
            badcase_rate = stats['badcases'] / stats['total'] if stats['total'] > 0 else 0
            print(f"{img_id:<12} {stats['total']:<10} {stats['badcases']:<12} {stats['good_cases']:<12} {badcase_rate*100:>6.2f}%")


def export_detailed_analysis(all_analyses: List[Dict], output_path: str):
    """å¯¼å‡ºè¯¦ç»†åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
    print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡ºè¯¦ç»†åˆ†æç»“æœåˆ°: {output_path}")
    
    export_data = {
        'summary': {
            'total_pairs': len(all_analyses),
            'badcases_count': sum(1 for a in all_analyses if 'error' not in a and a.get('gt_rank', 999) > 20),
            'good_cases_count': sum(1 for a in all_analyses if 'error' not in a and a.get('gt_rank', 999) <= 20),
            'missing_cases_count': sum(1 for a in all_analyses if 'error' in a)
        },
        'detailed_analyses': all_analyses
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å¯¼å‡ºå®Œæˆ")


def main():
    parser = argparse.ArgumentParser(
        description="Badcaseåˆ†æè„šæœ¬ - åˆ†ææ¯ä¸ªGT pairçš„ç›¸ä¼¼åº¦æ’å",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åˆ†ææ‰€æœ‰GT pairçš„æ’åæƒ…å†µ
  python analyze_badcases.py --json_file results.json
  
  # åˆ†ææŒ‡å®šå›¾ç‰‡
  python analyze_badcases.py --json_file results.json --image_id 2339501
  
  # åˆ†æbadcaseå¹¶å¯¼å‡ºè¯¦ç»†ç»“æœ
  python analyze_badcases.py --json_file results.json --export badcase_analysis.json
  
  # è®¾ç½®badcaseé˜ˆå€¼ï¼ˆé»˜è®¤20ï¼‰
  python analyze_badcases.py --json_file results.json --rank_threshold 10
        """
    )
    
    parser.add_argument('--json_file', type=str, default="/public/home/xiaojw2025/Data/embedding_similarity/vlm2vec_qwen2vl/result_recall_20_all.json",
                       help='é¢„æµ‹ç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--image_id', type=int, default=None,
                       help='æŒ‡å®šè¦åˆ†æçš„å›¾ç‰‡IDï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--rank_threshold', type=int, default=20,
                       help='Badcaseæ’åé˜ˆå€¼ï¼ˆé»˜è®¤20ï¼Œå³æ’å>20è®¤ä¸ºæ˜¯badcaseï¼‰')
    parser.add_argument('--export', type=str, default=None,
                       help='å¯¼å‡ºè¯¦ç»†åˆ†æç»“æœåˆ°æŒ‡å®šJSONæ–‡ä»¶')
    parser.add_argument('--show_top_badcases', type=int, default=20,
                       help='æ˜¾ç¤ºå‰Nä¸ªbadcaseï¼ˆé»˜è®¤20ï¼‰')
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    data = load_results(args.json_file)
    
    # åˆ†ææ‰€æœ‰GT pair
    all_analyses = analyze_all_gt_pairs(data)
    
    # æŒ‰å›¾ç‰‡åˆ†ææˆ–æ•´ä½“åˆ†æ
    if args.image_id is not None:
        analyze_by_image(data, args.image_id)
    else:
        analyze_by_image(data)
        
        # åˆ†æbadcase
        badcase_stats = analyze_badcases(all_analyses, rank_threshold=args.rank_threshold)
        
        # æ‰“å°badcaseæ‘˜è¦
        print_badcase_summary(badcase_stats['badcases_list'], top_n=args.show_top_badcases)
    
    # å¯¼å‡ºç»“æœ
    if args.export:
        export_detailed_analysis(all_analyses, args.export)
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()

